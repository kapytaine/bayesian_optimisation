{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3911da83",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Packages</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Link\" data-toc-modified-id=\"Link-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Link</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#Logit\" data-toc-modified-id=\"Logit-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Logit</a></span></li></ul></li><li><span><a href=\"#Bayesian-GLM\" data-toc-modified-id=\"Bayesian-GLM-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Bayesian GLM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#BayesianLogisticRegression\" data-toc-modified-id=\"BayesianLogisticRegression-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>BayesianLogisticRegression</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"#Miscellenaous\" data-toc-modified-id=\"Miscellenaous-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Miscellenaous</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read\" data-toc-modified-id=\"Read-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Read</a></span></li><li><span><a href=\"#Split-train-test\" data-toc-modified-id=\"Split-train-test-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Split train test</a></span></li></ul></li><li><span><a href=\"#Studies\" data-toc-modified-id=\"Studies-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Studies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-sigma2\" data-toc-modified-id=\"Single-sigma2-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Single sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-model\" data-toc-modified-id=\"Naive-model-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Naive model</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.1.2.1\"><span class=\"toc-item-num\">4.1.2.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.1.2.2\"><span class=\"toc-item-num\">4.1.2.2&nbsp;&nbsp;</span>Test</a></span></li></ul></li><li><span><a href=\"#Bayesian-optimisation\" data-toc-modified-id=\"Bayesian-optimisation-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Bayesian optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.1.3.1\"><span class=\"toc-item-num\">4.1.3.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-4.1.3.2\"><span class=\"toc-item-num\">4.1.3.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.1.3.3\"><span class=\"toc-item-num\">4.1.3.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li><li><span><a href=\"#Multiple-sigma2\" data-toc-modified-id=\"Multiple-sigma2-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Multiple sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocess-data\" data-toc-modified-id=\"Preprocess-data-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Preprocess data</a></span></li><li><span><a href=\"#Grid-search\" data-toc-modified-id=\"Grid-search-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Grid search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.2.2.1\"><span class=\"toc-item-num\">4.2.2.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.2.2.2\"><span class=\"toc-item-num\">4.2.2.2&nbsp;&nbsp;</span>Test</a></span></li></ul></li><li><span><a href=\"#Bayesian-optimization\" data-toc-modified-id=\"Bayesian-optimization-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Bayesian optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.2.3.1\"><span class=\"toc-item-num\">4.2.3.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-4.2.3.2\"><span class=\"toc-item-num\">4.2.3.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.2.3.3\"><span class=\"toc-item-num\">4.2.3.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Bayesian-optimization\" data-toc-modified-id=\"Bayesian-optimization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Bayesian optimization</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95e967",
   "metadata": {},
   "source": [
    "___\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8298c767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nfrom __future__ import annotations\\n\\nfrom abc import ABCMeta, abstractmethod\\nimport itertools\\nimport logging\\nfrom typing import Any, Tuple, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy.special import expit, logit\\nfrom scipy import optimize\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, make_scorer\\nfrom sklearn.linear_model import (\\n    BayesianRidge,\\n    LinearRegression,\\n    LogisticRegression,\\n    Ridge,\\n)\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.base import BaseEstimator\\nfrom scipy.optimize import approx_fprime, minimize\\nfrom scipy.special import expit\\nfrom scipy.stats import multivariate_normal\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nfrom __future__ import annotations\\n\\nfrom abc import ABCMeta, abstractmethod\\nimport itertools\\nimport logging\\nfrom typing import Any, Tuple, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy.special import expit, logit\\nfrom scipy import optimize\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, make_scorer\\nfrom sklearn.linear_model import (\\n    BayesianRidge,\\n    LinearRegression,\\n    LogisticRegression,\\n    Ridge,\\n)\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.base import BaseEstimator\\nfrom scipy.optimize import approx_fprime, minimize\\nfrom scipy.special import expit\\nfrom scipy.stats import multivariate_normal\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import itertools\n",
    "import logging\n",
    "from typing import Any, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit, logit\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, make_scorer\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import approx_fprime, minimize\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a63f4e",
   "metadata": {},
   "source": [
    "___\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda78f10",
   "metadata": {},
   "source": [
    "## Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c24f2",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b8ffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"class Link(metaclass=ABCMeta):\\n    @abstractmethod\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return self._inv_jac_link(self._inv_link(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return np.multiply(\\n            -self._hess_link(self._inv_link(x)),\\n            np.power(self._jac_inv_link(x), 3),\\n        )\\n\\n    @abstractmethod\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        This method has been chosen to avoid numerical error.\\n        When working with _jac_link, it may be close to 1/0 close to the\\n        optimum resulting in an zero division error.\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\";\n",
       "                var nbb_formatted_code = \"class Link(metaclass=ABCMeta):\\n    @abstractmethod\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return self._inv_jac_link(self._inv_link(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return np.multiply(\\n            -self._hess_link(self._inv_link(x)),\\n            np.power(self._jac_inv_link(x), 3),\\n        )\\n\\n    @abstractmethod\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        This method has been chosen to avoid numerical error.\\n        When working with _jac_link, it may be close to 1/0 close to the\\n        optimum resulting in an zero division error.\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Link(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def _inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self._inv_jac_link(self._inv_link(x))\n",
    "\n",
    "    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.multiply(\n",
    "            -self._hess_link(self._inv_link(x)),\n",
    "            np.power(self._jac_inv_link(x), 3),\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        This method has been chosen to avoid numerical error.\n",
    "        When working with _jac_link, it may be close to 1/0 close to the\n",
    "        optimum resulting in an zero division error.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _hess_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f111bf5",
   "metadata": {},
   "source": [
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811f567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"class Logit(Link):\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x)\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\\n\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        return logit(y)\\n\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        return y * (1 - y)\\n\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        return (2 * y - 1) / (y * (1 - y)) ** 2\";\n",
       "                var nbb_formatted_code = \"class Logit(Link):\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x)\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\\n\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        return logit(y)\\n\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        return y * (1 - y)\\n\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        return (2 * y - 1) / (y * (1 - y)) ** 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Logit(Link):\n",
    "    def _inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x)\n",
    "\n",
    "    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x) * (1 - expit(x))\n",
    "\n",
    "    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\n",
    "\n",
    "    def _link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return logit(y)\n",
    "\n",
    "    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return y * (1 - y)\n",
    "\n",
    "    def _hess_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return (2 * y - 1) / (y * (1 - y)) ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec15fa",
   "metadata": {},
   "source": [
    "## Bayesian GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0861d",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff1ffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\\n    def __init__(\\n        self,\\n        prior_parameters: dict = {},\\n        default_parameters: dict = {\\\"m\\\": 0, \\\"p\\\": 5},\\n        optimize_kwargs: dict = {},\\n        optimize_method: str = \\\"L-BFGS-B\\\",\\n    ) -> None:\\n        self.prior_parameters = prior_parameters\\n        self.default_parameters = default_parameters\\n        self.optimize_kwargs = optimize_kwargs\\n        self.optimize_method = optimize_method\\n\\n    def _check_distribution_params(self, params: dict) -> None:\\n        if not (\\n            isinstance(params, dict)\\n            and set(params.keys()) == set([\\\"m\\\", \\\"p\\\"])\\n            and (\\n                isinstance(params.get(\\\"m\\\"), Union[int, float])  # type: ignore\\n                and not np.isnan(params.get(\\\"m\\\"))  # type: ignore\\n            )\\n            and (\\n                isinstance(params.get(\\\"p\\\"), Union[int, float])  # type: ignore\\n                and params.get(\\\"p\\\") >= 0  # type: ignore\\n            )\\n        ):\\n            raise ValueError\\n\\n    @abstractmethod\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\\n        m = np.array([])\\n        p = np.array([])\\n        for col in feat_names:\\n            m = np.append(\\n                m,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"],\\n            )\\n            p = np.append(\\n                p,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"],\\n            )\\n        return m, p\\n\\n    def _update_m_p(\\n        self,\\n        res: optimize.OptimizeResult,\\n        X: pd.DataFrame,\\n        y: pd.Series,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> None:\\n        # get new updates values\\n        m_updated = res.x\\n        p_updated = self._diag_hess(\\n            res.x,\\n            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n        )\\n        feat_names = X.columns.to_list()\\n\\n        # check hessian positiveness\\n        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\\n            raise NotPositiveHessian\\n\\n        # update parameters\\n        self.posterior_parameters_ = dict(\\n            self.prior_parameters,\\n            **{\\n                col: {\\\"m\\\": m, \\\"p\\\": p}\\n                for col, m, p in zip(feat_names, m_updated, p_updated)\\n            },\\n        )\\n\\n        # check parameters\\n        for _, v in self.posterior_parameters_.items():\\n            self._check_distribution_params(v)\\n\\n    @abstractmethod\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\\n        logging.info(f\\\"Shape of the training dataset: {X.shape}.\\\")\\n\\n        # get parameter prior distribution parameters (mean and precision)\\n        m, p = self._get_prior_m_p(X.columns.to_list())\\n        logging.info(\\\"Prior distribution parameters obtained.\\\")\\n\\n        # optimize\\n        res = optimize.minimize(\\n            fun=self._loss,\\n            x0=m,\\n            jac=self._jac,\\n            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n            method=self.optimize_method,\\n            **self.optimize_kwargs,\\n        )\\n\\n        # log optimization result\\n        initial_loss = self._loss(\\n            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\\n        )\\n        log_message = (\\n            f\\\"initial fun: {initial_loss}\\\\n\\\"\\n            \\\"The OptimizeResult instance is:\\\\n\\\"\\n            f\\\"{res}\\\"\\n        )\\n\\n        if res.success:\\n            logging.info(f\\\"Log likelihood optimized successfully.\\\\n{log_message}\\\")\\n\\n            # calculate parameter posterior distribution parameters\\n            self._update_m_p(res, X, y, m, p, **kwargs)\\n            logging.info(\\\"Posterior distribution parameters computed.\\\")\\n        else:\\n            logging.warning(f\\\"Log likelihood optimization failed.\\\\n{log_message}\\\")\\n            if res.fun < initial_loss:\\n                # calculate parameter posterior distribution parameters\\n                self._update_m_p(res, X, y, m, p, **kwargs)\\n                logging.info(\\n                    \\\"Posterior distribution parameters computed even \\\"\\n                    \\\"though the optimization failed.\\\"\\n                )\\n            else:\\n                raise OptimizationError\\n\\n        return self\\n\\n    @abstractmethod\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def get_params(self, deep=True) -> dict:\\n        return {\\n            \\\"prior_parameters\\\": self.prior_parameters,\\n            \\\"default_parameters\\\": self.default_parameters,\\n            \\\"optimize_kwargs\\\": self.optimize_kwargs,\\n            \\\"optimize_method\\\": self.optimize_method,\\n        }\\n\\n    def set_params(self, **parameters) -> BayesianInterface:\\n        for parameter, value in parameters.items():\\n            setattr(self, parameter, value)\\n        return self\";\n",
       "                var nbb_formatted_code = \"class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\\n    def __init__(\\n        self,\\n        prior_parameters: dict = {},\\n        default_parameters: dict = {\\\"m\\\": 0, \\\"p\\\": 5},\\n        optimize_kwargs: dict = {},\\n        optimize_method: str = \\\"L-BFGS-B\\\",\\n    ) -> None:\\n        self.prior_parameters = prior_parameters\\n        self.default_parameters = default_parameters\\n        self.optimize_kwargs = optimize_kwargs\\n        self.optimize_method = optimize_method\\n\\n    def _check_distribution_params(self, params: dict) -> None:\\n        if not (\\n            isinstance(params, dict)\\n            and set(params.keys()) == set([\\\"m\\\", \\\"p\\\"])\\n            and (\\n                isinstance(params.get(\\\"m\\\"), Union[int, float])  # type: ignore\\n                and not np.isnan(params.get(\\\"m\\\"))  # type: ignore\\n            )\\n            and (\\n                isinstance(params.get(\\\"p\\\"), Union[int, float])  # type: ignore\\n                and params.get(\\\"p\\\") >= 0  # type: ignore\\n            )\\n        ):\\n            raise ValueError\\n\\n    @abstractmethod\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\\n        m = np.array([])\\n        p = np.array([])\\n        for col in feat_names:\\n            m = np.append(\\n                m,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"],\\n            )\\n            p = np.append(\\n                p,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"],\\n            )\\n        return m, p\\n\\n    def _update_m_p(\\n        self,\\n        res: optimize.OptimizeResult,\\n        X: pd.DataFrame,\\n        y: pd.Series,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> None:\\n        # get new updates values\\n        m_updated = res.x\\n        p_updated = self._diag_hess(\\n            res.x,\\n            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n        )\\n        feat_names = X.columns.to_list()\\n\\n        # check hessian positiveness\\n        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\\n            raise NotPositiveHessian\\n\\n        # update parameters\\n        self.posterior_parameters_ = dict(\\n            self.prior_parameters,\\n            **{\\n                col: {\\\"m\\\": m, \\\"p\\\": p}\\n                for col, m, p in zip(feat_names, m_updated, p_updated)\\n            },\\n        )\\n\\n        # check parameters\\n        for _, v in self.posterior_parameters_.items():\\n            self._check_distribution_params(v)\\n\\n    @abstractmethod\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\\n        logging.info(f\\\"Shape of the training dataset: {X.shape}.\\\")\\n\\n        # get parameter prior distribution parameters (mean and precision)\\n        m, p = self._get_prior_m_p(X.columns.to_list())\\n        logging.info(\\\"Prior distribution parameters obtained.\\\")\\n\\n        # optimize\\n        res = optimize.minimize(\\n            fun=self._loss,\\n            x0=m,\\n            jac=self._jac,\\n            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n            method=self.optimize_method,\\n            **self.optimize_kwargs,\\n        )\\n\\n        # log optimization result\\n        initial_loss = self._loss(\\n            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\\n        )\\n        log_message = (\\n            f\\\"initial fun: {initial_loss}\\\\n\\\"\\n            \\\"The OptimizeResult instance is:\\\\n\\\"\\n            f\\\"{res}\\\"\\n        )\\n\\n        if res.success:\\n            logging.info(f\\\"Log likelihood optimized successfully.\\\\n{log_message}\\\")\\n\\n            # calculate parameter posterior distribution parameters\\n            self._update_m_p(res, X, y, m, p, **kwargs)\\n            logging.info(\\\"Posterior distribution parameters computed.\\\")\\n        else:\\n            logging.warning(f\\\"Log likelihood optimization failed.\\\\n{log_message}\\\")\\n            if res.fun < initial_loss:\\n                # calculate parameter posterior distribution parameters\\n                self._update_m_p(res, X, y, m, p, **kwargs)\\n                logging.info(\\n                    \\\"Posterior distribution parameters computed even \\\"\\n                    \\\"though the optimization failed.\\\"\\n                )\\n            else:\\n                raise OptimizationError\\n\\n        return self\\n\\n    @abstractmethod\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def get_params(self, deep=True) -> dict:\\n        return {\\n            \\\"prior_parameters\\\": self.prior_parameters,\\n            \\\"default_parameters\\\": self.default_parameters,\\n            \\\"optimize_kwargs\\\": self.optimize_kwargs,\\n            \\\"optimize_method\\\": self.optimize_method,\\n        }\\n\\n    def set_params(self, **parameters) -> BayesianInterface:\\n        for parameter, value in parameters.items():\\n            setattr(self, parameter, value)\\n        return self\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\n",
    "    def __init__(\n",
    "        self,\n",
    "        prior_parameters: dict = {},\n",
    "        default_parameters: dict = {\"m\": 0, \"p\": 5},\n",
    "        optimize_kwargs: dict = {},\n",
    "        optimize_method: str = \"L-BFGS-B\",\n",
    "    ) -> None:\n",
    "        self.prior_parameters = prior_parameters\n",
    "        self.default_parameters = default_parameters\n",
    "        self.optimize_kwargs = optimize_kwargs\n",
    "        self.optimize_method = optimize_method\n",
    "\n",
    "    def _check_distribution_params(self, params: dict) -> None:\n",
    "        if not (\n",
    "            isinstance(params, dict)\n",
    "            and set(params.keys()) == set([\"m\", \"p\"])\n",
    "            and (\n",
    "                isinstance(params.get(\"m\"), Union[int, float])  # type: ignore\n",
    "                and not np.isnan(params.get(\"m\"))  # type: ignore\n",
    "            )\n",
    "            and (\n",
    "                isinstance(params.get(\"p\"), Union[int, float])  # type: ignore\n",
    "                and params.get(\"p\") >= 0  # type: ignore\n",
    "            )\n",
    "        ):\n",
    "            raise ValueError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _loss(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> float:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _jac(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _diag_hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        m = np.array([])\n",
    "        p = np.array([])\n",
    "        for col in feat_names:\n",
    "            m = np.append(\n",
    "                m,\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"m\"],\n",
    "            )\n",
    "            p = np.append(\n",
    "                p,\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"p\"],\n",
    "            )\n",
    "        return m, p\n",
    "\n",
    "    def _update_m_p(\n",
    "        self,\n",
    "        res: optimize.OptimizeResult,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        # get new updates values\n",
    "        m_updated = res.x\n",
    "        p_updated = self._diag_hess(\n",
    "            res.x,\n",
    "            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\n",
    "        )\n",
    "        feat_names = X.columns.to_list()\n",
    "\n",
    "        # check hessian positiveness\n",
    "        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\n",
    "            raise NotPositiveHessian\n",
    "\n",
    "        # update parameters\n",
    "        self.posterior_parameters_ = dict(\n",
    "            self.prior_parameters,\n",
    "            **{\n",
    "                col: {\"m\": m, \"p\": p}\n",
    "                for col, m, p in zip(feat_names, m_updated, p_updated)\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # check parameters\n",
    "        for _, v in self.posterior_parameters_.items():\n",
    "            self._check_distribution_params(v)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_args(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\n",
    "        logging.info(f\"Shape of the training dataset: {X.shape}.\")\n",
    "\n",
    "        # get parameter prior distribution parameters (mean and precision)\n",
    "        m, p = self._get_prior_m_p(X.columns.to_list())\n",
    "        logging.info(\"Prior distribution parameters obtained.\")\n",
    "\n",
    "        # optimize\n",
    "        res = optimize.minimize(\n",
    "            fun=self._loss,\n",
    "            x0=m,\n",
    "            jac=self._jac,\n",
    "            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\n",
    "            method=self.optimize_method,\n",
    "            **self.optimize_kwargs,\n",
    "        )\n",
    "\n",
    "        # log optimization result\n",
    "        initial_loss = self._loss(\n",
    "            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\n",
    "        )\n",
    "        log_message = (\n",
    "            f\"initial fun: {initial_loss}\\n\"\n",
    "            \"The OptimizeResult instance is:\\n\"\n",
    "            f\"{res}\"\n",
    "        )\n",
    "\n",
    "        if res.success:\n",
    "            logging.info(f\"Log likelihood optimized successfully.\\n{log_message}\")\n",
    "\n",
    "            # calculate parameter posterior distribution parameters\n",
    "            self._update_m_p(res, X, y, m, p, **kwargs)\n",
    "            logging.info(\"Posterior distribution parameters computed.\")\n",
    "        else:\n",
    "            logging.warning(f\"Log likelihood optimization failed.\\n{log_message}\")\n",
    "            if res.fun < initial_loss:\n",
    "                # calculate parameter posterior distribution parameters\n",
    "                self._update_m_p(res, X, y, m, p, **kwargs)\n",
    "                logging.info(\n",
    "                    \"Posterior distribution parameters computed even \"\n",
    "                    \"though the optimization failed.\"\n",
    "                )\n",
    "            else:\n",
    "                raise OptimizationError\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def get_params(self, deep=True) -> dict:\n",
    "        return {\n",
    "            \"prior_parameters\": self.prior_parameters,\n",
    "            \"default_parameters\": self.default_parameters,\n",
    "            \"optimize_kwargs\": self.optimize_kwargs,\n",
    "            \"optimize_method\": self.optimize_method,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters) -> BayesianInterface:\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef38757",
   "metadata": {},
   "source": [
    "### BayesianLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69813b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"class BayesianLogisticRegression(BayesianInterface):\\n    def __init__(self, link: Link = Logit(), **kwargs):\\n        self.link = link\\n        super().__init__(**kwargs)\\n\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> float:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :param r: regularization parameter\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        loss = (\\n            log_loss(y, y_pred, sample_weight=weights, normalize=False)\\n            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\\n            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\\n        )\\n\\n        return loss\\n\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        jac = np.add(\\n            -np.dot(weights * (y - y_pred), X),\\n            np.multiply(p, np.subtract(beta, m)),\\n        )\\n\\n        return jac\\n\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        diag_hess = p + np.dot(\\n            np.power(X, 2).T,\\n            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\\n        )\\n        return diag_hess\\n\\n    def _hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        hess = np.diag(p) + np.matmul(\\n            np.matmul(\\n                X.T,\\n                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n            ),\\n            X,\\n        )\\n        return hess\\n\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        m_posterior = np.array(\\n            [\\n                self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        m_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        p_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\\n\\n    def get_params(self, deep=True):\\n        shared_params = super().get_params()\\n        return dict(shared_params, **{\\\"link\\\": self.link})\\n\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        return (X, y, m, p)\\n\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        m = [\\n            self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n            for col in X.columns\\n        ]\\n        linear_pred = np.dot(X.to_numpy(), np.array(m))\\n        return self.link._inv_link(linear_pred)\";\n",
       "                var nbb_formatted_code = \"class BayesianLogisticRegression(BayesianInterface):\\n    def __init__(self, link: Link = Logit(), **kwargs):\\n        self.link = link\\n        super().__init__(**kwargs)\\n\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> float:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :param r: regularization parameter\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        loss = (\\n            log_loss(y, y_pred, sample_weight=weights, normalize=False)\\n            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\\n            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\\n        )\\n\\n        return loss\\n\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        jac = np.add(\\n            -np.dot(weights * (y - y_pred), X),\\n            np.multiply(p, np.subtract(beta, m)),\\n        )\\n\\n        return jac\\n\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        diag_hess = p + np.dot(\\n            np.power(X, 2).T,\\n            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\\n        )\\n        return diag_hess\\n\\n    def _hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        hess = np.diag(p) + np.matmul(\\n            np.matmul(\\n                X.T,\\n                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n            ),\\n            X,\\n        )\\n        return hess\\n\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        m_posterior = np.array(\\n            [\\n                self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        m_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        p_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\\n\\n    def get_params(self, deep=True):\\n        shared_params = super().get_params()\\n        return dict(shared_params, **{\\\"link\\\": self.link})\\n\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        return (X, y, m, p)\\n\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        m = [\\n            self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n            for col in X.columns\\n        ]\\n        linear_pred = np.dot(X.to_numpy(), np.array(m))\\n        return self.link._inv_link(linear_pred)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BayesianLogisticRegression(BayesianInterface):\n",
    "    def __init__(self, link: Link = Logit(), **kwargs):\n",
    "        self.link = link\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _loss(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        LogLikelihood defines the regularized loss function\n",
    "        :param omega: vector to optimize\n",
    "        :param y: responses (1/-1) of training data\n",
    "        :param X: dimensions of training data\n",
    "        :param m: previous vector of means\n",
    "        :param p: previous vector of inverse variances\n",
    "        :param r: regularization parameter\n",
    "        :return out: value of loss function\n",
    "        :return grad: gradient of loss function\n",
    "        \"\"\"\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        loss = (\n",
    "            log_loss(y, y_pred, sample_weight=weights, normalize=False)\n",
    "            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\n",
    "            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _jac(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        LogLikelihood defines the regularized loss function\n",
    "        :param omega: vector to optimize\n",
    "        :param y: responses (1/-1) of training data\n",
    "        :param X: dimensions of training data\n",
    "        :param m: previous vector of means\n",
    "        :param p: previous vector of inverse variances\n",
    "        :return out: value of loss function\n",
    "        :return grad: gradient of loss function\n",
    "        \"\"\"\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        jac = np.add(\n",
    "            -np.dot(weights * (y - y_pred), X),\n",
    "            np.multiply(p, np.subtract(beta, m)),\n",
    "        )\n",
    "\n",
    "        return jac\n",
    "\n",
    "    def _diag_hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> np.ndarray:\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        diag_hess = p + np.dot(\n",
    "            np.power(X, 2).T,\n",
    "            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\n",
    "        )\n",
    "        return diag_hess\n",
    "\n",
    "    def _hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        hess = np.diag(p) + np.matmul(\n",
    "            np.matmul(\n",
    "                X.T,\n",
    "                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\n",
    "            ),\n",
    "            X,\n",
    "        )\n",
    "        return hess\n",
    "\n",
    "    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        m_posterior = np.array(\n",
    "            [\n",
    "                self.posterior_parameters_.get(col, self.default_parameters)[\"m\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        m_prior = np.array(\n",
    "            [\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"m\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        p_prior = np.array(\n",
    "            [\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"p\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        shared_params = super().get_params()\n",
    "        return dict(shared_params, **{\"link\": self.link})\n",
    "\n",
    "    def _get_args(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> tuple:\n",
    "        return (X, y, m, p)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        m = [\n",
    "            self.posterior_parameters_.get(col, self.default_parameters)[\"m\"]\n",
    "            for col in X.columns\n",
    "        ]\n",
    "        linear_pred = np.dot(X.to_numpy(), np.array(m))\n",
    "        return self.link._inv_link(linear_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2f408",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf96e6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"beta = np.random.normal(size=4)\\nX = np.random.normal(size=(10, 4))\\ny = np.random.choice([0, 1], size=10)\\nm = np.random.normal(size=4)\\nq = np.exp(np.random.normal(size=4))\";\n",
       "                var nbb_formatted_code = \"beta = np.random.normal(size=4)\\nX = np.random.normal(size=(10, 4))\\ny = np.random.choice([0, 1], size=10)\\nm = np.random.normal(size=4)\\nq = np.exp(np.random.normal(size=4))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = np.random.normal(size=4)\n",
    "X = np.random.normal(size=(10, 4))\n",
    "y = np.random.choice([0, 1], size=10)\n",
    "m = np.random.normal(size=4)\n",
    "q = np.exp(np.random.normal(size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65efbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._loss,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._jac(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_formatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._loss,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._jac(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.testing.assert_allclose(\n",
    "    approx_fprime(\n",
    "        beta,\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        1.4901161193847656e-08,\n",
    "        *(X, y, m, q),\n",
    "    ),\n",
    "    BayesianLogisticRegression()._jac(beta, X, y, m, q),\n",
    "    atol=1e-6,\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0faa7a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._jac,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._hess(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_formatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._jac,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._hess(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.testing.assert_allclose(\n",
    "    approx_fprime(\n",
    "        beta,\n",
    "        BayesianLogisticRegression()._jac,\n",
    "        1.4901161193847656e-08,\n",
    "        *(X, y, m, q),\n",
    "    ),\n",
    "    BayesianLogisticRegression()._hess(beta, X, y, m, q),\n",
    "    atol=1e-6,\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196c5e4",
   "metadata": {},
   "source": [
    "## Miscellenaous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63db1a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"def get_col_to_id(cols, cats):\\n    out = {}\\n    for col in cols:\\n        for ii, cat in enumerate(cats):\\n            if isinstance(cat, str):\\n                if cat in col:\\n                    out[col] = ii\\n            else:\\n                for cc in cat:\\n                    if cc in col:\\n                        out[col] = ii\\n    return out\";\n",
       "                var nbb_formatted_code = \"def get_col_to_id(cols, cats):\\n    out = {}\\n    for col in cols:\\n        for ii, cat in enumerate(cats):\\n            if isinstance(cat, str):\\n                if cat in col:\\n                    out[col] = ii\\n            else:\\n                for cc in cat:\\n                    if cc in col:\\n                        out[col] = ii\\n    return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_col_to_id(cols, cats):\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        for ii, cat in enumerate(cats):\n",
    "            if isinstance(cat, str):\n",
    "                if cat in col:\n",
    "                    out[col] = ii\n",
    "            else:\n",
    "                for cc in cat:\n",
    "                    if cc in col:\n",
    "                        out[col] = ii\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f72ff",
   "metadata": {},
   "source": [
    "___\n",
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf333b60",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab81624b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"data = pd.read_csv(\\n    \\\"/home/capitaine/01_projects/2024/bayesian_optimization/data/titanic/train.csv\\\"\\n)\\ny = data.pop(\\\"Survived\\\")\\ndata[\\\"CabinCat\\\"] = data[\\\"Cabin\\\"].astype(str).str[0]\\ndata[\\\"Embarked\\\"] = data[\\\"Embarked\\\"].fillna(\\\"unknown\\\")\\ndata[\\\"intercept\\\"] = 1.0\\nX = data[\\n    [\\\"intercept\\\", \\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\", \\\"Age\\\", \\\"Fare\\\", \\\"SibSp\\\", \\\"Parch\\\"]\\n].copy()\";\n",
       "                var nbb_formatted_code = \"data = pd.read_csv(\\n    \\\"/home/capitaine/01_projects/2024/bayesian_optimization/data/titanic/train.csv\\\"\\n)\\ny = data.pop(\\\"Survived\\\")\\ndata[\\\"CabinCat\\\"] = data[\\\"Cabin\\\"].astype(str).str[0]\\ndata[\\\"Embarked\\\"] = data[\\\"Embarked\\\"].fillna(\\\"unknown\\\")\\ndata[\\\"intercept\\\"] = 1.0\\nX = data[\\n    [\\n        \\\"intercept\\\",\\n        \\\"Pclass\\\",\\n        \\\"Sex\\\",\\n        \\\"Embarked\\\",\\n        \\\"CabinCat\\\",\\n        \\\"Age\\\",\\n        \\\"Fare\\\",\\n        \\\"SibSp\\\",\\n        \\\"Parch\\\",\\n    ]\\n].copy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"/home/capitaine/01_projects/2024/bayesian_optimization/data/titanic/train.csv\"\n",
    ")\n",
    "y = data.pop(\"Survived\")\n",
    "data[\"CabinCat\"] = data[\"Cabin\"].astype(str).str[0]\n",
    "data[\"Embarked\"] = data[\"Embarked\"].fillna(\"unknown\")\n",
    "data[\"intercept\"] = 1.0\n",
    "X = data[\n",
    "    [\n",
    "        \"intercept\",\n",
    "        \"Pclass\",\n",
    "        \"Sex\",\n",
    "        \"Embarked\",\n",
    "        \"CabinCat\",\n",
    "        \"Age\",\n",
    "        \"Fare\",\n",
    "        \"SibSp\",\n",
    "        \"Parch\",\n",
    "    ]\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ded10b",
   "metadata": {},
   "source": [
    "## Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b91bb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\";\n",
       "                var nbb_formatted_code = \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfce23",
   "metadata": {},
   "source": [
    "___\n",
    "# Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e400bbf",
   "metadata": {},
   "source": [
    "## Single sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422bdfc",
   "metadata": {},
   "source": [
    "### Naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4580a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666897029983943\n",
      "0.5\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"y_pred = np.array([np.mean(y_train)] * len(y_test))\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = np.array([np.mean(y_train)] * len(y_test))\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = np.array([np.mean(y_train)] * len(y_test))\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab81dd",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616d7ea",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ceacef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"preprocessor = ColumnTransformer(\\n    [\\n        (\\n            \\\"categorical_features_wo_nan\\\",\\n            OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n            [\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\"],\\n        ),\\n        (\\n            \\\"numerical_features_w_nan\\\",\\n            Pipeline(\\n                [\\n                    (\\\"imputer\\\", SimpleImputer(strategy=\\\"mean\\\")),\\n                ]\\n            ),\\n            [\\\"Age\\\"],\\n        ),\\n    ],\\n    remainder=\\\"passthrough\\\",\\n)\\npreprocessor.set_output(transform=\\\"pandas\\\")\\npipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\\"classifier\\\", BayesianLogisticRegression()),\\n    ]\\n)\";\n",
       "                var nbb_formatted_code = \"preprocessor = ColumnTransformer(\\n    [\\n        (\\n            \\\"categorical_features_wo_nan\\\",\\n            OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n            [\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\"],\\n        ),\\n        (\\n            \\\"numerical_features_w_nan\\\",\\n            Pipeline(\\n                [\\n                    (\\\"imputer\\\", SimpleImputer(strategy=\\\"mean\\\")),\\n                ]\\n            ),\\n            [\\\"Age\\\"],\\n        ),\\n    ],\\n    remainder=\\\"passthrough\\\",\\n)\\npreprocessor.set_output(transform=\\\"pandas\\\")\\npipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\\"classifier\\\", BayesianLogisticRegression()),\\n    ]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"categorical_features_wo_nan\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            [\"Pclass\", \"Sex\", \"Embarked\", \"CabinCat\"],\n",
    "        ),\n",
    "        (\n",
    "            \"numerical_features_w_nan\",\n",
    "            Pipeline(\n",
    "                [\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                ]\n",
    "            ),\n",
    "            [\"Age\"],\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"classifier\", BayesianLogisticRegression()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b57a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__default_parameters</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.208152</td>\n",
       "      <td>0.167665</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>{'m': 0, 'p': 0.001}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468198</td>\n",
       "      <td>-0.485785</td>\n",
       "      <td>-0.402604</td>\n",
       "      <td>-0.452749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438506</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.406088</td>\n",
       "      <td>-0.401326</td>\n",
       "      <td>-0.422308</td>\n",
       "      <td>-0.410291</td>\n",
       "      <td>-0.427314</td>\n",
       "      <td>-0.413465</td>\n",
       "      <td>0.009817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.647650</td>\n",
       "      <td>0.711555</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>{'m': 0, 'p': 0.0017782794100389228}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468229</td>\n",
       "      <td>-0.485770</td>\n",
       "      <td>-0.402198</td>\n",
       "      <td>-0.452334</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438374</td>\n",
       "      <td>0.039167</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.406089</td>\n",
       "      <td>-0.401337</td>\n",
       "      <td>-0.422308</td>\n",
       "      <td>-0.410281</td>\n",
       "      <td>-0.427319</td>\n",
       "      <td>-0.413467</td>\n",
       "      <td>0.009816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.066778</td>\n",
       "      <td>0.460854</td>\n",
       "      <td>0.021874</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>{'m': 0, 'p': 0.0031622776601683794}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468134</td>\n",
       "      <td>-0.485856</td>\n",
       "      <td>-0.401924</td>\n",
       "      <td>-0.452353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438282</td>\n",
       "      <td>0.039280</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.406108</td>\n",
       "      <td>-0.401344</td>\n",
       "      <td>-0.422320</td>\n",
       "      <td>-0.410297</td>\n",
       "      <td>-0.427342</td>\n",
       "      <td>-0.413482</td>\n",
       "      <td>0.009819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.441888</td>\n",
       "      <td>0.354298</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>{'m': 0, 'p': 0.005623413251903491}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468188</td>\n",
       "      <td>-0.485562</td>\n",
       "      <td>-0.402041</td>\n",
       "      <td>-0.452046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438187</td>\n",
       "      <td>0.039186</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.406120</td>\n",
       "      <td>-0.401365</td>\n",
       "      <td>-0.422320</td>\n",
       "      <td>-0.410314</td>\n",
       "      <td>-0.427360</td>\n",
       "      <td>-0.413496</td>\n",
       "      <td>0.009816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.583265</td>\n",
       "      <td>0.303748</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>{'m': 0, 'p': 0.01}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468168</td>\n",
       "      <td>-0.485841</td>\n",
       "      <td>-0.402198</td>\n",
       "      <td>-0.452298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438355</td>\n",
       "      <td>0.039192</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.406149</td>\n",
       "      <td>-0.401391</td>\n",
       "      <td>-0.422332</td>\n",
       "      <td>-0.410334</td>\n",
       "      <td>-0.427388</td>\n",
       "      <td>-0.413519</td>\n",
       "      <td>0.009814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.945683</td>\n",
       "      <td>0.260959</td>\n",
       "      <td>0.020733</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>{'m': 0, 'p': 0.01778279410038923}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468094</td>\n",
       "      <td>-0.485662</td>\n",
       "      <td>-0.402198</td>\n",
       "      <td>-0.452597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438380</td>\n",
       "      <td>0.039136</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.406179</td>\n",
       "      <td>-0.401427</td>\n",
       "      <td>-0.422350</td>\n",
       "      <td>-0.410379</td>\n",
       "      <td>-0.427416</td>\n",
       "      <td>-0.413550</td>\n",
       "      <td>0.009809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.398492</td>\n",
       "      <td>0.127198</td>\n",
       "      <td>0.014901</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>{'m': 0, 'p': 0.03162277660168379}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468363</td>\n",
       "      <td>-0.485606</td>\n",
       "      <td>-0.402083</td>\n",
       "      <td>-0.452251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438251</td>\n",
       "      <td>0.039272</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.406244</td>\n",
       "      <td>-0.401489</td>\n",
       "      <td>-0.422376</td>\n",
       "      <td>-0.410406</td>\n",
       "      <td>-0.427478</td>\n",
       "      <td>-0.413599</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.111924</td>\n",
       "      <td>0.399737</td>\n",
       "      <td>0.017947</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>{'m': 0, 'p': 0.056234132519034905}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468149</td>\n",
       "      <td>-0.485471</td>\n",
       "      <td>-0.401745</td>\n",
       "      <td>-0.451992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438019</td>\n",
       "      <td>0.039311</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.406321</td>\n",
       "      <td>-0.401576</td>\n",
       "      <td>-0.422424</td>\n",
       "      <td>-0.410474</td>\n",
       "      <td>-0.427566</td>\n",
       "      <td>-0.413672</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.305567</td>\n",
       "      <td>0.194199</td>\n",
       "      <td>0.018658</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>{'m': 0, 'p': 0.09999999999999999}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.467850</td>\n",
       "      <td>-0.485407</td>\n",
       "      <td>-0.401488</td>\n",
       "      <td>-0.451817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.437737</td>\n",
       "      <td>0.039458</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.406438</td>\n",
       "      <td>-0.401677</td>\n",
       "      <td>-0.422488</td>\n",
       "      <td>-0.410570</td>\n",
       "      <td>-0.427696</td>\n",
       "      <td>-0.413774</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.072609</td>\n",
       "      <td>0.435024</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>0.012723</td>\n",
       "      <td>{'m': 0, 'p': 0.1778279410038923}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.467781</td>\n",
       "      <td>-0.485152</td>\n",
       "      <td>-0.400841</td>\n",
       "      <td>-0.451415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.437359</td>\n",
       "      <td>0.039622</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.406622</td>\n",
       "      <td>-0.401837</td>\n",
       "      <td>-0.422644</td>\n",
       "      <td>-0.410699</td>\n",
       "      <td>-0.427903</td>\n",
       "      <td>-0.413941</td>\n",
       "      <td>0.009811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.062428</td>\n",
       "      <td>0.159560</td>\n",
       "      <td>0.020381</td>\n",
       "      <td>0.007351</td>\n",
       "      <td>{'m': 0, 'p': 0.31622776601683794}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.467475</td>\n",
       "      <td>-0.484839</td>\n",
       "      <td>-0.400226</td>\n",
       "      <td>-0.450700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.436832</td>\n",
       "      <td>0.039755</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.406894</td>\n",
       "      <td>-0.402113</td>\n",
       "      <td>-0.422911</td>\n",
       "      <td>-0.410876</td>\n",
       "      <td>-0.428234</td>\n",
       "      <td>-0.414206</td>\n",
       "      <td>0.009832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.081585</td>\n",
       "      <td>0.214740</td>\n",
       "      <td>0.014613</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>{'m': 0, 'p': 0.5623413251903491}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.467656</td>\n",
       "      <td>-0.484194</td>\n",
       "      <td>-0.399798</td>\n",
       "      <td>-0.449367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.436128</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.407372</td>\n",
       "      <td>-0.402581</td>\n",
       "      <td>-0.423411</td>\n",
       "      <td>-0.411253</td>\n",
       "      <td>-0.428804</td>\n",
       "      <td>-0.414684</td>\n",
       "      <td>0.009872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.070840</td>\n",
       "      <td>0.210031</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>{'m': 0, 'p': 1.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.467868</td>\n",
       "      <td>-0.483791</td>\n",
       "      <td>-0.399871</td>\n",
       "      <td>-0.447617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435566</td>\n",
       "      <td>0.040061</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.408238</td>\n",
       "      <td>-0.403446</td>\n",
       "      <td>-0.424289</td>\n",
       "      <td>-0.412033</td>\n",
       "      <td>-0.429853</td>\n",
       "      <td>-0.415572</td>\n",
       "      <td>0.009933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.966319</td>\n",
       "      <td>0.160407</td>\n",
       "      <td>0.021932</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>{'m': 0, 'p': 1.7782794100389228}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.468994</td>\n",
       "      <td>-0.483532</td>\n",
       "      <td>-0.401289</td>\n",
       "      <td>-0.445099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435503</td>\n",
       "      <td>0.039822</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.409777</td>\n",
       "      <td>-0.405110</td>\n",
       "      <td>-0.425847</td>\n",
       "      <td>-0.413603</td>\n",
       "      <td>-0.431597</td>\n",
       "      <td>-0.417187</td>\n",
       "      <td>0.009962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.828986</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>{'m': 0, 'p': 3.162277660168379}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.471245</td>\n",
       "      <td>-0.484500</td>\n",
       "      <td>-0.404214</td>\n",
       "      <td>-0.442686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.436571</td>\n",
       "      <td>0.039396</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.412510</td>\n",
       "      <td>-0.408194</td>\n",
       "      <td>-0.428690</td>\n",
       "      <td>-0.416783</td>\n",
       "      <td>-0.434658</td>\n",
       "      <td>-0.420167</td>\n",
       "      <td>0.009961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.677359</td>\n",
       "      <td>0.061691</td>\n",
       "      <td>0.015329</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>{'m': 0, 'p': 5.62341325190349}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.475782</td>\n",
       "      <td>-0.487697</td>\n",
       "      <td>-0.409798</td>\n",
       "      <td>-0.441493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.439859</td>\n",
       "      <td>0.038851</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.413430</td>\n",
       "      <td>-0.433553</td>\n",
       "      <td>-0.422502</td>\n",
       "      <td>-0.439777</td>\n",
       "      <td>-0.425321</td>\n",
       "      <td>0.009897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.717899</td>\n",
       "      <td>0.130802</td>\n",
       "      <td>0.018628</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>{'m': 0, 'p': 10.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.483284</td>\n",
       "      <td>-0.494316</td>\n",
       "      <td>-0.419259</td>\n",
       "      <td>-0.442774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446445</td>\n",
       "      <td>0.038213</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.425297</td>\n",
       "      <td>-0.421739</td>\n",
       "      <td>-0.441411</td>\n",
       "      <td>-0.431677</td>\n",
       "      <td>-0.447945</td>\n",
       "      <td>-0.433614</td>\n",
       "      <td>0.009799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.333714</td>\n",
       "      <td>0.080004</td>\n",
       "      <td>0.016896</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>{'m': 0, 'p': 17.78279410038923}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.494725</td>\n",
       "      <td>-0.504792</td>\n",
       "      <td>-0.433619</td>\n",
       "      <td>-0.447570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457253</td>\n",
       "      <td>0.037386</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.437361</td>\n",
       "      <td>-0.433974</td>\n",
       "      <td>-0.453191</td>\n",
       "      <td>-0.445224</td>\n",
       "      <td>-0.460091</td>\n",
       "      <td>-0.445968</td>\n",
       "      <td>0.009696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.249147</td>\n",
       "      <td>0.080968</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>{'m': 0, 'p': 31.622776601683796}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.511424</td>\n",
       "      <td>-0.519655</td>\n",
       "      <td>-0.453746</td>\n",
       "      <td>-0.456973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473237</td>\n",
       "      <td>0.036452</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.454489</td>\n",
       "      <td>-0.451154</td>\n",
       "      <td>-0.469708</td>\n",
       "      <td>-0.464011</td>\n",
       "      <td>-0.477079</td>\n",
       "      <td>-0.463288</td>\n",
       "      <td>0.009556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.209183</td>\n",
       "      <td>0.067973</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>{'m': 0, 'p': 56.23413251903491}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.533669</td>\n",
       "      <td>-0.539207</td>\n",
       "      <td>-0.479420</td>\n",
       "      <td>-0.471829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.494642</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.476838</td>\n",
       "      <td>-0.473681</td>\n",
       "      <td>-0.491173</td>\n",
       "      <td>-0.488328</td>\n",
       "      <td>-0.499086</td>\n",
       "      <td>-0.485821</td>\n",
       "      <td>0.009370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.186266</td>\n",
       "      <td>0.049471</td>\n",
       "      <td>0.014513</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>{'m': 0, 'p': 100.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.559799</td>\n",
       "      <td>-0.562005</td>\n",
       "      <td>-0.508547</td>\n",
       "      <td>-0.491614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520009</td>\n",
       "      <td>0.034763</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.502897</td>\n",
       "      <td>-0.500291</td>\n",
       "      <td>-0.516190</td>\n",
       "      <td>-0.516646</td>\n",
       "      <td>-0.524742</td>\n",
       "      <td>-0.512153</td>\n",
       "      <td>0.009180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.200045</td>\n",
       "      <td>0.088174</td>\n",
       "      <td>0.016629</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>{'m': 0, 'p': 177.82794100389228}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.586266</td>\n",
       "      <td>-0.584912</td>\n",
       "      <td>-0.537370</td>\n",
       "      <td>-0.513733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546005</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.529407</td>\n",
       "      <td>-0.527644</td>\n",
       "      <td>-0.541773</td>\n",
       "      <td>-0.545338</td>\n",
       "      <td>-0.550751</td>\n",
       "      <td>-0.538983</td>\n",
       "      <td>0.009021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.165725</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.019724</td>\n",
       "      <td>0.006847</td>\n",
       "      <td>{'m': 0, 'p': 316.2277660168379}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.609296</td>\n",
       "      <td>-0.604643</td>\n",
       "      <td>-0.562134</td>\n",
       "      <td>-0.534640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.568966</td>\n",
       "      <td>0.032676</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.552878</td>\n",
       "      <td>-0.552090</td>\n",
       "      <td>-0.564502</td>\n",
       "      <td>-0.570420</td>\n",
       "      <td>-0.573518</td>\n",
       "      <td>-0.562682</td>\n",
       "      <td>0.008819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.120053</td>\n",
       "      <td>0.020221</td>\n",
       "      <td>0.015053</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>{'m': 0, 'p': 562.341325190349}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.626903</td>\n",
       "      <td>-0.619537</td>\n",
       "      <td>-0.581003</td>\n",
       "      <td>-0.551776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.586812</td>\n",
       "      <td>0.031501</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.571266</td>\n",
       "      <td>-0.571449</td>\n",
       "      <td>-0.582367</td>\n",
       "      <td>-0.589678</td>\n",
       "      <td>-0.591040</td>\n",
       "      <td>-0.581160</td>\n",
       "      <td>0.008530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.085317</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.013959</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>{'m': 0, 'p': 1000.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-0.638909</td>\n",
       "      <td>-0.629607</td>\n",
       "      <td>-0.594093</td>\n",
       "      <td>-0.564368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.599324</td>\n",
       "      <td>0.030379</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.584303</td>\n",
       "      <td>-0.585328</td>\n",
       "      <td>-0.595080</td>\n",
       "      <td>-0.603025</td>\n",
       "      <td>-0.603226</td>\n",
       "      <td>-0.594192</td>\n",
       "      <td>0.008207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        3.208152      0.167665         0.017860        0.004853   \n",
       "1        3.647650      0.711555         0.016813        0.000470   \n",
       "2        3.066778      0.460854         0.021874        0.005475   \n",
       "3        3.441888      0.354298         0.016124        0.002454   \n",
       "4        2.583265      0.303748         0.015283        0.001095   \n",
       "5        2.945683      0.260959         0.020733        0.005906   \n",
       "6        2.398492      0.127198         0.014901        0.000674   \n",
       "7        3.111924      0.399737         0.017947        0.005049   \n",
       "8        2.305567      0.194199         0.018658        0.005166   \n",
       "9        2.072609      0.435024         0.021843        0.012723   \n",
       "10       2.062428      0.159560         0.020381        0.007351   \n",
       "11       1.081585      0.214740         0.014613        0.000680   \n",
       "12       1.070840      0.210031         0.015308        0.001222   \n",
       "13       0.966319      0.160407         0.021932        0.005020   \n",
       "14       0.828986      0.065042         0.017111        0.003375   \n",
       "15       0.677359      0.061691         0.015329        0.000811   \n",
       "16       0.717899      0.130802         0.018628        0.006941   \n",
       "17       0.333714      0.080004         0.016896        0.003227   \n",
       "18       0.249147      0.080968         0.020910        0.007778   \n",
       "19       0.209183      0.067973         0.017397        0.004215   \n",
       "20       0.186266      0.049471         0.014513        0.000636   \n",
       "21       0.200045      0.088174         0.016629        0.004077   \n",
       "22       0.165725      0.034000         0.019724        0.006847   \n",
       "23       0.120053      0.020221         0.015053        0.001083   \n",
       "24       0.085317      0.008070         0.013959        0.000302   \n",
       "\n",
       "    param_classifier__default_parameters  \\\n",
       "0                   {'m': 0, 'p': 0.001}   \n",
       "1   {'m': 0, 'p': 0.0017782794100389228}   \n",
       "2   {'m': 0, 'p': 0.0031622776601683794}   \n",
       "3    {'m': 0, 'p': 0.005623413251903491}   \n",
       "4                    {'m': 0, 'p': 0.01}   \n",
       "5     {'m': 0, 'p': 0.01778279410038923}   \n",
       "6     {'m': 0, 'p': 0.03162277660168379}   \n",
       "7    {'m': 0, 'p': 0.056234132519034905}   \n",
       "8     {'m': 0, 'p': 0.09999999999999999}   \n",
       "9      {'m': 0, 'p': 0.1778279410038923}   \n",
       "10    {'m': 0, 'p': 0.31622776601683794}   \n",
       "11     {'m': 0, 'p': 0.5623413251903491}   \n",
       "12                    {'m': 0, 'p': 1.0}   \n",
       "13     {'m': 0, 'p': 1.7782794100389228}   \n",
       "14      {'m': 0, 'p': 3.162277660168379}   \n",
       "15       {'m': 0, 'p': 5.62341325190349}   \n",
       "16                   {'m': 0, 'p': 10.0}   \n",
       "17      {'m': 0, 'p': 17.78279410038923}   \n",
       "18     {'m': 0, 'p': 31.622776601683796}   \n",
       "19      {'m': 0, 'p': 56.23413251903491}   \n",
       "20                  {'m': 0, 'p': 100.0}   \n",
       "21     {'m': 0, 'p': 177.82794100389228}   \n",
       "22      {'m': 0, 'p': 316.2277660168379}   \n",
       "23       {'m': 0, 'p': 562.341325190349}   \n",
       "24                 {'m': 0, 'p': 1000.0}   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'classifier__default_parameters': {'m': 0, 'p...          -0.468198   \n",
       "1   {'classifier__default_parameters': {'m': 0, 'p...          -0.468229   \n",
       "2   {'classifier__default_parameters': {'m': 0, 'p...          -0.468134   \n",
       "3   {'classifier__default_parameters': {'m': 0, 'p...          -0.468188   \n",
       "4   {'classifier__default_parameters': {'m': 0, 'p...          -0.468168   \n",
       "5   {'classifier__default_parameters': {'m': 0, 'p...          -0.468094   \n",
       "6   {'classifier__default_parameters': {'m': 0, 'p...          -0.468363   \n",
       "7   {'classifier__default_parameters': {'m': 0, 'p...          -0.468149   \n",
       "8   {'classifier__default_parameters': {'m': 0, 'p...          -0.467850   \n",
       "9   {'classifier__default_parameters': {'m': 0, 'p...          -0.467781   \n",
       "10  {'classifier__default_parameters': {'m': 0, 'p...          -0.467475   \n",
       "11  {'classifier__default_parameters': {'m': 0, 'p...          -0.467656   \n",
       "12  {'classifier__default_parameters': {'m': 0, 'p...          -0.467868   \n",
       "13  {'classifier__default_parameters': {'m': 0, 'p...          -0.468994   \n",
       "14  {'classifier__default_parameters': {'m': 0, 'p...          -0.471245   \n",
       "15  {'classifier__default_parameters': {'m': 0, 'p...          -0.475782   \n",
       "16  {'classifier__default_parameters': {'m': 0, 'p...          -0.483284   \n",
       "17  {'classifier__default_parameters': {'m': 0, 'p...          -0.494725   \n",
       "18  {'classifier__default_parameters': {'m': 0, 'p...          -0.511424   \n",
       "19  {'classifier__default_parameters': {'m': 0, 'p...          -0.533669   \n",
       "20  {'classifier__default_parameters': {'m': 0, 'p...          -0.559799   \n",
       "21  {'classifier__default_parameters': {'m': 0, 'p...          -0.586266   \n",
       "22  {'classifier__default_parameters': {'m': 0, 'p...          -0.609296   \n",
       "23  {'classifier__default_parameters': {'m': 0, 'p...          -0.626903   \n",
       "24  {'classifier__default_parameters': {'m': 0, 'p...          -0.638909   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           -0.485785          -0.402604          -0.452749  ...   \n",
       "1           -0.485770          -0.402198          -0.452334  ...   \n",
       "2           -0.485856          -0.401924          -0.452353  ...   \n",
       "3           -0.485562          -0.402041          -0.452046  ...   \n",
       "4           -0.485841          -0.402198          -0.452298  ...   \n",
       "5           -0.485662          -0.402198          -0.452597  ...   \n",
       "6           -0.485606          -0.402083          -0.452251  ...   \n",
       "7           -0.485471          -0.401745          -0.451992  ...   \n",
       "8           -0.485407          -0.401488          -0.451817  ...   \n",
       "9           -0.485152          -0.400841          -0.451415  ...   \n",
       "10          -0.484839          -0.400226          -0.450700  ...   \n",
       "11          -0.484194          -0.399798          -0.449367  ...   \n",
       "12          -0.483791          -0.399871          -0.447617  ...   \n",
       "13          -0.483532          -0.401289          -0.445099  ...   \n",
       "14          -0.484500          -0.404214          -0.442686  ...   \n",
       "15          -0.487697          -0.409798          -0.441493  ...   \n",
       "16          -0.494316          -0.419259          -0.442774  ...   \n",
       "17          -0.504792          -0.433619          -0.447570  ...   \n",
       "18          -0.519655          -0.453746          -0.456973  ...   \n",
       "19          -0.539207          -0.479420          -0.471829  ...   \n",
       "20          -0.562005          -0.508547          -0.491614  ...   \n",
       "21          -0.584912          -0.537370          -0.513733  ...   \n",
       "22          -0.604643          -0.562134          -0.534640  ...   \n",
       "23          -0.619537          -0.581003          -0.551776  ...   \n",
       "24          -0.629607          -0.594093          -0.564368  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         -0.438506        0.039162               15           -0.406088   \n",
       "1         -0.438374        0.039167               13           -0.406089   \n",
       "2         -0.438282        0.039280               11           -0.406108   \n",
       "3         -0.438187        0.039186                9           -0.406120   \n",
       "4         -0.438355        0.039192               12           -0.406149   \n",
       "5         -0.438380        0.039136               14           -0.406179   \n",
       "6         -0.438251        0.039272               10           -0.406244   \n",
       "7         -0.438019        0.039311                8           -0.406321   \n",
       "8         -0.437737        0.039458                7           -0.406438   \n",
       "9         -0.437359        0.039622                6           -0.406622   \n",
       "10        -0.436832        0.039755                5           -0.406894   \n",
       "11        -0.436128        0.039980                3           -0.407372   \n",
       "12        -0.435566        0.040061                2           -0.408238   \n",
       "13        -0.435503        0.039822                1           -0.409777   \n",
       "14        -0.436571        0.039396                4           -0.412510   \n",
       "15        -0.439859        0.038851               16           -0.417342   \n",
       "16        -0.446445        0.038213               17           -0.425297   \n",
       "17        -0.457253        0.037386               18           -0.437361   \n",
       "18        -0.473237        0.036452               19           -0.454489   \n",
       "19        -0.494642        0.035600               20           -0.476838   \n",
       "20        -0.520009        0.034763               21           -0.502897   \n",
       "21        -0.546005        0.033807               22           -0.529407   \n",
       "22        -0.568966        0.032676               23           -0.552878   \n",
       "23        -0.586812        0.031501               24           -0.571266   \n",
       "24        -0.599324        0.030379               25           -0.584303   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            -0.401326           -0.422308           -0.410291   \n",
       "1            -0.401337           -0.422308           -0.410281   \n",
       "2            -0.401344           -0.422320           -0.410297   \n",
       "3            -0.401365           -0.422320           -0.410314   \n",
       "4            -0.401391           -0.422332           -0.410334   \n",
       "5            -0.401427           -0.422350           -0.410379   \n",
       "6            -0.401489           -0.422376           -0.410406   \n",
       "7            -0.401576           -0.422424           -0.410474   \n",
       "8            -0.401677           -0.422488           -0.410570   \n",
       "9            -0.401837           -0.422644           -0.410699   \n",
       "10           -0.402113           -0.422911           -0.410876   \n",
       "11           -0.402581           -0.423411           -0.411253   \n",
       "12           -0.403446           -0.424289           -0.412033   \n",
       "13           -0.405110           -0.425847           -0.413603   \n",
       "14           -0.408194           -0.428690           -0.416783   \n",
       "15           -0.413430           -0.433553           -0.422502   \n",
       "16           -0.421739           -0.441411           -0.431677   \n",
       "17           -0.433974           -0.453191           -0.445224   \n",
       "18           -0.451154           -0.469708           -0.464011   \n",
       "19           -0.473681           -0.491173           -0.488328   \n",
       "20           -0.500291           -0.516190           -0.516646   \n",
       "21           -0.527644           -0.541773           -0.545338   \n",
       "22           -0.552090           -0.564502           -0.570420   \n",
       "23           -0.571449           -0.582367           -0.589678   \n",
       "24           -0.585328           -0.595080           -0.603025   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0            -0.427314         -0.413465         0.009817  \n",
       "1            -0.427319         -0.413467         0.009816  \n",
       "2            -0.427342         -0.413482         0.009819  \n",
       "3            -0.427360         -0.413496         0.009816  \n",
       "4            -0.427388         -0.413519         0.009814  \n",
       "5            -0.427416         -0.413550         0.009809  \n",
       "6            -0.427478         -0.413599         0.009804  \n",
       "7            -0.427566         -0.413672         0.009800  \n",
       "8            -0.427696         -0.413774         0.009800  \n",
       "9            -0.427903         -0.413941         0.009811  \n",
       "10           -0.428234         -0.414206         0.009832  \n",
       "11           -0.428804         -0.414684         0.009872  \n",
       "12           -0.429853         -0.415572         0.009933  \n",
       "13           -0.431597         -0.417187         0.009962  \n",
       "14           -0.434658         -0.420167         0.009961  \n",
       "15           -0.439777         -0.425321         0.009897  \n",
       "16           -0.447945         -0.433614         0.009799  \n",
       "17           -0.460091         -0.445968         0.009696  \n",
       "18           -0.477079         -0.463288         0.009556  \n",
       "19           -0.499086         -0.485821         0.009370  \n",
       "20           -0.524742         -0.512153         0.009180  \n",
       "21           -0.550751         -0.538983         0.009021  \n",
       "22           -0.573518         -0.562682         0.008819  \n",
       "23           -0.591040         -0.581160         0.008530  \n",
       "24           -0.603226         -0.594192         0.008207  \n",
       "\n",
       "[25 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7782794100389228\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"param_grid = {\\n    \\\"classifier__default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 3, 25)]\\n}\\ngrid_search = GridSearchCV(\\n    pipe,\\n    param_grid,\\n    scoring=make_scorer(log_loss, greater_is_better=False),\\n    return_train_score=True,\\n)\\ngrid_search.fit(X_train, y_train)\\ndisplay(pd.DataFrame(grid_search.cv_results_))\\nprint(grid_search.best_estimator_[-1].default_parameters[\\\"p\\\"])\";\n",
       "                var nbb_formatted_code = \"param_grid = {\\n    \\\"classifier__default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 3, 25)]\\n}\\ngrid_search = GridSearchCV(\\n    pipe,\\n    param_grid,\\n    scoring=make_scorer(log_loss, greater_is_better=False),\\n    return_train_score=True,\\n)\\ngrid_search.fit(X_train, y_train)\\ndisplay(pd.DataFrame(grid_search.cv_results_))\\nprint(grid_search.best_estimator_[-1].default_parameters[\\\"p\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"classifier__default_parameters\": [{\"m\": 0, \"p\": p} for p in np.logspace(-3, 3, 25)]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(log_loss, greater_is_better=False),\n",
    "    return_train_score=True,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "display(pd.DataFrame(grid_search.cv_results_))\n",
    "print(grid_search.best_estimator_[-1].default_parameters[\"p\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d9c0c",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e12ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4862555562991484\n",
      "0.8144927536231884\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"y_pred = grid_search.best_estimator_.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = grid_search.best_estimator_.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e5324",
   "metadata": {},
   "source": [
    "### Bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc8e5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "def loss(log_sigma2, X, y):\n",
    "    n_feat = X.shape[1]\n",
    "    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\n",
    "    diag_sigma2 = np.exp(log_sigma2)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=BayesianLogisticRegression()._jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = loss(xk, X, y)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = loss(xk, X, y)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb1653",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02a0bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([-1.])=332.4260968767515\n",
      "fun([-0.53290337])=331.8367963472044\n",
      "fun([-0.57278344])=331.8327571256535\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 331.8327571256535\n",
      "        x: [-5.728e-01]\n",
      "      nit: 3\n",
      "      jac: [-4.571e-06]\n",
      "     nfev: 4\n",
      "     njev: 4\n",
      " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
      "[1.77319578]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"res = minimize(\\n    loss,\\n    np.array([0.0]),\\n    args=(\\n        preprocessor.fit_transform(X_train).to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n",
       "                var nbb_formatted_code = \"res = minimize(\\n    loss,\\n    np.array([0.0]),\\n    args=(\\n        preprocessor.fit_transform(X_train).to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    loss,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        preprocessor.fit_transform(X_train).to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac,\n",
    "    callback=callback,\n",
    ")\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8ba6f",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c547a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False),\n",
       "                                                  [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;,\n",
       "                                                   &#x27;CabinCat&#x27;]),\n",
       "                                                 (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer())]),\n",
       "                                                  [&#x27;Age&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                                                &#x27;p&#x27;: array([1.77319578])},\n",
       "                                            optimize_kwargs={},\n",
       "                                            optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                                            prior_parameters={}))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False),\n",
       "                                                  [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;,\n",
       "                                                   &#x27;CabinCat&#x27;]),\n",
       "                                                 (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer())]),\n",
       "                                                  [&#x27;Age&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                                                &#x27;p&#x27;: array([1.77319578])},\n",
       "                                            optimize_kwargs={},\n",
       "                                            optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                                            prior_parameters={}))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                               sparse_output=False),\n",
       "                                 [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;, &#x27;CabinCat&#x27;]),\n",
       "                                (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer())]),\n",
       "                                 [&#x27;Age&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_features_wo_nan</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;, &#x27;CabinCat&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_features_w_nan</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Age&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;intercept&#x27;, &#x27;Fare&#x27;, &#x27;SibSp&#x27;, &#x27;Parch&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianLogisticRegression</label><div class=\"sk-toggleable__content\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                               &#x27;p&#x27;: array([1.77319578])},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={})</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('categorical_features_wo_nan',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse_output=False),\n",
       "                                                  ['Pclass', 'Sex', 'Embarked',\n",
       "                                                   'CabinCat']),\n",
       "                                                 ('numerical_features_w_nan',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer())]),\n",
       "                                                  ['Age'])])),\n",
       "                ('classifier',\n",
       "                 BayesianLogisticRegression(default_parameters={'m': 0,\n",
       "                                                                'p': array([1.77319578])},\n",
       "                                            optimize_kwargs={},\n",
       "                                            optimize_method='L-BFGS-B',\n",
       "                                            prior_parameters={}))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"pipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\n            \\\"classifier\\\",\\n            BayesianLogisticRegression(\\n                default_parameters={\\\"m\\\": 0, \\\"p\\\": 1 / np.exp(res.x)}\\n            ),\\n        ),\\n    ]\\n)\\npipe.fit(X_train, y_train)\";\n",
       "                var nbb_formatted_code = \"pipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\n            \\\"classifier\\\",\\n            BayesianLogisticRegression(\\n                default_parameters={\\\"m\\\": 0, \\\"p\\\": 1 / np.exp(res.x)}\\n            ),\\n        ),\\n    ]\\n)\\npipe.fit(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            BayesianLogisticRegression(\n",
    "                default_parameters={\"m\": 0, \"p\": 1 / np.exp(res.x)}\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5823562",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "708cb5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48622663532185695\n",
      "0.8146245059288537\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"y_pred = pipe.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = pipe.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769d024",
   "metadata": {},
   "source": [
    "## Multiple sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b99d86de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"cats = [\\\"intercept\\\", \\\"Pclass\\\", \\\"CabinCat\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"Age\\\", \\\"Fare\\\", \\\"SibSp\\\", \\\"Parch\\\"]\";\n",
       "                var nbb_formatted_code = \"cats = [\\n    \\\"intercept\\\",\\n    \\\"Pclass\\\",\\n    \\\"CabinCat\\\",\\n    \\\"Sex\\\",\\n    \\\"Embarked\\\",\\n    \\\"Age\\\",\\n    \\\"Fare\\\",\\n    \\\"SibSp\\\",\\n    \\\"Parch\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cats = [\n",
    "    \"intercept\",\n",
    "    \"Pclass\",\n",
    "    \"CabinCat\",\n",
    "    \"Sex\",\n",
    "    \"Embarked\",\n",
    "    \"Age\",\n",
    "    \"Fare\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958c65a",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "267ef593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"preprocessor = ColumnTransformer(\\n    [\\n        (\\n            \\\"categorical_features_wo_nan\\\",\\n            OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n            [\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\"],\\n        ),\\n        (\\n            \\\"numerical_features_w_nan\\\",\\n            Pipeline(\\n                [\\n                    (\\\"imputer\\\", SimpleImputer(strategy=\\\"mean\\\")),\\n                ]\\n            ),\\n            [\\\"Age\\\"],\\n        ),\\n    ],\\n    remainder=\\\"passthrough\\\",\\n)\\npreprocessor.set_output(transform=\\\"pandas\\\")\\nX_train_preprocessed = preprocessor.fit_transform(X_train)\\nX_test_preprocessed = preprocessor.transform(X_test)\";\n",
       "                var nbb_formatted_code = \"preprocessor = ColumnTransformer(\\n    [\\n        (\\n            \\\"categorical_features_wo_nan\\\",\\n            OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n            [\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\"],\\n        ),\\n        (\\n            \\\"numerical_features_w_nan\\\",\\n            Pipeline(\\n                [\\n                    (\\\"imputer\\\", SimpleImputer(strategy=\\\"mean\\\")),\\n                ]\\n            ),\\n            [\\\"Age\\\"],\\n        ),\\n    ],\\n    remainder=\\\"passthrough\\\",\\n)\\npreprocessor.set_output(transform=\\\"pandas\\\")\\nX_train_preprocessed = preprocessor.fit_transform(X_train)\\nX_test_preprocessed = preprocessor.transform(X_test)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"categorical_features_wo_nan\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            [\"Pclass\", \"Sex\", \"Embarked\", \"CabinCat\"],\n",
    "        ),\n",
    "        (\n",
    "            \"numerical_features_w_nan\",\n",
    "            Pipeline(\n",
    "                [\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                ]\n",
    "            ),\n",
    "            [\"Age\"],\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc63f684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"col_to_id = get_col_to_id(X_train_preprocessed.columns, cats)\";\n",
       "                var nbb_formatted_code = \"col_to_id = get_col_to_id(X_train_preprocessed.columns, cats)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_id = get_col_to_id(X_train_preprocessed.columns, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d024e1e",
   "metadata": {},
   "source": [
    "### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e9e2b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y, cols, col_to_id):\\n    n_feat = X.shape[1]\\n    sigma2_list = []\\n    for col in cols:\\n        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\\n    diag_sigma2 = np.array(sigma2_list)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y, cols, col_to_id):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y, cols, col_to_id)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y, cols, col_to_id)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y, cols, col_to_id):\\n    n_feat = X.shape[1]\\n    sigma2_list = []\\n    for col in cols:\\n        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\\n    diag_sigma2 = np.array(sigma2_list)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y, cols, col_to_id):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y, cols, col_to_id)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y, cols, col_to_id)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "def loss(log_sigma2, X, y, cols, col_to_id):\n",
    "    n_feat = X.shape[1]\n",
    "    sigma2_list = []\n",
    "    for col in cols:\n",
    "        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\n",
    "    diag_sigma2 = np.array(sigma2_list)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=BayesianLogisticRegression()._jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y, cols, col_to_id):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = loss(xk, X, y, cols, col_to_id)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = loss(xk, X, y, cols, col_to_id)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b16238",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce721f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([ 0.19908006 -0.00645629 -0.5824319   0.48615685 -0.33319885 -0.32497538\n",
      " -0.33263059 -0.13348474 -0.19914391])=330.649431777481\n",
      "fun([-0.10952821 -0.56641951 -2.0146628   1.29413587 -1.35170829 -0.97166226\n",
      " -1.17140039 -0.69732218 -1.07633126])=328.18644301463746\n",
      "fun([-0.20870325 -0.66710548 -2.1292034   1.86632182 -2.66342049 -2.09794486\n",
      " -2.44873395 -1.49537251 -1.96927078])=326.3148783970275\n",
      "fun([-0.72754747 -0.08329768 -1.90828386  2.32851475 -4.09858538 -5.05217582\n",
      " -6.58528877 -3.53085422 -4.70208423])=323.1639341857672\n",
      "fun([-0.82310784 -0.21649414 -1.80288469  2.04408138 -4.55648261 -6.60399522\n",
      " -8.77575918 -3.20516568 -5.81425898])=321.525071688462\n",
      "fun([ -1.17654427  -0.97686696  -1.02296124   2.14762705  -4.49141211\n",
      "  -7.49788387 -11.92531574  -2.08258697  -6.65590791])=321.1247470048541\n",
      "fun([ -1.05021965  -0.45453269  -1.32611211   1.44876611  -3.77947009\n",
      "  -6.31674656 -10.81079955  -1.51337954  -5.73078498])=320.1431379922154\n",
      "fun([ -1.21408306  -0.28105439  -1.49113179   1.35160038  -3.11329288\n",
      "  -6.47007107 -11.13893981  -1.51033991  -5.81323691])=319.7909654521394\n",
      "fun([ -1.21408306  -0.28105439  -1.49113179   1.35160038  -3.11329288\n",
      "  -6.47007107 -11.13893981  -1.51033991  -5.81323691])=319.79003040040675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"col_to_id = get_col_to_id(X_train_preprocessed.columns, cats)\\nres = minimize(\\n    loss,\\n    np.array([0.0] * len(cats)),\\n    args=(\\n        X_train_preprocessed.to_numpy(),\\n        y_train.to_numpy(),\\n        X_train_preprocessed.columns,\\n        col_to_id,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n",
       "                var nbb_formatted_code = \"col_to_id = get_col_to_id(X_train_preprocessed.columns, cats)\\nres = minimize(\\n    loss,\\n    np.array([0.0] * len(cats)),\\n    args=(\\n        X_train_preprocessed.to_numpy(),\\n        y_train.to_numpy(),\\n        X_train_preprocessed.columns,\\n        col_to_id,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_id = get_col_to_id(X_train_preprocessed.columns, cats)\n",
    "res = minimize(\n",
    "    loss,\n",
    "    np.array([0.0] * len(cats)),\n",
    "    args=(\n",
    "        X_train_preprocessed.to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "        X_train_preprocessed.columns,\n",
    "        col_to_id,\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac,\n",
    "    callback=callback,\n",
    ")\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cbcd9",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_parameters = {}\n",
    "\n",
    "for col in X_train_preprocessed.columns:\n",
    "    prior_parameters[col] = {\"m\": 0, \"p\": (1 / np.exp(res.x))[col_to_id[col]]}\n",
    "\n",
    "model = BayesianLogisticRegression(prior_parameters=prior_parameters)\n",
    "model.fit(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c5079",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9e9c2",
   "metadata": {},
   "source": [
    "___\n",
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba1c154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun={intermediate_result.fun}\\\")\";\n",
       "                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun={intermediate_result.fun}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun={intermediate_result.fun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "795cacd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun=416.015700488111\n",
      "fun=415.7457521852242\n",
      "fun=415.7028985462279\n",
      "fun=415.70233681909383\n",
      "fun=415.70095393097137\n",
      "fun=415.7002621046415\n",
      "fun=415.6994301589399\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        preprocessor.fit_transform(X).to_numpy(),\\n        y.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_formatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        preprocessor.fit_transform(X).to_numpy(),\\n        y.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    func_with_bias,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        preprocessor.fit_transform(X).to_numpy(),\n",
    "        y.to_numpy(),\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac_func_with_bias,\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f4076e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: ABNORMAL_TERMINATION_IN_LNSRCH\n",
       "  success: False\n",
       "   status: 2\n",
       "      fun: 415.7002601196556\n",
       "        x: [-3.364e-01]\n",
       "      nit: 7\n",
       "      jac: [ 7.452e-05]\n",
       "     nfev: 64\n",
       "     njev: 64\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"res\";\n",
       "                var nbb_formatted_code = \"res\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f60875b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.39996636])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"1/np.exp(res.x)\";\n",
       "                var nbb_formatted_code = \"1 / np.exp(res.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cc7c7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"X1 = preprocessor.fit_transform(X).to_numpy()\\ny1 = y.to_numpy()\";\n",
       "                var nbb_formatted_code = \"X1 = preprocessor.fit_transform(X).to_numpy()\\ny1 = y.to_numpy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1 = preprocessor.fit_transform(X).to_numpy()\n",
    "y1 = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "def9650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun=917.3936949762424\n",
      "fun=909.2815416894933\n",
      "fun=908.7238866764843\n",
      "fun=908.5830502425201\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 182;\n",
       "                var nbb_unformatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        X1,\\n        y1,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_formatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        X1,\\n        y1,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    func_with_bias,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        X1,\n",
    "        y1,\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac_func_with_bias,\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c404cc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 908.5830502425201\n",
       "        x: [-3.176e+00]\n",
       "      nit: 4\n",
       "      jac: [ 6.802e-06]\n",
       "     nfev: 8\n",
       "     njev: 8\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 183;\n",
       "                var nbb_unformatted_code = \"res\";\n",
       "                var nbb_formatted_code = \"res\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7db450de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04176261])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 184;\n",
       "                var nbb_unformatted_code = \"np.exp(res.x)\";\n",
       "                var nbb_formatted_code = \"np.exp(res.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.exp(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2e5c6bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianRidge(alpha_1=1000000.0, alpha_2=1000000.0, fit_intercept=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianRidge</label><div class=\"sk-toggleable__content\"><pre>BayesianRidge(alpha_1=1000000.0, alpha_2=1000000.0, fit_intercept=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianRidge(alpha_1=1000000.0, alpha_2=1000000.0, fit_intercept=False)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 185;\n",
       "                var nbb_unformatted_code = \"model = BayesianRidge(\\n    alpha_1=1 / 1 * 1e6,\\n    alpha_2=1e6,\\n    lambda_1=1e-6,\\n    lambda_2=1e-6,\\n    fit_intercept=False,\\n)\\nmodel.fit(X1, y1)\";\n",
       "                var nbb_formatted_code = \"model = BayesianRidge(\\n    alpha_1=1 / 1 * 1e6,\\n    alpha_2=1e6,\\n    lambda_1=1e-6,\\n    lambda_2=1e-6,\\n    fit_intercept=False,\\n)\\nmodel.fit(X1, y1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BayesianRidge(\n",
    "    alpha_1=1 / 1 * 1e6,\n",
    "    alpha_2=1e6,\n",
    "    lambda_1=1e-6,\n",
    "    lambda_2=1e-6,\n",
    "    fit_intercept=False,\n",
    ")\n",
    "model.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0d915f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.189134422067746"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 191;\n",
       "                var nbb_unformatted_code = \"np.log(1 / model.lambda_)\";\n",
       "                var nbb_formatted_code = \"np.log(1 / model.lambda_)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(1 / model.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54a8bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.2375065318942\n",
      "447.80239593850894\n",
      "426.7367949669429\n",
      "417.1934379524614\n",
      "416.017499735816\n",
      "419.8408348902286\n",
      "426.16091279686424\n",
      "433.5910798852872\n",
      "441.47220323542643\n",
      "449.58914179809045\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"for log_sigma2 in [\\n    -4,\\n    -3,-2,-1,0,1,2,3,4,5,\\n]:\\n    print(\\n        func_with_bias(\\n            np.array([log_sigma2]),\\n            X1,\\n            y1,\\n        )\\n    )\";\n",
       "                var nbb_formatted_code = \"for log_sigma2 in [\\n    -4,\\n    -3,\\n    -2,\\n    -1,\\n    0,\\n    1,\\n    2,\\n    3,\\n    4,\\n    5,\\n]:\\n    print(\\n        func_with_bias(\\n            np.array([log_sigma2]),\\n            X1,\\n            y1,\\n        )\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for log_sigma2 in [\n",
    "    -4,\n",
    "    -3,\n",
    "    -2,\n",
    "    -1,\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "]:\n",
    "    print(\n",
    "        func_with_bias(\n",
    "            np.array([log_sigma2]),\n",
    "            X1,\n",
    "            y1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3620fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.81102697e-22])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"1/np.exp(res.x)\";\n",
       "                var nbb_formatted_code = \"1 / np.exp(res.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 / np.exp(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f9f6a8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 329.1250092965352\n",
       "        x: [ 4.932e+01]\n",
       "      nit: 3\n",
       "      jac: [ 0.000e+00]\n",
       "     nfev: 6\n",
       "     njev: 6\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 127;\n",
       "                var nbb_unformatted_code = \"res\";\n",
       "                var nbb_formatted_code = \"res\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "380eb6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17095.442744711374"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 130;\n",
       "                var nbb_unformatted_code = \"loss(np.array([1.0] * 21), preprocessor.fit_transform(X), y, np.array([2.0] * 21))\";\n",
       "                var nbb_formatted_code = \"loss(np.array([1.0] * 21), preprocessor.fit_transform(X), y, np.array([2.0] * 21))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss(np.array([1.0] * 21), preprocessor.fit_transform(X), y, np.array([2.0] * 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea00ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00532150e-01, 0.00000000e+00, 0.00000000e+00, 1.23977661e-04,\n",
       "        4.08172607e-04, 0.00000000e+00, 0.00000000e+00, 5.32150269e-04,\n",
       "        0.00000000e+00, 4.57763672e-05, 0.00000000e+00, 4.86373901e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.62939453e-04, 4.86373901e-04,\n",
       "        1.06430054e-03],\n",
       "       [0.00000000e+00, 5.05719185e-01, 0.00000000e+00, 4.68254089e-04,\n",
       "        5.24997711e-03, 9.55581665e-04, 0.00000000e+00, 4.76360321e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.10623169e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.90856171e-03, 6.50882721e-03, 4.06169891e-03,\n",
       "        7.76195526e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 5.09937286e-01, 4.88662720e-03,\n",
       "        5.05065918e-03, 6.29043579e-03, 1.90734863e-05, 3.62777710e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.52587891e-05, 0.00000000e+00, 9.57489014e-04,\n",
       "        0.00000000e+00, 8.96453857e-03, 1.07955933e-02, 3.83377075e-03,\n",
       "        1.11389160e-02],\n",
       "       [1.23977661e-04, 4.69207764e-04, 4.88662720e-03, 5.05477905e-01,\n",
       "        0.00000000e+00, 1.94549561e-03, 0.00000000e+00, 3.53145599e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.23977661e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.71661377e-05, 9.56535339e-04,\n",
       "        0.00000000e+00, 4.38213348e-03, 9.17148590e-03, 3.40652466e-03,\n",
       "        6.51168823e-03],\n",
       "       [4.11987305e-04, 5.25283813e-03, 5.05065918e-03, 0.00000000e+00,\n",
       "        5.10711670e-01, 5.30242920e-03, 2.28881836e-05, 5.39398193e-03,\n",
       "        0.00000000e+00, 4.57763672e-05, 0.00000000e+00, 3.66210938e-04,\n",
       "        0.00000000e+00, 1.52587891e-05, 8.01086426e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.49096680e-03, 8.90350342e-03, 4.98199463e-03,\n",
       "        1.34582520e-02],\n",
       "       [0.00000000e+00, 9.55581665e-04, 6.28852844e-03, 1.94549561e-03,\n",
       "        5.29861450e-03, 5.07244110e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.24411011e-03, 4.97627258e-03, 1.77860260e-03,\n",
       "        9.10949707e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.90734863e-05, 0.00000000e+00,\n",
       "        1.90734863e-05, 0.00000000e+00, 5.00019073e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.90734863e-05, 4.38690186e-05, 7.62939453e-05,\n",
       "        1.90734863e-05],\n",
       "       [5.34057617e-04, 4.76074219e-03, 3.62396240e-03, 3.53240967e-03,\n",
       "        5.38635254e-03, 0.00000000e+00, 0.00000000e+00, 5.08922577e-01,\n",
       "        0.00000000e+00, 4.57763672e-05, 0.00000000e+00, 4.88281250e-04,\n",
       "        0.00000000e+00, 1.52587891e-05, 8.08715820e-04, 9.53674316e-04,\n",
       "        0.00000000e+00, 6.60705566e-03, 1.30462646e-02, 6.52694702e-03,\n",
       "        1.08337402e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [4.54187393e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.54187393e-05, 0.00000000e+00, 0.00000000e+00, 4.54187393e-05,\n",
       "        0.00000000e+00, 5.00045419e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.81555748e-04, 0.00000000e+00,\n",
       "        9.08374786e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00000000e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.19209290e-07, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [4.86373901e-04, 0.00000000e+00, 0.00000000e+00, 1.23262405e-04,\n",
       "        3.63111496e-04, 0.00000000e+00, 0.00000000e+00, 4.86373901e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00486374e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.80787659e-04, 4.86373901e-04,\n",
       "        9.72986221e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.66893005e-05, 0.00000000e+00,\n",
       "        1.66893005e-05, 0.00000000e+00, 0.00000000e+00, 1.66893005e-05,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.00016689e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00135803e-04, 0.00000000e+00,\n",
       "        1.66893005e-05],\n",
       "       [0.00000000e+00, 8.10503960e-04, 0.00000000e+00, 1.66893005e-05,\n",
       "        7.93874264e-04, 0.00000000e+00, 0.00000000e+00, 8.10503960e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00810504e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.44267082e-03, 1.16252899e-03,\n",
       "        8.10503960e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 9.55611467e-04, 9.55611467e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.55611467e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00955611e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.00203061e-03, 4.53889370e-05,\n",
       "        9.55611467e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [0.00000000e+00, 4.90951538e-03, 8.96453857e-03, 4.38308716e-03,\n",
       "        9.49096680e-03, 7.24792480e-03, 1.90734863e-05, 6.60705566e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.13874054e-01, 1.37634277e-02, 6.69097900e-03,\n",
       "        1.71203613e-02],\n",
       "       [9.76562500e-04, 6.59179688e-03, 1.09863281e-02, 9.27734375e-03,\n",
       "        8.78906250e-03, 4.88281250e-03, 0.00000000e+00, 1.31835938e-02,\n",
       "        0.00000000e+00, 2.44140625e-04, 0.00000000e+00, 4.88281250e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.46484375e-03, 1.95312500e-03,\n",
       "        0.00000000e+00, 1.36718750e-02, 5.32714844e-01, 1.02539062e-02,\n",
       "        2.17285156e-02],\n",
       "       [4.84466553e-04, 4.06265259e-03, 3.83377075e-03, 3.40652466e-03,\n",
       "        4.97436523e-03, 1.77764893e-03, 7.62939453e-05, 6.52694702e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.84466553e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.16348267e-03, 4.57763672e-05,\n",
       "        0.00000000e+00, 6.69097900e-03, 1.00784302e-02, 5.12523651e-01,\n",
       "        9.38415527e-03],\n",
       "       [1.06430054e-03, 7.76290894e-03, 1.11351013e-02, 6.50978088e-03,\n",
       "        1.34487152e-02, 9.10949707e-03, 1.90734863e-05, 1.08318329e-02,\n",
       "        0.00000000e+00, 8.96453857e-05, 0.00000000e+00, 9.72747803e-04,\n",
       "        0.00000000e+00, 1.52587891e-05, 8.08715820e-04, 9.55581665e-04,\n",
       "        0.00000000e+00, 1.71165466e-02, 2.17056274e-02, 9.38034058e-03,\n",
       "        5.27763367e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"approx_fprime(\\n    np.array([1.0] * 21),\\n    jac,\\n    1.4901161193847656e-08,\\n    *(preprocessor.fit_transform(X), y, np.array([2.0] * 21)),\\n)\";\n",
       "                var nbb_formatted_code = \"approx_fprime(\\n    np.array([1.0] * 21),\\n    jac,\\n    1.4901161193847656e-08,\\n    *(preprocessor.fit_transform(X), y, np.array([2.0] * 21)),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "approx_fprime(\n",
    "    np.array([1.0] * 21),\n",
    "    jac,\n",
    "    1.4901161193847656e-08,\n",
    "    *(preprocessor.fit_transform(X), y, np.array([2.0] * 21)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c6074a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"r = 1\\nbeta = np.array([1.0] * 21)\\nm = np.array([0.0] * 21)\\nq = 1/np.array([2.0] * 21)\\n\\nweights = np.ones(y.shape)\\nweights[y == 0] = 1 / r\\n\\nlinear_pred = np.dot(preprocessor.fit_transform(X), beta)\\ny_pred = BayesianLogisticRegression().link._inv_link(linear_pred)\\n\\nhess = np.diag(q) + np.matmul(\\n    np.matmul(\\n        preprocessor.fit_transform(X).T.to_numpy(),\\n        np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n    ),\\n    preprocessor.fit_transform(X).to_numpy(),\\n)\";\n",
       "                var nbb_formatted_code = \"r = 1\\nbeta = np.array([1.0] * 21)\\nm = np.array([0.0] * 21)\\nq = 1 / np.array([2.0] * 21)\\n\\nweights = np.ones(y.shape)\\nweights[y == 0] = 1 / r\\n\\nlinear_pred = np.dot(preprocessor.fit_transform(X), beta)\\ny_pred = BayesianLogisticRegression().link._inv_link(linear_pred)\\n\\nhess = np.diag(q) + np.matmul(\\n    np.matmul(\\n        preprocessor.fit_transform(X).T.to_numpy(),\\n        np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n    ),\\n    preprocessor.fit_transform(X).to_numpy(),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = 1\n",
    "beta = np.array([1.0] * 21)\n",
    "m = np.array([0.0] * 21)\n",
    "q = 1 / np.array([2.0] * 21)\n",
    "\n",
    "weights = np.ones(y.shape)\n",
    "weights[y == 0] = 1 / r\n",
    "\n",
    "linear_pred = np.dot(preprocessor.fit_transform(X), beta)\n",
    "y_pred = BayesianLogisticRegression().link._inv_link(linear_pred)\n",
    "\n",
    "hess = np.diag(q) + np.matmul(\n",
    "    np.matmul(\n",
    "        preprocessor.fit_transform(X).T.to_numpy(),\n",
    "        np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\n",
    "    ),\n",
    "    preprocessor.fit_transform(X).to_numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abef22ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00531936e-01, 0.00000000e+00, 0.00000000e+00, 1.23386632e-04,\n",
       "        4.08549642e-04, 1.39942169e-09, 0.00000000e+00, 5.31934874e-04,\n",
       "        0.00000000e+00, 4.53958078e-05, 2.12966051e-08, 4.86518289e-04,\n",
       "        8.30318036e-10, 3.97397670e-11, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.03952402e-11, 7.62715736e-04, 4.86534540e-04,\n",
       "        1.06386289e-03],\n",
       "       [0.00000000e+00, 5.05719221e-01, 0.00000000e+00, 4.68690204e-04,\n",
       "        5.25053032e-03, 9.55623353e-04, 3.99680289e-15, 4.76359717e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.87960758e-12, 3.68594044e-14, 8.10555835e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.90866468e-03, 6.50974333e-03, 4.06259720e-03,\n",
       "        7.76258607e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 5.09934095e-01, 4.88596690e-03,\n",
       "        5.04812852e-03, 6.28940733e-03, 1.91256391e-05, 3.62556246e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.67011429e-05, 1.02875708e-10, 9.55616988e-04,\n",
       "        0.00000000e+00, 8.96177719e-03, 1.07949735e-02, 3.83242776e-03,\n",
       "        1.11363759e-02],\n",
       "       [1.23386632e-04, 4.68690204e-04, 4.88596690e-03, 5.05478044e-01,\n",
       "        0.00000000e+00, 1.94625626e-03, 1.01257133e-08, 3.53177736e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.06558358e-09, 1.23379735e-04,\n",
       "        7.88604514e-10, 3.96831457e-11, 1.67011436e-05, 9.55616988e-04,\n",
       "        0.00000000e+00, 4.38233898e-03, 9.17135383e-03, 3.40562015e-03,\n",
       "        6.51163716e-03],\n",
       "       [4.08549642e-04, 5.25053032e-03, 5.04812852e-03, 0.00000000e+00,\n",
       "        5.10707208e-01, 5.29877583e-03, 1.91155134e-05, 5.38931714e-03,\n",
       "        0.00000000e+00, 4.53958078e-05, 1.52310215e-08, 3.63138554e-04,\n",
       "        4.35931291e-11, 1.67011430e-05, 7.93854794e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.48810291e-03, 8.89607869e-03, 4.97593935e-03,\n",
       "        1.34511877e-02],\n",
       "       [1.39942169e-09, 9.55623353e-04, 6.28940733e-03, 1.94625626e-03,\n",
       "        5.29877583e-03, 5.07245032e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.04485811e-10, 5.88289195e-10,\n",
       "        3.29736238e-12, 9.52571355e-14, 2.22044605e-16, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.24503069e-03, 4.97701225e-03, 1.77953626e-03,\n",
       "        9.11045725e-03],\n",
       "       [0.00000000e+00, 3.99680289e-15, 1.91256391e-05, 1.01257133e-08,\n",
       "        1.91155134e-05, 0.00000000e+00, 5.00019126e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.22044605e-15, 2.22044605e-15, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.91256391e-05, 4.37221281e-05, 7.64615865e-05,\n",
       "        1.91153966e-05],\n",
       "       [5.31934874e-04, 4.76359717e-03, 3.62556246e-03, 3.53177736e-03,\n",
       "        5.38931714e-03, 0.00000000e+00, 0.00000000e+00, 5.08921094e-01,\n",
       "        0.00000000e+00, 4.53958078e-05, 2.04921193e-08, 4.86517700e-04,\n",
       "        8.28900281e-10, 1.67011826e-05, 8.10555938e-04, 9.55616988e-04,\n",
       "        0.00000000e+00, 6.60628556e-03, 1.30466981e-02, 6.52556165e-03,\n",
       "        1.08332522e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [4.53958078e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.53958078e-05, 0.00000000e+00, 0.00000000e+00, 4.53958078e-05,\n",
       "        0.00000000e+00, 5.00045396e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.81583232e-04, 0.00000000e+00,\n",
       "        9.07916155e-05],\n",
       "       [2.12966051e-08, 0.00000000e+00, 0.00000000e+00, 6.06558358e-09,\n",
       "        1.52310215e-08, 8.04485811e-10, 0.00000000e+00, 2.04921193e-08,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00000021e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.61753451e-07, 1.63157554e-08,\n",
       "        3.48082449e-08],\n",
       "       [4.86518289e-04, 0.00000000e+00, 0.00000000e+00, 1.23379735e-04,\n",
       "        3.63138554e-04, 5.88289195e-10, 0.00000000e+00, 4.86517700e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00486518e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.80856252e-04, 4.86518186e-04,\n",
       "        9.73035602e-04],\n",
       "       [8.30318036e-10, 1.87960758e-12, 0.00000000e+00, 7.88604514e-10,\n",
       "        4.35931291e-11, 3.29736238e-12, 0.00000000e+00, 8.28900281e-10,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000001e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.35883917e-08, 3.84439147e-11,\n",
       "        7.91832599e-10],\n",
       "       [3.97397670e-11, 3.68594044e-14, 1.67011429e-05, 3.96831457e-11,\n",
       "        1.67011430e-05, 9.52571355e-14, 2.22044605e-15, 1.67011826e-05,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.00016701e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00207582e-04, 1.06137321e-13,\n",
       "        1.67012203e-05],\n",
       "       [0.00000000e+00, 8.10555835e-04, 1.02875708e-10, 1.67011436e-05,\n",
       "        7.93854794e-04, 2.22044605e-16, 2.22044605e-15, 8.10555938e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00810556e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.44265761e-03, 1.16249465e-03,\n",
       "        8.10555834e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 9.55616988e-04, 9.55616988e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.55616988e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00955617e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.00202559e-03, 4.53958077e-05,\n",
       "        9.55616988e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [1.03952402e-11, 4.90866468e-03, 8.96177719e-03, 4.38233898e-03,\n",
       "        9.48810291e-03, 7.24503069e-03, 1.91256391e-05, 6.60628556e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.13870442e-01, 1.37598269e-02, 6.68713450e-03,\n",
       "        1.71160880e-02],\n",
       "       [7.62715736e-04, 6.50974333e-03, 1.07949735e-02, 9.17135383e-03,\n",
       "        8.89607869e-03, 4.97701225e-03, 4.37221281e-05, 1.30466981e-02,\n",
       "        0.00000000e+00, 1.81583232e-04, 2.61753451e-07, 5.80856252e-04,\n",
       "        1.35883917e-08, 1.00207582e-04, 1.44265761e-03, 2.00202559e-03,\n",
       "        0.00000000e+00, 1.37598269e-02, 5.32815243e-01, 1.00782545e-02,\n",
       "        2.17071454e-02],\n",
       "       [4.86534540e-04, 4.06259720e-03, 3.83242776e-03, 3.40562015e-03,\n",
       "        4.97593935e-03, 1.77953626e-03, 7.64615865e-05, 6.52556165e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.63157554e-08, 4.86518186e-04,\n",
       "        3.84439147e-11, 1.06137321e-13, 1.16249465e-03, 4.53958077e-05,\n",
       "        0.00000000e+00, 6.68713450e-03, 1.00782545e-02, 5.12521273e-01,\n",
       "        9.38123843e-03],\n",
       "       [1.06386289e-03, 7.76258607e-03, 1.11363759e-02, 6.51163716e-03,\n",
       "        1.34511877e-02, 9.11045725e-03, 1.91153966e-05, 1.08332522e-02,\n",
       "        0.00000000e+00, 9.07916155e-05, 3.48082449e-08, 9.73035602e-04,\n",
       "        7.91832599e-10, 1.67012203e-05, 8.10555834e-04, 9.55616988e-04,\n",
       "        0.00000000e+00, 1.71160880e-02, 2.17071454e-02, 9.38123843e-03,\n",
       "        5.27765661e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"hess\";\n",
       "                var nbb_formatted_code = \"hess\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e5ce16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01217139e-01, 1.00152907e-01, 2.69488563e+02, 4.15603116e+00,\n",
       "       3.49809805e-02])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 52;\n",
       "                var nbb_unformatted_code = \"1/np.exp(np.array([2.29048718, 2.30105719, -5.59652595, -1.42456057, 3.35295078]))\";\n",
       "                var nbb_formatted_code = \"1 / np.exp(np.array([2.29048718, 2.30105719, -5.59652595, -1.42456057, 3.35295078]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 / np.exp(np.array([2.29048718, 2.30105719, -5.59652595, -1.42456057, 3.35295078]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360bc83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cr_model_research",
   "language": "python",
   "name": "cr_model_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
