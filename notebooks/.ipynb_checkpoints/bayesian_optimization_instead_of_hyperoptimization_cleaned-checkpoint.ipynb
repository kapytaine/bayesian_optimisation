{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3911da83",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Packages</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Link\" data-toc-modified-id=\"Link-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Link</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#Logit\" data-toc-modified-id=\"Logit-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Logit</a></span></li></ul></li><li><span><a href=\"#Bayesian-GLM\" data-toc-modified-id=\"Bayesian-GLM-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Bayesian GLM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#BayesianLogisticRegression\" data-toc-modified-id=\"BayesianLogisticRegression-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>BayesianLogisticRegression</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"#Bayesian-loss\" data-toc-modified-id=\"Bayesian-loss-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Bayesian loss</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read\" data-toc-modified-id=\"Read-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Read</a></span></li><li><span><a href=\"#Split-train-test\" data-toc-modified-id=\"Split-train-test-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Split train test</a></span></li></ul></li><li><span><a href=\"#Studies\" data-toc-modified-id=\"Studies-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Studies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-sigma2\" data-toc-modified-id=\"Single-sigma2-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Single sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.1.1.1\"><span class=\"toc-item-num\">4.1.1.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.1.1.2\"><span class=\"toc-item-num\">4.1.1.2&nbsp;&nbsp;</span>Test</a></span></li></ul></li><li><span><a href=\"#Bayesian-optimisation\" data-toc-modified-id=\"Bayesian-optimisation-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Bayesian optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.1.2.1\"><span class=\"toc-item-num\">4.1.2.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-4.1.2.2\"><span class=\"toc-item-num\">4.1.2.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.1.2.3\"><span class=\"toc-item-num\">4.1.2.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li><li><span><a href=\"#Multiple-sigma2\" data-toc-modified-id=\"Multiple-sigma2-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Multiple sigma2</a></span></li></ul></li><li><span><a href=\"#Bayesian-optimization\" data-toc-modified-id=\"Bayesian-optimization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Bayesian optimization</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95e967",
   "metadata": {},
   "source": [
    "___\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8298c767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nfrom __future__ import annotations\\n\\nfrom abc import ABCMeta, abstractmethod\\nimport logging\\nfrom typing import Any, Tuple, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy.special import expit, logit\\nfrom scipy import optimize\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.metrics import accuracy_score, log_loss\\nfrom sklearn.linear_model import (\\n    BayesianRidge,\\n    LinearRegression,\\n    LogisticRegression,\\n    Ridge,\\n)\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.base import BaseEstimator\\nfrom scipy.optimize import approx_fprime, minimize\\nfrom scipy.special import expit\\nfrom scipy.stats import multivariate_normal\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nfrom __future__ import annotations\\n\\nfrom abc import ABCMeta, abstractmethod\\nimport logging\\nfrom typing import Any, Tuple, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy.special import expit, logit\\nfrom scipy import optimize\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.metrics import accuracy_score, log_loss\\nfrom sklearn.linear_model import (\\n    BayesianRidge,\\n    LinearRegression,\\n    LogisticRegression,\\n    Ridge,\\n)\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.base import BaseEstimator\\nfrom scipy.optimize import approx_fprime, minimize\\nfrom scipy.special import expit\\nfrom scipy.stats import multivariate_normal\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import logging\n",
    "from typing import Any, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit, logit\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import approx_fprime, minimize\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a63f4e",
   "metadata": {},
   "source": [
    "___\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda78f10",
   "metadata": {},
   "source": [
    "## Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c24f2",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b8ffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"class Link(metaclass=ABCMeta):\\n    @abstractmethod\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return self._inv_jac_link(self._inv_link(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return np.multiply(\\n            -self._hess_link(self._inv_link(x)),\\n            np.power(self._jac_inv_link(x), 3),\\n        )\\n\\n    @abstractmethod\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        This method has been chosen to avoid numerical error.\\n        When working with _jac_link, it may be close to 1/0 close to the\\n        optimum resulting in an zero division error.\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\";\n",
       "                var nbb_formatted_code = \"class Link(metaclass=ABCMeta):\\n    @abstractmethod\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return self._inv_jac_link(self._inv_link(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return np.multiply(\\n            -self._hess_link(self._inv_link(x)),\\n            np.power(self._jac_inv_link(x), 3),\\n        )\\n\\n    @abstractmethod\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        This method has been chosen to avoid numerical error.\\n        When working with _jac_link, it may be close to 1/0 close to the\\n        optimum resulting in an zero division error.\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Link(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def _inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self._inv_jac_link(self._inv_link(x))\n",
    "\n",
    "    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.multiply(\n",
    "            -self._hess_link(self._inv_link(x)),\n",
    "            np.power(self._jac_inv_link(x), 3),\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        This method has been chosen to avoid numerical error.\n",
    "        When working with _jac_link, it may be close to 1/0 close to the\n",
    "        optimum resulting in an zero division error.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _hess_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f111bf5",
   "metadata": {},
   "source": [
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811f567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"class Logit(Link):\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x)\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\\n\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        return logit(y)\\n\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        return y * (1 - y)\\n\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        return (2 * y - 1) / (y * (1 - y)) ** 2\";\n",
       "                var nbb_formatted_code = \"class Logit(Link):\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x)\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\\n\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        return logit(y)\\n\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        return y * (1 - y)\\n\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        return (2 * y - 1) / (y * (1 - y)) ** 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Logit(Link):\n",
    "    def _inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x)\n",
    "\n",
    "    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x) * (1 - expit(x))\n",
    "\n",
    "    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\n",
    "\n",
    "    def _link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return logit(y)\n",
    "\n",
    "    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return y * (1 - y)\n",
    "\n",
    "    def _hess_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return (2 * y - 1) / (y * (1 - y)) ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec15fa",
   "metadata": {},
   "source": [
    "## Bayesian GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0861d",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff1ffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\\n    def __init__(\\n        self,\\n        prior_parameters: dict = {},\\n        default_parameters: dict = {\\\"m\\\": 0, \\\"p\\\": 5},\\n        optimize_kwargs: dict = {},\\n        optimize_method: str = \\\"L-BFGS-B\\\",\\n    ) -> None:\\n        self.prior_parameters = prior_parameters\\n        self.default_parameters = default_parameters\\n        self.optimize_kwargs = optimize_kwargs\\n        self.optimize_method = optimize_method\\n\\n    def _check_distribution_params(self, params: dict) -> None:\\n        if not (\\n            isinstance(params, dict)\\n            and set(params.keys()) == set([\\\"m\\\", \\\"p\\\"])\\n            and (\\n                isinstance(params.get(\\\"m\\\"), Union[int, float])  # type: ignore\\n                and not np.isnan(params.get(\\\"m\\\"))  # type: ignore\\n            )\\n            and (\\n                isinstance(params.get(\\\"p\\\"), Union[int, float])  # type: ignore\\n                and params.get(\\\"p\\\") >= 0  # type: ignore\\n            )\\n        ):\\n            raise ValueError\\n\\n    @abstractmethod\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\\n        m = np.array([])\\n        p = np.array([])\\n        for col in feat_names:\\n            m = np.append(\\n                m,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"],\\n            )\\n            p = np.append(\\n                p,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"],\\n            )\\n        return m, p\\n\\n    def _update_m_p(\\n        self,\\n        res: optimize.OptimizeResult,\\n        X: pd.DataFrame,\\n        y: pd.Series,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> None:\\n        # get new updates values\\n        m_updated = res.x\\n        p_updated = self._diag_hess(\\n            res.x,\\n            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n        )\\n        feat_names = X.columns.to_list()\\n\\n        # check hessian positiveness\\n        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\\n            raise NotPositiveHessian\\n\\n        # update parameters\\n        self.posterior_parameters_ = dict(\\n            self.prior_parameters,\\n            **{\\n                col: {\\\"m\\\": m, \\\"p\\\": p}\\n                for col, m, p in zip(feat_names, m_updated, p_updated)\\n            },\\n        )\\n\\n        # check parameters\\n        for _, v in self.posterior_parameters_.items():\\n            self._check_distribution_params(v)\\n\\n    @abstractmethod\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\\n        logging.info(f\\\"Shape of the training dataset: {X.shape}.\\\")\\n\\n        # get parameter prior distribution parameters (mean and precision)\\n        m, p = self._get_prior_m_p(X.columns.to_list())\\n        logging.info(\\\"Prior distribution parameters obtained.\\\")\\n\\n        # optimize\\n        res = optimize.minimize(\\n            fun=self._loss,\\n            x0=m,\\n            jac=self._jac,\\n            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n            method=self.optimize_method,\\n            **self.optimize_kwargs,\\n        )\\n\\n        # log optimization result\\n        initial_loss = self._loss(\\n            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\\n        )\\n        log_message = (\\n            f\\\"initial fun: {initial_loss}\\\\n\\\"\\n            \\\"The OptimizeResult instance is:\\\\n\\\"\\n            f\\\"{res}\\\"\\n        )\\n\\n        if res.success:\\n            logging.info(f\\\"Log likelihood optimized successfully.\\\\n{log_message}\\\")\\n\\n            # calculate parameter posterior distribution parameters\\n            self._update_m_p(res, X, y, m, p, **kwargs)\\n            logging.info(\\\"Posterior distribution parameters computed.\\\")\\n        else:\\n            logging.warning(f\\\"Log likelihood optimization failed.\\\\n{log_message}\\\")\\n            if res.fun < initial_loss:\\n                # calculate parameter posterior distribution parameters\\n                self._update_m_p(res, X, y, m, p, **kwargs)\\n                logging.info(\\n                    \\\"Posterior distribution parameters computed even \\\"\\n                    \\\"though the optimization failed.\\\"\\n                )\\n            else:\\n                raise OptimizationError\\n\\n        return self\\n\\n    @abstractmethod\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def get_params(self, deep=True) -> dict:\\n        return {\\n            \\\"prior_parameters\\\": self.prior_parameters,\\n            \\\"default_parameters\\\": self.default_parameters,\\n            \\\"optimize_kwargs\\\": self.optimize_kwargs,\\n            \\\"optimize_method\\\": self.optimize_method,\\n        }\\n\\n    def set_params(self, **parameters) -> BayesianInterface:\\n        for parameter, value in parameters.items():\\n            setattr(self, parameter, value)\\n        return self\";\n",
       "                var nbb_formatted_code = \"class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\\n    def __init__(\\n        self,\\n        prior_parameters: dict = {},\\n        default_parameters: dict = {\\\"m\\\": 0, \\\"p\\\": 5},\\n        optimize_kwargs: dict = {},\\n        optimize_method: str = \\\"L-BFGS-B\\\",\\n    ) -> None:\\n        self.prior_parameters = prior_parameters\\n        self.default_parameters = default_parameters\\n        self.optimize_kwargs = optimize_kwargs\\n        self.optimize_method = optimize_method\\n\\n    def _check_distribution_params(self, params: dict) -> None:\\n        if not (\\n            isinstance(params, dict)\\n            and set(params.keys()) == set([\\\"m\\\", \\\"p\\\"])\\n            and (\\n                isinstance(params.get(\\\"m\\\"), Union[int, float])  # type: ignore\\n                and not np.isnan(params.get(\\\"m\\\"))  # type: ignore\\n            )\\n            and (\\n                isinstance(params.get(\\\"p\\\"), Union[int, float])  # type: ignore\\n                and params.get(\\\"p\\\") >= 0  # type: ignore\\n            )\\n        ):\\n            raise ValueError\\n\\n    @abstractmethod\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\\n        m = np.array([])\\n        p = np.array([])\\n        for col in feat_names:\\n            m = np.append(\\n                m,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"],\\n            )\\n            p = np.append(\\n                p,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"],\\n            )\\n        return m, p\\n\\n    def _update_m_p(\\n        self,\\n        res: optimize.OptimizeResult,\\n        X: pd.DataFrame,\\n        y: pd.Series,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> None:\\n        # get new updates values\\n        m_updated = res.x\\n        p_updated = self._diag_hess(\\n            res.x,\\n            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n        )\\n        feat_names = X.columns.to_list()\\n\\n        # check hessian positiveness\\n        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\\n            raise NotPositiveHessian\\n\\n        # update parameters\\n        self.posterior_parameters_ = dict(\\n            self.prior_parameters,\\n            **{\\n                col: {\\\"m\\\": m, \\\"p\\\": p}\\n                for col, m, p in zip(feat_names, m_updated, p_updated)\\n            },\\n        )\\n\\n        # check parameters\\n        for _, v in self.posterior_parameters_.items():\\n            self._check_distribution_params(v)\\n\\n    @abstractmethod\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\\n        logging.info(f\\\"Shape of the training dataset: {X.shape}.\\\")\\n\\n        # get parameter prior distribution parameters (mean and precision)\\n        m, p = self._get_prior_m_p(X.columns.to_list())\\n        logging.info(\\\"Prior distribution parameters obtained.\\\")\\n\\n        # optimize\\n        res = optimize.minimize(\\n            fun=self._loss,\\n            x0=m,\\n            jac=self._jac,\\n            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n            method=self.optimize_method,\\n            **self.optimize_kwargs,\\n        )\\n\\n        # log optimization result\\n        initial_loss = self._loss(\\n            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\\n        )\\n        log_message = (\\n            f\\\"initial fun: {initial_loss}\\\\n\\\"\\n            \\\"The OptimizeResult instance is:\\\\n\\\"\\n            f\\\"{res}\\\"\\n        )\\n\\n        if res.success:\\n            logging.info(f\\\"Log likelihood optimized successfully.\\\\n{log_message}\\\")\\n\\n            # calculate parameter posterior distribution parameters\\n            self._update_m_p(res, X, y, m, p, **kwargs)\\n            logging.info(\\\"Posterior distribution parameters computed.\\\")\\n        else:\\n            logging.warning(f\\\"Log likelihood optimization failed.\\\\n{log_message}\\\")\\n            if res.fun < initial_loss:\\n                # calculate parameter posterior distribution parameters\\n                self._update_m_p(res, X, y, m, p, **kwargs)\\n                logging.info(\\n                    \\\"Posterior distribution parameters computed even \\\"\\n                    \\\"though the optimization failed.\\\"\\n                )\\n            else:\\n                raise OptimizationError\\n\\n        return self\\n\\n    @abstractmethod\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def get_params(self, deep=True) -> dict:\\n        return {\\n            \\\"prior_parameters\\\": self.prior_parameters,\\n            \\\"default_parameters\\\": self.default_parameters,\\n            \\\"optimize_kwargs\\\": self.optimize_kwargs,\\n            \\\"optimize_method\\\": self.optimize_method,\\n        }\\n\\n    def set_params(self, **parameters) -> BayesianInterface:\\n        for parameter, value in parameters.items():\\n            setattr(self, parameter, value)\\n        return self\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\n",
    "    def __init__(\n",
    "        self,\n",
    "        prior_parameters: dict = {},\n",
    "        default_parameters: dict = {\"m\": 0, \"p\": 5},\n",
    "        optimize_kwargs: dict = {},\n",
    "        optimize_method: str = \"L-BFGS-B\",\n",
    "    ) -> None:\n",
    "        self.prior_parameters = prior_parameters\n",
    "        self.default_parameters = default_parameters\n",
    "        self.optimize_kwargs = optimize_kwargs\n",
    "        self.optimize_method = optimize_method\n",
    "\n",
    "    def _check_distribution_params(self, params: dict) -> None:\n",
    "        if not (\n",
    "            isinstance(params, dict)\n",
    "            and set(params.keys()) == set([\"m\", \"p\"])\n",
    "            and (\n",
    "                isinstance(params.get(\"m\"), Union[int, float])  # type: ignore\n",
    "                and not np.isnan(params.get(\"m\"))  # type: ignore\n",
    "            )\n",
    "            and (\n",
    "                isinstance(params.get(\"p\"), Union[int, float])  # type: ignore\n",
    "                and params.get(\"p\") >= 0  # type: ignore\n",
    "            )\n",
    "        ):\n",
    "            raise ValueError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _loss(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> float:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _jac(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _diag_hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        m = np.array([])\n",
    "        p = np.array([])\n",
    "        for col in feat_names:\n",
    "            m = np.append(\n",
    "                m,\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"m\"],\n",
    "            )\n",
    "            p = np.append(\n",
    "                p,\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"p\"],\n",
    "            )\n",
    "        return m, p\n",
    "\n",
    "    def _update_m_p(\n",
    "        self,\n",
    "        res: optimize.OptimizeResult,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        # get new updates values\n",
    "        m_updated = res.x\n",
    "        p_updated = self._diag_hess(\n",
    "            res.x,\n",
    "            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\n",
    "        )\n",
    "        feat_names = X.columns.to_list()\n",
    "\n",
    "        # check hessian positiveness\n",
    "        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\n",
    "            raise NotPositiveHessian\n",
    "\n",
    "        # update parameters\n",
    "        self.posterior_parameters_ = dict(\n",
    "            self.prior_parameters,\n",
    "            **{\n",
    "                col: {\"m\": m, \"p\": p}\n",
    "                for col, m, p in zip(feat_names, m_updated, p_updated)\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # check parameters\n",
    "        for _, v in self.posterior_parameters_.items():\n",
    "            self._check_distribution_params(v)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_args(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\n",
    "        logging.info(f\"Shape of the training dataset: {X.shape}.\")\n",
    "\n",
    "        # get parameter prior distribution parameters (mean and precision)\n",
    "        m, p = self._get_prior_m_p(X.columns.to_list())\n",
    "        logging.info(\"Prior distribution parameters obtained.\")\n",
    "\n",
    "        # optimize\n",
    "        res = optimize.minimize(\n",
    "            fun=self._loss,\n",
    "            x0=m,\n",
    "            jac=self._jac,\n",
    "            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\n",
    "            method=self.optimize_method,\n",
    "            **self.optimize_kwargs,\n",
    "        )\n",
    "\n",
    "        # log optimization result\n",
    "        initial_loss = self._loss(\n",
    "            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\n",
    "        )\n",
    "        log_message = (\n",
    "            f\"initial fun: {initial_loss}\\n\"\n",
    "            \"The OptimizeResult instance is:\\n\"\n",
    "            f\"{res}\"\n",
    "        )\n",
    "\n",
    "        if res.success:\n",
    "            logging.info(f\"Log likelihood optimized successfully.\\n{log_message}\")\n",
    "\n",
    "            # calculate parameter posterior distribution parameters\n",
    "            self._update_m_p(res, X, y, m, p, **kwargs)\n",
    "            logging.info(\"Posterior distribution parameters computed.\")\n",
    "        else:\n",
    "            logging.warning(f\"Log likelihood optimization failed.\\n{log_message}\")\n",
    "            if res.fun < initial_loss:\n",
    "                # calculate parameter posterior distribution parameters\n",
    "                self._update_m_p(res, X, y, m, p, **kwargs)\n",
    "                logging.info(\n",
    "                    \"Posterior distribution parameters computed even \"\n",
    "                    \"though the optimization failed.\"\n",
    "                )\n",
    "            else:\n",
    "                raise OptimizationError\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def get_params(self, deep=True) -> dict:\n",
    "        return {\n",
    "            \"prior_parameters\": self.prior_parameters,\n",
    "            \"default_parameters\": self.default_parameters,\n",
    "            \"optimize_kwargs\": self.optimize_kwargs,\n",
    "            \"optimize_method\": self.optimize_method,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters) -> BayesianInterface:\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef38757",
   "metadata": {},
   "source": [
    "### BayesianLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f69813b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"class BayesianLogisticRegression(BayesianInterface):\\n    def __init__(self, link: Link = Logit(), **kwargs):\\n        self.link = link\\n        super().__init__(**kwargs)\\n\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> float:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :param r: regularization parameter\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        loss = (\\n            log_loss(y, y_pred, sample_weight=weights, normalize=False)\\n            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\\n            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\\n        )\\n\\n        return loss\\n\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        jac = np.add(\\n            -np.dot(weights * (y - y_pred), X),\\n            np.multiply(p, np.subtract(beta, m)),\\n        )\\n\\n        return jac\\n\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        diag_hess = p + np.dot(\\n            np.power(X, 2).T,\\n            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\\n        )\\n        return diag_hess\\n\\n    def _hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        hess = np.diag(p) + np.matmul(\\n            np.matmul(\\n                X.T,\\n                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n            ),\\n            X,\\n        )\\n        return hess\\n\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        m_posterior = np.array(\\n            [\\n                self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        m_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        p_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\\n\\n    def get_params(self, deep=True):\\n        shared_params = super().get_params()\\n        return dict(shared_params, **{\\\"link\\\": self.link})\\n\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        return (X, y, m, p)\\n\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        m = [\\n            self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n            for col in X.columns\\n        ]\\n        linear_pred = np.dot(X.to_numpy(), np.array(m))\\n        return self.link._inv_link(linear_pred)\";\n",
       "                var nbb_formatted_code = \"class BayesianLogisticRegression(BayesianInterface):\\n    def __init__(self, link: Link = Logit(), **kwargs):\\n        self.link = link\\n        super().__init__(**kwargs)\\n\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> float:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :param r: regularization parameter\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        loss = (\\n            log_loss(y, y_pred, sample_weight=weights, normalize=False)\\n            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\\n            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\\n        )\\n\\n        return loss\\n\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        jac = np.add(\\n            -np.dot(weights * (y - y_pred), X),\\n            np.multiply(p, np.subtract(beta, m)),\\n        )\\n\\n        return jac\\n\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        diag_hess = p + np.dot(\\n            np.power(X, 2).T,\\n            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\\n        )\\n        return diag_hess\\n\\n    def _hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        hess = np.diag(p) + np.matmul(\\n            np.matmul(\\n                X.T,\\n                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n            ),\\n            X,\\n        )\\n        return hess\\n\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        m_posterior = np.array(\\n            [\\n                self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        m_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        p_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\\n\\n    def get_params(self, deep=True):\\n        shared_params = super().get_params()\\n        return dict(shared_params, **{\\\"link\\\": self.link})\\n\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        return (X, y, m, p)\\n\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        m = [\\n            self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n            for col in X.columns\\n        ]\\n        linear_pred = np.dot(X.to_numpy(), np.array(m))\\n        return self.link._inv_link(linear_pred)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BayesianLogisticRegression(BayesianInterface):\n",
    "    def __init__(self, link: Link = Logit(), **kwargs):\n",
    "        self.link = link\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _loss(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        LogLikelihood defines the regularized loss function\n",
    "        :param omega: vector to optimize\n",
    "        :param y: responses (1/-1) of training data\n",
    "        :param X: dimensions of training data\n",
    "        :param m: previous vector of means\n",
    "        :param p: previous vector of inverse variances\n",
    "        :param r: regularization parameter\n",
    "        :return out: value of loss function\n",
    "        :return grad: gradient of loss function\n",
    "        \"\"\"\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        loss = (\n",
    "            log_loss(y, y_pred, sample_weight=weights, normalize=False)\n",
    "            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\n",
    "            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _jac(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        LogLikelihood defines the regularized loss function\n",
    "        :param omega: vector to optimize\n",
    "        :param y: responses (1/-1) of training data\n",
    "        :param X: dimensions of training data\n",
    "        :param m: previous vector of means\n",
    "        :param p: previous vector of inverse variances\n",
    "        :return out: value of loss function\n",
    "        :return grad: gradient of loss function\n",
    "        \"\"\"\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        jac = np.add(\n",
    "            -np.dot(weights * (y - y_pred), X),\n",
    "            np.multiply(p, np.subtract(beta, m)),\n",
    "        )\n",
    "\n",
    "        return jac\n",
    "\n",
    "    def _diag_hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> np.ndarray:\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        diag_hess = p + np.dot(\n",
    "            np.power(X, 2).T,\n",
    "            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\n",
    "        )\n",
    "        return diag_hess\n",
    "\n",
    "    def _hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        hess = np.diag(p) + np.matmul(\n",
    "            np.matmul(\n",
    "                X.T,\n",
    "                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\n",
    "            ),\n",
    "            X,\n",
    "        )\n",
    "        return hess\n",
    "\n",
    "    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        m_posterior = np.array(\n",
    "            [\n",
    "                self.posterior_parameters_.get(col, self.default_parameters)[\"m\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        m_prior = np.array(\n",
    "            [\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"m\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        p_prior = np.array(\n",
    "            [\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"p\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        shared_params = super().get_params()\n",
    "        return dict(shared_params, **{\"link\": self.link})\n",
    "\n",
    "    def _get_args(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> tuple:\n",
    "        return (X, y, m, p)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        m = [\n",
    "            self.posterior_parameters_.get(col, self.default_parameters)[\"m\"]\n",
    "            for col in X.columns\n",
    "        ]\n",
    "        linear_pred = np.dot(X.to_numpy(), np.array(m))\n",
    "        return self.link._inv_link(linear_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2f408",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf96e6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"beta = np.random.normal(size=4)\\nX = np.random.normal(size=(10, 4))\\ny = np.random.choice([0, 1], size=10)\\nm = np.random.normal(size=4)\\nq = np.exp(np.random.normal(size=4))\";\n",
       "                var nbb_formatted_code = \"beta = np.random.normal(size=4)\\nX = np.random.normal(size=(10, 4))\\ny = np.random.choice([0, 1], size=10)\\nm = np.random.normal(size=4)\\nq = np.exp(np.random.normal(size=4))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = np.random.normal(size=4)\n",
    "X = np.random.normal(size=(10, 4))\n",
    "y = np.random.choice([0, 1], size=10)\n",
    "m = np.random.normal(size=4)\n",
    "q = np.exp(np.random.normal(size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e65efbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._loss,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._jac(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_formatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._loss,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._jac(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.testing.assert_allclose(\n",
    "    approx_fprime(\n",
    "        beta,\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        1.4901161193847656e-08,\n",
    "        *(X, y, m, q),\n",
    "    ),\n",
    "    BayesianLogisticRegression()._jac(beta, X, y, m, q),\n",
    "    atol=1e-6,\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0faa7a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._jac,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._hess(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_formatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._jac,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._hess(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.testing.assert_allclose(\n",
    "    approx_fprime(\n",
    "        beta,\n",
    "        BayesianLogisticRegression()._jac,\n",
    "        1.4901161193847656e-08,\n",
    "        *(X, y, m, q),\n",
    "    ),\n",
    "    BayesianLogisticRegression()._hess(beta, X, y, m, q),\n",
    "    atol=1e-6,\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6188d88",
   "metadata": {},
   "source": [
    "## Bayesian loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c79b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\";\n",
       "                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5838e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"def loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = func_with_bias(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = func_with_bias(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_formatted_code = \"def loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = func_with_bias(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = func_with_bias(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loss(log_sigma2, X, y):\n",
    "    n_feat = X.shape[1]\n",
    "    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\n",
    "    diag_sigma2 = np.exp(log_sigma2)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = func_with_bias(xk, X, y)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = func_with_bias(xk, X, y)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f72ff",
   "metadata": {},
   "source": [
    "___\n",
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf333b60",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab81624b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"data = pd.read_csv(\\n    \\\"/home/capitaine/01_projects/2024/bayesian_optimization/data/titanic/train.csv\\\"\\n)\\ny = data.pop(\\\"Survived\\\")\\ndata[\\\"CabinCat\\\"] = data[\\\"Cabin\\\"].astype(str).str[0]\\ndata[\\\"Embarked\\\"] = data[\\\"Embarked\\\"].fillna(\\\"unknown\\\")\\nX = data[[\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\", \\\"Age\\\", \\\"SibSp\\\", \\\"Parch\\\"]].copy()\";\n",
       "                var nbb_formatted_code = \"data = pd.read_csv(\\n    \\\"/home/capitaine/01_projects/2024/bayesian_optimization/data/titanic/train.csv\\\"\\n)\\ny = data.pop(\\\"Survived\\\")\\ndata[\\\"CabinCat\\\"] = data[\\\"Cabin\\\"].astype(str).str[0]\\ndata[\\\"Embarked\\\"] = data[\\\"Embarked\\\"].fillna(\\\"unknown\\\")\\nX = data[[\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\", \\\"Age\\\", \\\"SibSp\\\", \\\"Parch\\\"]].copy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"/home/capitaine/01_projects/2024/bayesian_optimization/data/titanic/train.csv\"\n",
    ")\n",
    "y = data.pop(\"Survived\")\n",
    "data[\"CabinCat\"] = data[\"Cabin\"].astype(str).str[0]\n",
    "data[\"Embarked\"] = data[\"Embarked\"].fillna(\"unknown\")\n",
    "X = data[[\"Pclass\", \"Sex\", \"Embarked\", \"CabinCat\", \"Age\", \"SibSp\", \"Parch\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ded10b",
   "metadata": {},
   "source": [
    "## Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b91bb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\";\n",
       "                var nbb_formatted_code = \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfce23",
   "metadata": {},
   "source": [
    "___\n",
    "# Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e400bbf",
   "metadata": {},
   "source": [
    "## Single sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab81dd",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616d7ea",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2ceacef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"preprocessor = ColumnTransformer(\\n    [\\n        (\\n            \\\"categorical_features_wo_nan\\\",\\n            OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n            [\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\"],\\n        ),\\n        (\\n            \\\"numerical_features_w_nan\\\",\\n            Pipeline(\\n                [\\n                    (\\\"imputer\\\", SimpleImputer(strategy=\\\"mean\\\")),\\n                ]\\n            ),\\n            [\\\"Age\\\"],\\n        ),\\n    ],\\n    remainder=\\\"passthrough\\\",\\n)\\npreprocessor.set_output(transform=\\\"pandas\\\")\\npipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\\"classifier\\\", BayesianLogisticRegression()),\\n    ]\\n)\";\n",
       "                var nbb_formatted_code = \"preprocessor = ColumnTransformer(\\n    [\\n        (\\n            \\\"categorical_features_wo_nan\\\",\\n            OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n            [\\\"Pclass\\\", \\\"Sex\\\", \\\"Embarked\\\", \\\"CabinCat\\\"],\\n        ),\\n        (\\n            \\\"numerical_features_w_nan\\\",\\n            Pipeline(\\n                [\\n                    (\\\"imputer\\\", SimpleImputer(strategy=\\\"mean\\\")),\\n                ]\\n            ),\\n            [\\\"Age\\\"],\\n        ),\\n    ],\\n    remainder=\\\"passthrough\\\",\\n)\\npreprocessor.set_output(transform=\\\"pandas\\\")\\npipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\\"classifier\\\", BayesianLogisticRegression()),\\n    ]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"categorical_features_wo_nan\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            [\"Pclass\", \"Sex\", \"Embarked\", \"CabinCat\"],\n",
    "        ),\n",
    "        (\n",
    "            \"numerical_features_w_nan\",\n",
    "            Pipeline(\n",
    "                [\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                ]\n",
    "            ),\n",
    "            [\"Age\"],\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"classifier\", BayesianLogisticRegression()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0b57a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"param_grid = {\\n    \\\"classifier__default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 5, 17)]\\n}\";\n",
       "                var nbb_formatted_code = \"param_grid = {\\n    \\\"classifier__default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 5, 17)]\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"classifier__default_parameters\": [{\"m\": 0, \"p\": p} for p in np.logspace(-3, 5, 17)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f63af0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"grid_search = GridSearchCV(pipe, param_grid, return_train_score=True)\";\n",
       "                var nbb_formatted_code = \"grid_search = GridSearchCV(pipe, param_grid, return_train_score=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c04aea2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                                        ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                          transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                                                         OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                       sparse_output=False),\n",
       "                                                                         [&#x27;Pclass&#x27;,\n",
       "                                                                          &#x27;Sex&#x27;,\n",
       "                                                                          &#x27;Embarked&#x27;,\n",
       "                                                                          &#x27;CabinCat&#x27;]),\n",
       "                                                                        (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                                          SimpleImputer())]),\n",
       "                                                                         [&#x27;Age&#x27;])])),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        Bayesian...\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 0.09999999999999999},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 0.31622776601683794},\n",
       "                                                            {&#x27;m&#x27;: 0, &#x27;p&#x27;: 1.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 3.162277660168379},\n",
       "                                                            {&#x27;m&#x27;: 0, &#x27;p&#x27;: 10.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 31.622776601683796},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 100.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 316.2277660168379},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 1000.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 3162.277660168379},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 10000.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 31622.776601683796},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 100000.0}]},\n",
       "             return_train_score=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                                        ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                          transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                                                         OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                       sparse_output=False),\n",
       "                                                                         [&#x27;Pclass&#x27;,\n",
       "                                                                          &#x27;Sex&#x27;,\n",
       "                                                                          &#x27;Embarked&#x27;,\n",
       "                                                                          &#x27;CabinCat&#x27;]),\n",
       "                                                                        (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                                          SimpleImputer())]),\n",
       "                                                                         [&#x27;Age&#x27;])])),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        Bayesian...\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 0.09999999999999999},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 0.31622776601683794},\n",
       "                                                            {&#x27;m&#x27;: 0, &#x27;p&#x27;: 1.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 3.162277660168379},\n",
       "                                                            {&#x27;m&#x27;: 0, &#x27;p&#x27;: 10.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 31.622776601683796},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 100.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 316.2277660168379},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 1000.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 3162.277660168379},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 10000.0},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 31622.776601683796},\n",
       "                                                            {&#x27;m&#x27;: 0,\n",
       "                                                             &#x27;p&#x27;: 100000.0}]},\n",
       "             return_train_score=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False),\n",
       "                                                  [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;,\n",
       "                                                   &#x27;CabinCat&#x27;]),\n",
       "                                                 (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer())]),\n",
       "                                                  [&#x27;Age&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0, &#x27;p&#x27;: 5},\n",
       "                                            optimize_kwargs={},\n",
       "                                            optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                                            prior_parameters={}))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                               sparse_output=False),\n",
       "                                 [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;, &#x27;CabinCat&#x27;]),\n",
       "                                (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer())]),\n",
       "                                 [&#x27;Age&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_features_wo_nan</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;, &#x27;CabinCat&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_features_w_nan</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Age&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianLogisticRegression</label><div class=\"sk-toggleable__content\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0, &#x27;p&#x27;: 5},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={})</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('preprocessing',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('categorical_features_wo_nan',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                       sparse_output=False),\n",
       "                                                                         ['Pclass',\n",
       "                                                                          'Sex',\n",
       "                                                                          'Embarked',\n",
       "                                                                          'CabinCat']),\n",
       "                                                                        ('numerical_features_w_nan',\n",
       "                                                                         Pipeline(steps=[('imputer',\n",
       "                                                                                          SimpleImputer())]),\n",
       "                                                                         ['Age'])])),\n",
       "                                       ('classifier',\n",
       "                                        Bayesian...\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 0.09999999999999999},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 0.31622776601683794},\n",
       "                                                            {'m': 0, 'p': 1.0},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 3.162277660168379},\n",
       "                                                            {'m': 0, 'p': 10.0},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 31.622776601683796},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 100.0},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 316.2277660168379},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 1000.0},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 3162.277660168379},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 10000.0},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 31622.776601683796},\n",
       "                                                            {'m': 0,\n",
       "                                                             'p': 100000.0}]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"grid_search.fit(X, y)\";\n",
       "                var nbb_formatted_code = \"grid_search.fit(X, y)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff520d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__default_parameters</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.622352</td>\n",
       "      <td>1.824817</td>\n",
       "      <td>0.031287</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>{'m': 0, 'p': 0.001}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-173.822470</td>\n",
       "      <td>-184.260668</td>\n",
       "      <td>-171.280747</td>\n",
       "      <td>-181.702957</td>\n",
       "      <td>...</td>\n",
       "      <td>-174.457392</td>\n",
       "      <td>8.177066</td>\n",
       "      <td>17</td>\n",
       "      <td>-395.369373</td>\n",
       "      <td>-386.387963</td>\n",
       "      <td>-396.213989</td>\n",
       "      <td>-390.293968</td>\n",
       "      <td>-407.053889</td>\n",
       "      <td>-395.063836</td>\n",
       "      <td>6.974238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.482885</td>\n",
       "      <td>0.851154</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>{'m': 0, 'p': 0.0031622776601683794}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-161.775152</td>\n",
       "      <td>-171.053824</td>\n",
       "      <td>-159.271037</td>\n",
       "      <td>-169.585077</td>\n",
       "      <td>...</td>\n",
       "      <td>-162.167175</td>\n",
       "      <td>7.898930</td>\n",
       "      <td>16</td>\n",
       "      <td>-383.325038</td>\n",
       "      <td>-374.921836</td>\n",
       "      <td>-384.174720</td>\n",
       "      <td>-378.252684</td>\n",
       "      <td>-395.010604</td>\n",
       "      <td>-383.136976</td>\n",
       "      <td>6.832864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.792763</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.023451</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>{'m': 0, 'p': 0.01}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-149.739296</td>\n",
       "      <td>-157.614562</td>\n",
       "      <td>-147.223078</td>\n",
       "      <td>-157.594317</td>\n",
       "      <td>...</td>\n",
       "      <td>-149.868679</td>\n",
       "      <td>7.588178</td>\n",
       "      <td>15</td>\n",
       "      <td>-371.341451</td>\n",
       "      <td>-363.516496</td>\n",
       "      <td>-372.202477</td>\n",
       "      <td>-366.272361</td>\n",
       "      <td>-383.005650</td>\n",
       "      <td>-371.267687</td>\n",
       "      <td>6.688696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.375778</td>\n",
       "      <td>0.228086</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>{'m': 0, 'p': 0.03162277660168379}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-137.841944</td>\n",
       "      <td>-144.431796</td>\n",
       "      <td>-135.299507</td>\n",
       "      <td>-145.663396</td>\n",
       "      <td>...</td>\n",
       "      <td>-137.699588</td>\n",
       "      <td>7.338073</td>\n",
       "      <td>14</td>\n",
       "      <td>-359.504733</td>\n",
       "      <td>-352.252240</td>\n",
       "      <td>-360.397984</td>\n",
       "      <td>-354.444280</td>\n",
       "      <td>-371.129515</td>\n",
       "      <td>-359.545750</td>\n",
       "      <td>6.544186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.144342</td>\n",
       "      <td>0.463431</td>\n",
       "      <td>0.016058</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>{'m': 0, 'p': 0.09999999999999999}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-126.162471</td>\n",
       "      <td>-131.838638</td>\n",
       "      <td>-123.526255</td>\n",
       "      <td>-133.972219</td>\n",
       "      <td>...</td>\n",
       "      <td>-125.822033</td>\n",
       "      <td>7.171207</td>\n",
       "      <td>13</td>\n",
       "      <td>-348.044474</td>\n",
       "      <td>-341.332866</td>\n",
       "      <td>-349.013277</td>\n",
       "      <td>-342.997826</td>\n",
       "      <td>-359.573375</td>\n",
       "      <td>-348.192364</td>\n",
       "      <td>6.391543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.420264</td>\n",
       "      <td>0.354410</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>{'m': 0, 'p': 0.31622776601683794}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-115.137118</td>\n",
       "      <td>-120.080571</td>\n",
       "      <td>-112.396023</td>\n",
       "      <td>-122.854385</td>\n",
       "      <td>...</td>\n",
       "      <td>-114.635247</td>\n",
       "      <td>6.996109</td>\n",
       "      <td>12</td>\n",
       "      <td>-337.563157</td>\n",
       "      <td>-331.320997</td>\n",
       "      <td>-338.667527</td>\n",
       "      <td>-332.547011</td>\n",
       "      <td>-348.873372</td>\n",
       "      <td>-337.794413</td>\n",
       "      <td>6.212882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.104722</td>\n",
       "      <td>0.197425</td>\n",
       "      <td>0.017896</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>{'m': 0, 'p': 1.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-105.776255</td>\n",
       "      <td>-110.426577</td>\n",
       "      <td>-102.863712</td>\n",
       "      <td>-113.249369</td>\n",
       "      <td>...</td>\n",
       "      <td>-105.193055</td>\n",
       "      <td>6.801717</td>\n",
       "      <td>11</td>\n",
       "      <td>-329.504017</td>\n",
       "      <td>-323.683949</td>\n",
       "      <td>-330.703741</td>\n",
       "      <td>-324.531026</td>\n",
       "      <td>-340.344349</td>\n",
       "      <td>-329.753416</td>\n",
       "      <td>5.953906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.706362</td>\n",
       "      <td>0.235579</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>0.005940</td>\n",
       "      <td>{'m': 0, 'p': 3.162277660168379}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-99.705536</td>\n",
       "      <td>-104.377936</td>\n",
       "      <td>-96.885678</td>\n",
       "      <td>-106.766850</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.178857</td>\n",
       "      <td>6.505338</td>\n",
       "      <td>8</td>\n",
       "      <td>-326.724765</td>\n",
       "      <td>-321.419210</td>\n",
       "      <td>-327.744144</td>\n",
       "      <td>-321.671240</td>\n",
       "      <td>-336.608309</td>\n",
       "      <td>-326.833534</td>\n",
       "      <td>5.519990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.346612</td>\n",
       "      <td>0.111055</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>{'m': 0, 'p': 10.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-98.045459</td>\n",
       "      <td>-102.944548</td>\n",
       "      <td>-95.799562</td>\n",
       "      <td>-104.836514</td>\n",
       "      <td>...</td>\n",
       "      <td>-97.888664</td>\n",
       "      <td>5.994283</td>\n",
       "      <td>7</td>\n",
       "      <td>-333.176049</td>\n",
       "      <td>-328.549920</td>\n",
       "      <td>-333.648274</td>\n",
       "      <td>-327.703771</td>\n",
       "      <td>-341.437608</td>\n",
       "      <td>-332.903125</td>\n",
       "      <td>4.887756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.309426</td>\n",
       "      <td>0.134791</td>\n",
       "      <td>0.017918</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>{'m': 0, 'p': 31.622776601683796}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-99.866744</td>\n",
       "      <td>-104.928288</td>\n",
       "      <td>-98.561601</td>\n",
       "      <td>-106.603056</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.315495</td>\n",
       "      <td>5.286567</td>\n",
       "      <td>10</td>\n",
       "      <td>-351.589467</td>\n",
       "      <td>-347.752137</td>\n",
       "      <td>-351.260553</td>\n",
       "      <td>-345.817551</td>\n",
       "      <td>-357.954349</td>\n",
       "      <td>-350.874811</td>\n",
       "      <td>4.149852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.155619</td>\n",
       "      <td>0.030631</td>\n",
       "      <td>0.014638</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>{'m': 0, 'p': 100.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-98.696509</td>\n",
       "      <td>-104.050372</td>\n",
       "      <td>-98.464872</td>\n",
       "      <td>-104.605225</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.616639</td>\n",
       "      <td>4.488784</td>\n",
       "      <td>9</td>\n",
       "      <td>-378.891659</td>\n",
       "      <td>-375.355022</td>\n",
       "      <td>-377.424366</td>\n",
       "      <td>-373.452304</td>\n",
       "      <td>-383.232877</td>\n",
       "      <td>-377.671246</td>\n",
       "      <td>3.336059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.123688</td>\n",
       "      <td>0.041272</td>\n",
       "      <td>0.016123</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>{'m': 0, 'p': 316.2277660168379}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-86.275862</td>\n",
       "      <td>-93.148486</td>\n",
       "      <td>-87.754187</td>\n",
       "      <td>-91.440785</td>\n",
       "      <td>...</td>\n",
       "      <td>-88.197702</td>\n",
       "      <td>3.818103</td>\n",
       "      <td>6</td>\n",
       "      <td>-402.329661</td>\n",
       "      <td>-398.166907</td>\n",
       "      <td>-399.295353</td>\n",
       "      <td>-396.913986</td>\n",
       "      <td>-404.053733</td>\n",
       "      <td>-400.151928</td>\n",
       "      <td>2.650468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.118507</td>\n",
       "      <td>0.023298</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>{'m': 0, 'p': 1000.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-68.009046</td>\n",
       "      <td>-77.087976</td>\n",
       "      <td>-71.294239</td>\n",
       "      <td>-73.497522</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.382253</td>\n",
       "      <td>3.672153</td>\n",
       "      <td>5</td>\n",
       "      <td>-411.049270</td>\n",
       "      <td>-406.326340</td>\n",
       "      <td>-406.834239</td>\n",
       "      <td>-405.298473</td>\n",
       "      <td>-410.648569</td>\n",
       "      <td>-408.031378</td>\n",
       "      <td>2.356541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.062746</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.013177</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>{'m': 0, 'p': 3162.277660168379}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-52.046354</td>\n",
       "      <td>-62.651708</td>\n",
       "      <td>-56.272548</td>\n",
       "      <td>-57.899284</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.318227</td>\n",
       "      <td>3.841928</td>\n",
       "      <td>4</td>\n",
       "      <td>-407.765645</td>\n",
       "      <td>-403.162038</td>\n",
       "      <td>-403.088912</td>\n",
       "      <td>-401.885418</td>\n",
       "      <td>-406.381438</td>\n",
       "      <td>-404.456690</td>\n",
       "      <td>2.227654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.053509</td>\n",
       "      <td>0.005712</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>{'m': 0, 'p': 10000.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-39.035281</td>\n",
       "      <td>-50.502240</td>\n",
       "      <td>-43.564582</td>\n",
       "      <td>-44.972107</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.683390</td>\n",
       "      <td>4.023344</td>\n",
       "      <td>3</td>\n",
       "      <td>-399.248778</td>\n",
       "      <td>-395.351048</td>\n",
       "      <td>-394.608286</td>\n",
       "      <td>-393.577672</td>\n",
       "      <td>-397.590057</td>\n",
       "      <td>-396.075168</td>\n",
       "      <td>2.063020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.072671</td>\n",
       "      <td>0.022833</td>\n",
       "      <td>0.018614</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>{'m': 0, 'p': 31622.776601683796}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-28.239177</td>\n",
       "      <td>-40.055747</td>\n",
       "      <td>-32.682269</td>\n",
       "      <td>-33.917811</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.918882</td>\n",
       "      <td>4.104957</td>\n",
       "      <td>2</td>\n",
       "      <td>-389.681373</td>\n",
       "      <td>-387.194734</td>\n",
       "      <td>-385.578677</td>\n",
       "      <td>-384.761997</td>\n",
       "      <td>-388.157631</td>\n",
       "      <td>-387.074883</td>\n",
       "      <td>1.764572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.053104</td>\n",
       "      <td>0.021047</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>{'m': 0, 'p': 100000.0}</td>\n",
       "      <td>{'classifier__default_parameters': {'m': 0, 'p...</td>\n",
       "      <td>-19.193555</td>\n",
       "      <td>-30.441839</td>\n",
       "      <td>-22.945988</td>\n",
       "      <td>-23.852179</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.377576</td>\n",
       "      <td>3.908114</td>\n",
       "      <td>1</td>\n",
       "      <td>-381.214817</td>\n",
       "      <td>-381.153912</td>\n",
       "      <td>-378.324985</td>\n",
       "      <td>-377.839372</td>\n",
       "      <td>-380.197177</td>\n",
       "      <td>-379.746053</td>\n",
       "      <td>1.414052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        2.622352      1.824817         0.031287        0.012953   \n",
       "1        3.482885      0.851154         0.015638        0.001301   \n",
       "2        2.792763      0.381660         0.023451        0.008795   \n",
       "3        1.375778      0.228086         0.015128        0.001812   \n",
       "4        1.144342      0.463431         0.016058        0.003245   \n",
       "5        1.420264      0.354410         0.016997        0.003068   \n",
       "6        1.104722      0.197425         0.017896        0.005756   \n",
       "7        0.706362      0.235579         0.019557        0.005940   \n",
       "8        0.346612      0.111055         0.013750        0.000642   \n",
       "9        0.309426      0.134791         0.017918        0.003753   \n",
       "10       0.155619      0.030631         0.014638        0.000843   \n",
       "11       0.123688      0.041272         0.016123        0.004566   \n",
       "12       0.118507      0.023298         0.015771        0.003917   \n",
       "13       0.062746      0.005218         0.013177        0.000432   \n",
       "14       0.053509      0.005712         0.013703        0.000365   \n",
       "15       0.072671      0.022833         0.018614        0.004966   \n",
       "16       0.053104      0.021047         0.016502        0.006114   \n",
       "\n",
       "    param_classifier__default_parameters  \\\n",
       "0                   {'m': 0, 'p': 0.001}   \n",
       "1   {'m': 0, 'p': 0.0031622776601683794}   \n",
       "2                    {'m': 0, 'p': 0.01}   \n",
       "3     {'m': 0, 'p': 0.03162277660168379}   \n",
       "4     {'m': 0, 'p': 0.09999999999999999}   \n",
       "5     {'m': 0, 'p': 0.31622776601683794}   \n",
       "6                     {'m': 0, 'p': 1.0}   \n",
       "7       {'m': 0, 'p': 3.162277660168379}   \n",
       "8                    {'m': 0, 'p': 10.0}   \n",
       "9      {'m': 0, 'p': 31.622776601683796}   \n",
       "10                  {'m': 0, 'p': 100.0}   \n",
       "11      {'m': 0, 'p': 316.2277660168379}   \n",
       "12                 {'m': 0, 'p': 1000.0}   \n",
       "13      {'m': 0, 'p': 3162.277660168379}   \n",
       "14                {'m': 0, 'p': 10000.0}   \n",
       "15     {'m': 0, 'p': 31622.776601683796}   \n",
       "16               {'m': 0, 'p': 100000.0}   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'classifier__default_parameters': {'m': 0, 'p...        -173.822470   \n",
       "1   {'classifier__default_parameters': {'m': 0, 'p...        -161.775152   \n",
       "2   {'classifier__default_parameters': {'m': 0, 'p...        -149.739296   \n",
       "3   {'classifier__default_parameters': {'m': 0, 'p...        -137.841944   \n",
       "4   {'classifier__default_parameters': {'m': 0, 'p...        -126.162471   \n",
       "5   {'classifier__default_parameters': {'m': 0, 'p...        -115.137118   \n",
       "6   {'classifier__default_parameters': {'m': 0, 'p...        -105.776255   \n",
       "7   {'classifier__default_parameters': {'m': 0, 'p...         -99.705536   \n",
       "8   {'classifier__default_parameters': {'m': 0, 'p...         -98.045459   \n",
       "9   {'classifier__default_parameters': {'m': 0, 'p...         -99.866744   \n",
       "10  {'classifier__default_parameters': {'m': 0, 'p...         -98.696509   \n",
       "11  {'classifier__default_parameters': {'m': 0, 'p...         -86.275862   \n",
       "12  {'classifier__default_parameters': {'m': 0, 'p...         -68.009046   \n",
       "13  {'classifier__default_parameters': {'m': 0, 'p...         -52.046354   \n",
       "14  {'classifier__default_parameters': {'m': 0, 'p...         -39.035281   \n",
       "15  {'classifier__default_parameters': {'m': 0, 'p...         -28.239177   \n",
       "16  {'classifier__default_parameters': {'m': 0, 'p...         -19.193555   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0         -184.260668        -171.280747        -181.702957  ...   \n",
       "1         -171.053824        -159.271037        -169.585077  ...   \n",
       "2         -157.614562        -147.223078        -157.594317  ...   \n",
       "3         -144.431796        -135.299507        -145.663396  ...   \n",
       "4         -131.838638        -123.526255        -133.972219  ...   \n",
       "5         -120.080571        -112.396023        -122.854385  ...   \n",
       "6         -110.426577        -102.863712        -113.249369  ...   \n",
       "7         -104.377936         -96.885678        -106.766850  ...   \n",
       "8         -102.944548         -95.799562        -104.836514  ...   \n",
       "9         -104.928288         -98.561601        -106.603056  ...   \n",
       "10        -104.050372         -98.464872        -104.605225  ...   \n",
       "11         -93.148486         -87.754187         -91.440785  ...   \n",
       "12         -77.087976         -71.294239         -73.497522  ...   \n",
       "13         -62.651708         -56.272548         -57.899284  ...   \n",
       "14         -50.502240         -43.564582         -44.972107  ...   \n",
       "15         -40.055747         -32.682269         -33.917811  ...   \n",
       "16         -30.441839         -22.945988         -23.852179  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0       -174.457392        8.177066               17         -395.369373   \n",
       "1       -162.167175        7.898930               16         -383.325038   \n",
       "2       -149.868679        7.588178               15         -371.341451   \n",
       "3       -137.699588        7.338073               14         -359.504733   \n",
       "4       -125.822033        7.171207               13         -348.044474   \n",
       "5       -114.635247        6.996109               12         -337.563157   \n",
       "6       -105.193055        6.801717               11         -329.504017   \n",
       "7        -99.178857        6.505338                8         -326.724765   \n",
       "8        -97.888664        5.994283                7         -333.176049   \n",
       "9       -100.315495        5.286567               10         -351.589467   \n",
       "10       -99.616639        4.488784                9         -378.891659   \n",
       "11       -88.197702        3.818103                6         -402.329661   \n",
       "12       -71.382253        3.672153                5         -411.049270   \n",
       "13       -56.318227        3.841928                4         -407.765645   \n",
       "14       -43.683390        4.023344                3         -399.248778   \n",
       "15       -32.918882        4.104957                2         -389.681373   \n",
       "16       -23.377576        3.908114                1         -381.214817   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0          -386.387963         -396.213989         -390.293968   \n",
       "1          -374.921836         -384.174720         -378.252684   \n",
       "2          -363.516496         -372.202477         -366.272361   \n",
       "3          -352.252240         -360.397984         -354.444280   \n",
       "4          -341.332866         -349.013277         -342.997826   \n",
       "5          -331.320997         -338.667527         -332.547011   \n",
       "6          -323.683949         -330.703741         -324.531026   \n",
       "7          -321.419210         -327.744144         -321.671240   \n",
       "8          -328.549920         -333.648274         -327.703771   \n",
       "9          -347.752137         -351.260553         -345.817551   \n",
       "10         -375.355022         -377.424366         -373.452304   \n",
       "11         -398.166907         -399.295353         -396.913986   \n",
       "12         -406.326340         -406.834239         -405.298473   \n",
       "13         -403.162038         -403.088912         -401.885418   \n",
       "14         -395.351048         -394.608286         -393.577672   \n",
       "15         -387.194734         -385.578677         -384.761997   \n",
       "16         -381.153912         -378.324985         -377.839372   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0          -407.053889       -395.063836         6.974238  \n",
       "1          -395.010604       -383.136976         6.832864  \n",
       "2          -383.005650       -371.267687         6.688696  \n",
       "3          -371.129515       -359.545750         6.544186  \n",
       "4          -359.573375       -348.192364         6.391543  \n",
       "5          -348.873372       -337.794413         6.212882  \n",
       "6          -340.344349       -329.753416         5.953906  \n",
       "7          -336.608309       -326.833534         5.519990  \n",
       "8          -341.437608       -332.903125         4.887756  \n",
       "9          -357.954349       -350.874811         4.149852  \n",
       "10         -383.232877       -377.671246         3.336059  \n",
       "11         -404.053733       -400.151928         2.650468  \n",
       "12         -410.648569       -408.031378         2.356541  \n",
       "13         -406.381438       -404.456690         2.227654  \n",
       "14         -397.590057       -396.075168         2.063020  \n",
       "15         -388.157631       -387.074883         1.764572  \n",
       "16         -380.197177       -379.746053         1.414052  \n",
       "\n",
       "[17 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"pd.DataFrame(grid_search.cv_results_)\";\n",
       "                var nbb_formatted_code = \"pd.DataFrame(grid_search.cv_results_)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13083558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': <__main__.Logit at 0x7fbc90d93400>,\n",
       " 'prior_parameters': {},\n",
       " 'default_parameters': {'m': 0, 'p': 100000.0},\n",
       " 'optimize_kwargs': {},\n",
       " 'optimize_method': 'L-BFGS-B',\n",
       " 'posterior_parameters_': {'categorical_features_wo_nan__Pclass_1': {'m': 0.00048567248387854333,\n",
       "   'p': 100051.77665820834},\n",
       "  'categorical_features_wo_nan__Pclass_2': {'m': 9.255857146504486e-05,\n",
       "   'p': 100044.67864801607},\n",
       "  'categorical_features_wo_nan__Pclass_3': {'m': -0.0009256628034478769,\n",
       "   'p': 100120.0326040957},\n",
       "  'categorical_features_wo_nan__Sex_female': {'m': 0.0009892216366017106,\n",
       "   'p': 100076.49285475907},\n",
       "  'categorical_features_wo_nan__Sex_male': {'m': -0.0013366533847059988,\n",
       "   'p': 100139.99505556104},\n",
       "  'categorical_features_wo_nan__Embarked_C': {'m': 0.00022275006651386415,\n",
       "   'p': 100040.7561589734},\n",
       "  'categorical_features_wo_nan__Embarked_Q': {'m': -2.661741747544523e-05,\n",
       "   'p': 100018.75827769526},\n",
       "  'categorical_features_wo_nan__Embarked_S': {'m': -0.0005561036688456341,\n",
       "   'p': 100156.5074304822},\n",
       "  'categorical_features_wo_nan__Embarked_unknown': {'m': 1.2539271702926959e-05,\n",
       "   'p': 100000.46604316925},\n",
       "  'categorical_features_wo_nan__CabinCat_A': {'m': 1.102762717896439e-05,\n",
       "   'p': 100003.54836215188},\n",
       "  'categorical_features_wo_nan__CabinCat_B': {'m': 0.00015689958487469395,\n",
       "   'p': 100011.31000538325},\n",
       "  'categorical_features_wo_nan__CabinCat_C': {'m': 0.00010848946512474281,\n",
       "   'p': 100014.1976372167},\n",
       "  'categorical_features_wo_nan__CabinCat_D': {'m': 0.00011756989403364364,\n",
       "   'p': 100007.8882841226},\n",
       "  'categorical_features_wo_nan__CabinCat_E': {'m': 0.00011095615789966003,\n",
       "   'p': 100007.67061459979},\n",
       "  'categorical_features_wo_nan__CabinCat_F': {'m': 2.2269500842045618e-05,\n",
       "   'p': 100003.19297507669},\n",
       "  'categorical_features_wo_nan__CabinCat_G': {'m': 1.53900320115826e-06,\n",
       "   'p': 100000.99026336461},\n",
       "  'categorical_features_wo_nan__CabinCat_T': {'m': -3.8414729301695364e-06,\n",
       "   'p': 100000.23657806151},\n",
       "  'categorical_features_wo_nan__CabinCat_n': {'m': -0.000872341508329028,\n",
       "   'p': 100167.45319034309},\n",
       "  'numerical_features_w_nan__Age': {'m': -0.010457165818790312,\n",
       "   'p': 322988.1503064388},\n",
       "  'remainder__SibSp': {'m': -0.00042543945781483363, 'p': 100325.94464264488},\n",
       "  'remainder__Parch': {'m': 0.00010893393911551836, 'p': 100172.10924812553}}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"grid_search.best_estimator_[-1].__dict__\";\n",
       "                var nbb_formatted_code = \"grid_search.best_estimator_[-1].__dict__\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search.best_estimator_[-1].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9860aa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categorical_features_wo_nan__Pclass_1': {'m': 2.8823595037971328e-05,\n",
       "  'p': 1000044.4060644698},\n",
       " 'categorical_features_wo_nan__Pclass_2': {'m': -4.939479751131075e-06,\n",
       "  'p': 1000035.1948734885},\n",
       " 'categorical_features_wo_nan__Pclass_3': {'m': -9.439329066229653e-05,\n",
       "  'p': 1000098.1401714905},\n",
       " 'categorical_features_wo_nan__Sex_female': {'m': 6.422891656019559e-05,\n",
       "  'p': 1000062.9152331759},\n",
       " 'categorical_features_wo_nan__Sex_male': {'m': -0.00013473809193565188,\n",
       "  'p': 1000114.825876273},\n",
       " 'categorical_features_wo_nan__Embarked_C': {'m': 1.1460545086801027e-05,\n",
       "  'p': 1000033.4472216371},\n",
       " 'categorical_features_wo_nan__Embarked_Q': {'m': -5.4764617318361395e-06,\n",
       "  'p': 1000015.2312278064},\n",
       " 'categorical_features_wo_nan__Embarked_S': {'m': -7.702981351766125e-05,\n",
       "  'p': 1000128.8139962578},\n",
       " 'categorical_features_wo_nan__Embarked_unknown': {'m': 5.365547872400719e-07,\n",
       "  'p': 1000000.2486637476},\n",
       " 'categorical_features_wo_nan__CabinCat_A': {'m': -7.89547442680933e-07,\n",
       "  'p': 1000002.4949080395},\n",
       " 'categorical_features_wo_nan__CabinCat_B': {'m': 1.132959717846526e-05,\n",
       "  'p': 1000010.2300330807},\n",
       " 'categorical_features_wo_nan__CabinCat_C': {'m': 5.43367368537932e-06,\n",
       "  'p': 1000011.7284843255},\n",
       " 'categorical_features_wo_nan__CabinCat_D': {'m': 8.164206425664817e-06,\n",
       "  'p': 1000007.2327887978},\n",
       " 'categorical_features_wo_nan__CabinCat_E': {'m': 7.1183873177032e-06,\n",
       "  'p': 1000006.734595502},\n",
       " 'categorical_features_wo_nan__CabinCat_F': {'m': 1.4210627109746397e-07,\n",
       "  'p': 1000002.4974515615},\n",
       " 'categorical_features_wo_nan__CabinCat_G': {'m': 3.5001066933818585e-08,\n",
       "  'p': 1000000.999495819},\n",
       " 'categorical_features_wo_nan__CabinCat_T': {'m': -4.7338117191792016e-07,\n",
       "  'p': 1000000.2492914381},\n",
       " 'categorical_features_wo_nan__CabinCat_n': {'m': -0.00010146921870610128,\n",
       "  'p': 1000135.5740608847},\n",
       " 'numerical_features_w_nan__Age': {'m': -0.0023642809924155294,\n",
       "  'p': 1183982.2268910804},\n",
       " 'remainder__SibSp': {'m': -4.932651177787904e-05, 'p': 1000250.529653173},\n",
       " 'remainder__Parch': {'m': -5.3654313618666766e-06, 'p': 1000146.0254964967}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"pipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\n            \\\"classifier\\\",\\n            BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1e6}),\\n        ),\\n    ]\\n)\\npipe.fit(X_train, y_train)\\npipe[-1].posterior_parameters_\";\n",
       "                var nbb_formatted_code = \"pipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\n            \\\"classifier\\\",\\n            BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1e6}),\\n        ),\\n    ]\\n)\\npipe.fit(X_train, y_train)\\npipe[-1].posterior_parameters_\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            BayesianLogisticRegression(default_parameters={\"m\": 0, \"p\": 1e6}),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe[-1].posterior_parameters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2407a3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 60;\n",
       "                var nbb_unformatted_code = \"m_posterior = np.array(\\n    [\\n        pipe[-1].posterior_parameters_.get(col, pipe[-1].default_parameters)[\\\"m\\\"]\\n        for col in X_train.columns\\n    ]\\n)\\nm_prior = np.array(\\n    [\\n        pipe[-1].prior_parameters.get(col, pipe[-1].default_parameters)[\\\"m\\\"]\\n        for col in X_train.columns\\n    ]\\n)\\np_prior = np.array(\\n    [\\n        pipe[-1].prior_parameters.get(col, pipe[-1].default_parameters)[\\\"p\\\"]\\n        for col in X_train.columns\\n    ]\\n)\";\n",
       "                var nbb_formatted_code = \"m_posterior = np.array(\\n    [\\n        pipe[-1].posterior_parameters_.get(col, pipe[-1].default_parameters)[\\\"m\\\"]\\n        for col in X_train.columns\\n    ]\\n)\\nm_prior = np.array(\\n    [\\n        pipe[-1].prior_parameters.get(col, pipe[-1].default_parameters)[\\\"m\\\"]\\n        for col in X_train.columns\\n    ]\\n)\\np_prior = np.array(\\n    [\\n        pipe[-1].prior_parameters.get(col, pipe[-1].default_parameters)[\\\"p\\\"]\\n        for col in X_train.columns\\n    ]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_posterior = np.array(\n",
    "    [\n",
    "        pipe[-1].posterior_parameters_.get(col, pipe[-1].default_parameters)[\"m\"]\n",
    "        for col in X_train.columns\n",
    "    ]\n",
    ")\n",
    "m_prior = np.array(\n",
    "    [\n",
    "        pipe[-1].prior_parameters.get(col, pipe[-1].default_parameters)[\"m\"]\n",
    "        for col in X_train.columns\n",
    "    ]\n",
    ")\n",
    "p_prior = np.array(\n",
    "    [\n",
    "        pipe[-1].prior_parameters.get(col, pipe[-1].default_parameters)[\"p\"]\n",
    "        for col in X_train.columns\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d1bcaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-06, 1.e-06, 1.e-06, 1.e-06, 1.e-06, 1.e-06, 1.e-06])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 63;\n",
       "                var nbb_unformatted_code = \"p_prior\";\n",
       "                var nbb_formatted_code = \"p_prior\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a71177dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 62;\n",
       "                var nbb_unformatted_code = \"m_prior\";\n",
       "                var nbb_formatted_code = \"m_prior\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6040f0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 61;\n",
       "                var nbb_unformatted_code = \"m_posterior\";\n",
       "                var nbb_formatted_code = \"m_posterior\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bbfc2e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-467.79030629118137"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 59;\n",
       "                var nbb_unformatted_code = \"pipe.score(X_train, y_train)\";\n",
       "                var nbb_formatted_code = \"pipe.score(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f9dfd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47157975, 0.48243986, 0.48222322, 0.48461478, 0.47924493,\n",
       "       0.4805114 , 0.47861894, 0.48584558, 0.48578834, 0.48932987,\n",
       "       0.48099071, 0.47162524, 0.48247567, 0.47863992, 0.47924469,\n",
       "       0.48247343, 0.4845352 , 0.48243986, 0.48248507, 0.48247276,\n",
       "       0.48869509, 0.47404522, 0.47931738, 0.47220443, 0.49634947,\n",
       "       0.48575367, 0.48242892, 0.48689755, 0.47870435, 0.482796  ,\n",
       "       0.48243986, 0.49699252, 0.4798227 , 0.46804627, 0.48992823,\n",
       "       0.48396706, 0.4863069 , 0.4834263 , 0.47397432, 0.46869498,\n",
       "       0.48807895, 0.46814126, 0.47568341, 0.49694051, 0.48247276,\n",
       "       0.48748823, 0.47339718, 0.48811634, 0.48402161, 0.48748823,\n",
       "       0.48241753, 0.4733956 , 0.48869205, 0.48240521, 0.48243539,\n",
       "       0.4893102 , 0.47808774, 0.47873846, 0.48073001, 0.48402497,\n",
       "       0.49758712, 0.4816416 , 0.48247646, 0.4811229 , 0.48337662,\n",
       "       0.48098069, 0.48170084, 0.4964225 , 0.47931738, 0.48933121,\n",
       "       0.48282359, 0.48241753, 0.47684389, 0.48689755, 0.47045116,\n",
       "       0.48280821, 0.48992313, 0.47107593, 0.48877839, 0.46984175,\n",
       "       0.48985974, 0.48241753, 0.48245221, 0.46981263, 0.48282203,\n",
       "       0.47689353, 0.48755582, 0.48697671, 0.47214666, 0.48514807,\n",
       "       0.48253592, 0.47103544, 0.48045982, 0.48635661, 0.4793251 ,\n",
       "       0.49759527, 0.48244829, 0.49867762, 0.48341399, 0.48243539,\n",
       "       0.48243539, 0.48286128, 0.48810106, 0.48394472, 0.48280903,\n",
       "       0.48046173, 0.4710341 , 0.48514783, 0.48247276, 0.47745273,\n",
       "       0.48275157, 0.4816877 , 0.4621818 , 0.49104145, 0.49281553,\n",
       "       0.47391471, 0.48101087, 0.48339483, 0.49929201, 0.48455754,\n",
       "       0.48099301, 0.4886697 , 0.49044213, 0.47511608, 0.48571629,\n",
       "       0.45864026, 0.48099301, 0.48748823, 0.49050974, 0.48241753,\n",
       "       0.48807895, 0.48244829, 0.4863069 , 0.47338355, 0.49752016,\n",
       "       0.48702565, 0.46929619, 0.47745273, 0.46714112, 0.49333237,\n",
       "       0.48584706, 0.48241753, 0.48243539, 0.49463686, 0.48247343,\n",
       "       0.48748823, 0.48160111, 0.48694726, 0.48691965, 0.48242308,\n",
       "       0.48243962, 0.46931878, 0.4824893 , 0.48231632, 0.48753794,\n",
       "       0.48227608, 0.48926048, 0.47338412, 0.47332528, 0.48578566,\n",
       "       0.47697878, 0.47635104, 0.48243539, 0.48335428, 0.48337528,\n",
       "       0.4845352 , 0.48926048, 0.48932808, 0.46282502, 0.48248507,\n",
       "       0.48512573, 0.47045116, 0.48243986, 0.48247499, 0.4562709 ,\n",
       "       0.48276389, 0.48924681, 0.4839545 , 0.48689755, 0.47308796,\n",
       "       0.48049803, 0.48252375, 0.47928046, 0.48242597, 0.48695044,\n",
       "       0.46098927, 0.49880821, 0.47565881, 0.48243962, 0.48869205,\n",
       "       0.49518109, 0.4886697 , 0.49041748, 0.47096834, 0.4834263 ,\n",
       "       0.48247343, 0.47866993, 0.47042476, 0.48512573, 0.46803399,\n",
       "       0.47581292, 0.49868318, 0.48335428, 0.48778358, 0.49110484,\n",
       "       0.48241753, 0.4751657 , 0.48584558, 0.48343608, 0.46687754,\n",
       "       0.48335428, 0.47511911, 0.48226342, 0.4886697 , 0.48241753,\n",
       "       0.48278622, 0.48578383, 0.47869225, 0.48341175, 0.49394801,\n",
       "       0.46808171, 0.48807895, 0.48248507, 0.48242308, 0.48162986,\n",
       "       0.48512573, 0.48937749, 0.48248239, 0.48219588, 0.48748823,\n",
       "       0.48227608, 0.4886697 , 0.4851631 , 0.49749415, 0.49110061,\n",
       "       0.47930359, 0.47752468, 0.47224065, 0.48243986, 0.48224556,\n",
       "       0.47749783, 0.49046449, 0.49967062, 0.48584706, 0.48985129,\n",
       "       0.48334196, 0.49572793, 0.4834263 , 0.49750105, 0.48240521,\n",
       "       0.48241753, 0.49344069, 0.48810106, 0.48243962, 0.48994767,\n",
       "       0.49458874, 0.46635725, 0.48994548, 0.47281682, 0.4952391 ,\n",
       "       0.49878973, 0.48187839, 0.48807895, 0.48694726, 0.48248662,\n",
       "       0.4646109 , 0.47927544, 0.48158325, 0.48689755, 0.48222322,\n",
       "       0.48750611, 0.47632592, 0.48241753, 0.48099301, 0.48217354,\n",
       "       0.46984809, 0.48220566, 0.48243986, 0.49941619, 0.48243986,\n",
       "       0.48241753, 0.47981268, 0.48243962, 0.47159074, 0.48870709,\n",
       "       0.48102012, 0.47863259, 0.48241753, 0.48248507, 0.47931738,\n",
       "       0.48241753, 0.48241753, 0.48248507, 0.48877492, 0.48243962,\n",
       "       0.48517542, 0.48162061, 0.48689755, 0.47338118, 0.48240253,\n",
       "       0.49815898, 0.48243962, 0.48277391, 0.47745273, 0.47510378,\n",
       "       0.49866529, 0.47625546, 0.48246196, 0.48457257, 0.47983501,\n",
       "       0.48332964, 0.48248507, 0.48581372, 0.48241753, 0.48219564,\n",
       "       0.48248507, 0.48868758, 0.49114037, 0.4639644 , 0.49925635,\n",
       "       0.48163292, 0.48243539, 0.48248507, 0.48926048, 0.48511341,\n",
       "       0.48404387, 0.48511341, 0.48985129, 0.48748823, 0.48936544,\n",
       "       0.46804627, 0.47627312, 0.48040282, 0.49872602, 0.48515111,\n",
       "       0.48219564, 0.48106501, 0.48338505, 0.48247699, 0.48759169,\n",
       "       0.48246044, 0.49951077, 0.48576236, 0.48700383, 0.47697836,\n",
       "       0.48243539, 0.47461133, 0.48241753, 0.48160558, 0.48241753,\n",
       "       0.4851436 , 0.48928555, 0.48542101, 0.49875005, 0.48163915,\n",
       "       0.48246196, 0.47692368, 0.46456086, 0.47597558, 0.48812866,\n",
       "       0.48519341, 0.47988467, 0.4768852 , 0.48241753, 0.48926048,\n",
       "       0.49044213, 0.46922376, 0.4893102 , 0.47513954, 0.48241753,\n",
       "       0.47515158, 0.49748182, 0.48399029, 0.4745042 , 0.48247039,\n",
       "       0.485714  , 0.47808708, 0.48243539, 0.48689755, 0.46163388,\n",
       "       0.48160558, 0.48688522, 0.47631044, 0.4798227 , 0.48040282,\n",
       "       0.476389  , 0.46811397, 0.4759961 , 0.47038542, 0.49950065,\n",
       "       0.48519643, 0.49758089, 0.48247499, 0.4792907 , 0.48042515,\n",
       "       0.48243539, 0.49167362, 0.48241076, 0.48689755, 0.48869205,\n",
       "       0.48458489, 0.48248507, 0.46749268, 0.48632925, 0.46979035,\n",
       "       0.49168341, 0.48243539, 0.48630326, 0.48107402, 0.47801801,\n",
       "       0.48399173, 0.46807952, 0.47339196, 0.48340397, 0.48248954,\n",
       "       0.48247499, 0.49451434, 0.48400443, 0.48397549, 0.48276389,\n",
       "       0.47220443, 0.48694726, 0.47573036, 0.4768852 , 0.47981268,\n",
       "       0.48570397, 0.47922261, 0.47867649, 0.48696513, 0.48637445,\n",
       "       0.48220957, 0.46750719, 0.47872615, 0.48228215, 0.48243539,\n",
       "       0.48248507, 0.4775088 , 0.47335723, 0.48282359, 0.4893102 ,\n",
       "       0.48992337, 0.48926048, 0.46925879, 0.48748823, 0.48099301,\n",
       "       0.47981268, 0.48217354, 0.494565  , 0.4798227 , 0.48243962,\n",
       "       0.48241753, 0.49228556, 0.48247039, 0.482366  , 0.48282526,\n",
       "       0.48241753, 0.48250102, 0.46157234, 0.49877439, 0.47865491,\n",
       "       0.47686289, 0.48101534, 0.47748337, 0.48751058, 0.48871556,\n",
       "       0.47863259, 0.48042491, 0.49228666, 0.48243962, 0.4845573 ,\n",
       "       0.48243962, 0.48246721, 0.48454498, 0.49876435, 0.47932969,\n",
       "       0.48247699, 0.47161437, 0.47391471, 0.46510414, 0.49044213,\n",
       "       0.4845352 , 0.45866307, 0.48394472, 0.47155746, 0.4783826 ,\n",
       "       0.48874177, 0.4633484 , 0.47695671, 0.48748823, 0.48571629,\n",
       "       0.47922261, 0.48040282, 0.47044639, 0.47575401, 0.48867838,\n",
       "       0.47869043, 0.48996831, 0.48512573, 0.48748823, 0.48402653,\n",
       "       0.48458489, 0.47220401, 0.48251235, 0.47629519, 0.49457599,\n",
       "       0.48246076, 0.48396706, 0.48250775, 0.48241499, 0.48246721,\n",
       "       0.49807288, 0.48985129, 0.48575968, 0.46808064, 0.47691874,\n",
       "       0.4886697 , 0.49571004, 0.48335428, 0.47922261, 0.48936395,\n",
       "       0.48642456, 0.49934883, 0.47987102, 0.48579735, 0.47865491,\n",
       "       0.48402019, 0.49759226, 0.48571629, 0.48394472, 0.49155957,\n",
       "       0.48248507, 0.4786777 , 0.48759612, 0.48512573, 0.48399441,\n",
       "       0.48518411, 0.48873729, 0.48241753, 0.4886697 , 0.48228215,\n",
       "       0.47337488, 0.48394472, 0.48748823, 0.48243962, 0.49339444,\n",
       "       0.48099301, 0.473356  , 0.46697437, 0.4823032 , 0.48241753,\n",
       "       0.48930886, 0.48459358, 0.46161217, 0.46979035, 0.49939682,\n",
       "       0.48248507, 0.4845202 , 0.46344521, 0.47396532, 0.47876174,\n",
       "       0.47862028, 0.47453349, 0.48511341, 0.48243539, 0.48451056,\n",
       "       0.48217354, 0.48872944, 0.47924493, 0.48219588, 0.48241753,\n",
       "       0.47627732, 0.48241753, 0.47519619, 0.49403473, 0.48933097,\n",
       "       0.47214666, 0.48699242, 0.48231632, 0.48252007, 0.47748347,\n",
       "       0.48573864, 0.48041555, 0.48241753, 0.48283299, 0.48104689,\n",
       "       0.48237924, 0.49042578, 0.48335428, 0.49760435, 0.4657474 ,\n",
       "       0.48689755, 0.48251303, 0.48632925, 0.48754507, 0.47929459,\n",
       "       0.46922376, 0.48929787, 0.48808739, 0.49942719, 0.48099301,\n",
       "       0.47167259, 0.47050838, 0.48244829, 0.48243962, 0.48241753,\n",
       "       0.46924559, 0.48253444, 0.48241753, 0.48308117, 0.48278598,\n",
       "       0.47161522, 0.48242892, 0.47103389, 0.4917176 , 0.46450239,\n",
       "       0.48042491, 0.46692463, 0.48807895, 0.47160251, 0.48932777,\n",
       "       0.4828725 , 0.48243539, 0.4845352 , 0.49042754, 0.48243962,\n",
       "       0.48079701, 0.48245489, 0.49517837, 0.48163292, 0.47686289,\n",
       "       0.47108569, 0.47511608, 0.46396361, 0.49811122, 0.48337662,\n",
       "       0.48248507, 0.48276389, 0.49457641, 0.48752562, 0.48519446,\n",
       "       0.49057161, 0.48689755, 0.47953972, 0.49044213, 0.48247499,\n",
       "       0.47050825, 0.49055084, 0.48578867, 0.4886697 , 0.48573864,\n",
       "       0.48806662, 0.47873192, 0.48807895, 0.48248507, 0.48632925,\n",
       "       0.48247276, 0.48243539, 0.4886697 , 0.47871223, 0.48247039,\n",
       "       0.48219588, 0.48241753, 0.46217282, 0.48231632, 0.4981363 ,\n",
       "       0.47392471, 0.48276389, 0.47509377, 0.48335428, 0.49157324,\n",
       "       0.48452288, 0.48248507, 0.48642845, 0.47040159, 0.49934309,\n",
       "       0.48244829, 0.48983897, 0.48241753, 0.4703793 , 0.482366  ,\n",
       "       0.47686289, 0.48241753, 0.48241753, 0.48868754, 0.48691965,\n",
       "       0.48243539, 0.48217354, 0.4852179 , 0.48241753, 0.47404388,\n",
       "       0.48632925, 0.47275823, 0.48515517, 0.48571629, 0.46277939,\n",
       "       0.48638584, 0.45808364, 0.47633199, 0.4886697 , 0.48337662,\n",
       "       0.48248566, 0.48514807, 0.48689755, 0.47983501, 0.48694726,\n",
       "       0.47870456, 0.48241753, 0.46988563, 0.48246721, 0.47979903,\n",
       "       0.48807895, 0.47863259])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 55;\n",
       "                var nbb_unformatted_code = \"pipe.predict(X_train)\";\n",
       "                var nbb_formatted_code = \"pipe.predict(X_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d97631d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categorical_features_wo_nan__Pclass_1': {'m': 1.7066025248427965,\n",
       "  'p': 25.849999558560956},\n",
       " 'categorical_features_wo_nan__Pclass_2': {'m': 1.0234970491055668,\n",
       "  'p': 21.920149992985873},\n",
       " 'categorical_features_wo_nan__Pclass_3': {'m': -0.11084733550471265,\n",
       "  'p': 48.557243400617764},\n",
       " 'categorical_features_wo_nan__Sex_female': {'m': 2.6637129623899214,\n",
       "  'p': 38.953699129364836},\n",
       " 'categorical_features_wo_nan__Sex_male': {'m': -0.04446072395099028,\n",
       "  'p': 57.373692822799754},\n",
       " 'categorical_features_wo_nan__Embarked_C': {'m': -0.6464021015862894,\n",
       "  'p': 18.22649160336165},\n",
       " 'categorical_features_wo_nan__Embarked_Q': {'m': -0.8421875360313485,\n",
       "  'p': 9.613322520882372},\n",
       " 'categorical_features_wo_nan__Embarked_S': {'m': -1.3193040034115493,\n",
       "  'p': 68.48726601229781},\n",
       " 'categorical_features_wo_nan__Embarked_unknown': {'m': 5.4271458794734695,\n",
       "  'p': 0.0003138156227680167},\n",
       " 'categorical_features_wo_nan__CabinCat_A': {'m': 0.5613900446611643,\n",
       "  'p': 2.0716773607330867},\n",
       " 'categorical_features_wo_nan__CabinCat_B': {'m': 1.3850056658807588,\n",
       "  'p': 4.7414555809419925},\n",
       " 'categorical_features_wo_nan__CabinCat_C': {'m': 0.8332645829244436,\n",
       "  'p': 7.160313031858371},\n",
       " 'categorical_features_wo_nan__CabinCat_D': {'m': 1.69928328568486,\n",
       "  'p': 3.8988503714234257},\n",
       " 'categorical_features_wo_nan__CabinCat_E': {'m': 2.2194935303812713,\n",
       "  'p': 3.702378498632382},\n",
       " 'categorical_features_wo_nan__CabinCat_F': {'m': 1.4017543063517086,\n",
       "  'p': 1.6147431920046118},\n",
       " 'categorical_features_wo_nan__CabinCat_G': {'m': -0.25395011133173745,\n",
       "  'p': 0.9041089502688957},\n",
       " 'categorical_features_wo_nan__CabinCat_T': {'m': -5.726157359550431,\n",
       "  'p': 0.00047997784982506414},\n",
       " 'categorical_features_wo_nan__CabinCat_n': {'m': 0.4991682934456451,\n",
       "  'p': 72.233391988452},\n",
       " 'numerical_features_w_nan__Age': {'m': -0.050212844386584674,\n",
       "  'p': 97273.62471694627},\n",
       " 'remainder__SibSp': {'m': -0.3308396695743961, 'p': 99.5956683090262},\n",
       " 'remainder__Parch': {'m': -0.06108215314996051, 'p': 88.49332523296579}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 56;\n",
       "                var nbb_unformatted_code = \"pipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\n            \\\"classifier\\\",\\n            BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1e-6}),\\n        ),\\n    ]\\n)\\npipe.fit(X_train, y_train)\\npipe[-1].posterior_parameters_\";\n",
       "                var nbb_formatted_code = \"pipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\n            \\\"classifier\\\",\\n            BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1e-6}),\\n        ),\\n    ]\\n)\\npipe.fit(X_train, y_train)\\npipe[-1].posterior_parameters_\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            BayesianLogisticRegression(default_parameters={\"m\": 0, \"p\": 1e-6}),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe[-1].posterior_parameters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77045f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06850741, 0.08460182, 0.25041119, 0.73407771, 0.89336204])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 58;\n",
       "                var nbb_unformatted_code = \"np.quantile(pipe.predict(X_train), [0.1,0.2,0.5,0.8,0.9])\";\n",
       "                var nbb_formatted_code = \"np.quantile(pipe.predict(X_train), [0.1, 0.2, 0.5, 0.8, 0.9])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.quantile(pipe.predict(X_train), [0.1, 0.2, 0.5, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10e1d1a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__fit_intercept</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.114581</td>\n",
       "      <td>0.051303</td>\n",
       "      <td>0.044885</td>\n",
       "      <td>0.030787</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 0.001, 'classifier__fit_inte...</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618404</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>22</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.618513</td>\n",
       "      <td>0.615708</td>\n",
       "      <td>0.615708</td>\n",
       "      <td>0.618513</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.001301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.220788</td>\n",
       "      <td>0.173720</td>\n",
       "      <td>0.029648</td>\n",
       "      <td>0.015426</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__fit_interc...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.016070</td>\n",
       "      <td>14</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.816269</td>\n",
       "      <td>0.805049</td>\n",
       "      <td>0.805049</td>\n",
       "      <td>0.811728</td>\n",
       "      <td>0.006106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.418276</td>\n",
       "      <td>0.153319</td>\n",
       "      <td>0.061758</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 0.2, 'classifier__fit_interc...</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799096</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818820</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>0.813464</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.805049</td>\n",
       "      <td>0.813694</td>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487942</td>\n",
       "      <td>0.214513</td>\n",
       "      <td>0.061554</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 0.5, 'classifier__fit_interc...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801356</td>\n",
       "      <td>0.016003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.813202</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>0.817672</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.812062</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.004508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322878</td>\n",
       "      <td>0.076468</td>\n",
       "      <td>0.053918</td>\n",
       "      <td>0.011732</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__fit_intercep...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801356</td>\n",
       "      <td>0.012951</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.809257</td>\n",
       "      <td>0.817057</td>\n",
       "      <td>0.006853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.272265</td>\n",
       "      <td>0.030051</td>\n",
       "      <td>0.046046</td>\n",
       "      <td>0.010587</td>\n",
       "      <td>1.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 1.1, 'classifier__fit_interc...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799109</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>5</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.813464</td>\n",
       "      <td>0.817618</td>\n",
       "      <td>0.005691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.206467</td>\n",
       "      <td>0.095098</td>\n",
       "      <td>0.044080</td>\n",
       "      <td>0.027309</td>\n",
       "      <td>1.2</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 1.2, 'classifier__fit_interc...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797985</td>\n",
       "      <td>0.014128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.818179</td>\n",
       "      <td>0.005599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.271312</td>\n",
       "      <td>0.062948</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.014991</td>\n",
       "      <td>1.3</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 1.3, 'classifier__fit_interc...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.012971</td>\n",
       "      <td>3</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.812062</td>\n",
       "      <td>0.818179</td>\n",
       "      <td>0.006569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.331966</td>\n",
       "      <td>0.079762</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.011076</td>\n",
       "      <td>1.4</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 1.4, 'classifier__fit_interc...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797979</td>\n",
       "      <td>0.011793</td>\n",
       "      <td>11</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.810659</td>\n",
       "      <td>0.817618</td>\n",
       "      <td>0.006647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.453501</td>\n",
       "      <td>0.147809</td>\n",
       "      <td>0.046054</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 1.5, 'classifier__fit_interc...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799102</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>6</td>\n",
       "      <td>0.810393</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.813464</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.005295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.386406</td>\n",
       "      <td>0.138167</td>\n",
       "      <td>0.042149</td>\n",
       "      <td>0.017252</td>\n",
       "      <td>1.6</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 1.6, 'classifier__fit_interc...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800226</td>\n",
       "      <td>0.011533</td>\n",
       "      <td>4</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>0.817672</td>\n",
       "      <td>0.813464</td>\n",
       "      <td>0.817057</td>\n",
       "      <td>0.005445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.385071</td>\n",
       "      <td>0.052941</td>\n",
       "      <td>0.047776</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 2, 'classifier__fit_intercep...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799102</td>\n",
       "      <td>0.012995</td>\n",
       "      <td>7</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.827489</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.812062</td>\n",
       "      <td>0.819302</td>\n",
       "      <td>0.006297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.414939</td>\n",
       "      <td>0.068393</td>\n",
       "      <td>0.069023</td>\n",
       "      <td>0.020411</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 3, 'classifier__fit_intercep...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797979</td>\n",
       "      <td>0.009412</td>\n",
       "      <td>11</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.827489</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.809257</td>\n",
       "      <td>0.818461</td>\n",
       "      <td>0.006898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.312866</td>\n",
       "      <td>0.056189</td>\n",
       "      <td>0.067559</td>\n",
       "      <td>0.023231</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 4, 'classifier__fit_intercep...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>17</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.812062</td>\n",
       "      <td>0.817339</td>\n",
       "      <td>0.004620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.355945</td>\n",
       "      <td>0.140808</td>\n",
       "      <td>0.051938</td>\n",
       "      <td>0.022336</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 5, 'classifier__fit_intercep...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>14</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.810659</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.005305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.445338</td>\n",
       "      <td>0.069065</td>\n",
       "      <td>0.075676</td>\n",
       "      <td>0.024825</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 6, 'classifier__fit_intercep...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794608</td>\n",
       "      <td>0.008510</td>\n",
       "      <td>21</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.809257</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.005174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.429614</td>\n",
       "      <td>0.214215</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 7, 'classifier__fit_intercep...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>14</td>\n",
       "      <td>0.810393</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.807854</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.006223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.273992</td>\n",
       "      <td>0.116231</td>\n",
       "      <td>0.049959</td>\n",
       "      <td>0.020581</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 8, 'classifier__fit_intercep...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799102</td>\n",
       "      <td>0.012995</td>\n",
       "      <td>7</td>\n",
       "      <td>0.810393</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.809257</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.006964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.265422</td>\n",
       "      <td>0.097044</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 9, 'classifier__fit_intercep...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>17</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.813464</td>\n",
       "      <td>0.818741</td>\n",
       "      <td>0.005196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.445086</td>\n",
       "      <td>0.059750</td>\n",
       "      <td>0.052628</td>\n",
       "      <td>0.011413</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 10, 'classifier__fit_interce...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794614</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>19</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.807854</td>\n",
       "      <td>0.817619</td>\n",
       "      <td>0.006624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.518678</td>\n",
       "      <td>0.174374</td>\n",
       "      <td>0.069007</td>\n",
       "      <td>0.025861</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 100, 'classifier__fit_interc...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796861</td>\n",
       "      <td>0.011918</td>\n",
       "      <td>13</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.813464</td>\n",
       "      <td>0.818179</td>\n",
       "      <td>0.005873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.442513</td>\n",
       "      <td>0.115612</td>\n",
       "      <td>0.044235</td>\n",
       "      <td>0.013838</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>{'classifier__C': 1000, 'classifier__fit_inter...</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794614</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>20</td>\n",
       "      <td>0.811798</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>0.823282</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.807854</td>\n",
       "      <td>0.817339</td>\n",
       "      <td>0.006526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.114581      0.051303         0.044885        0.030787   \n",
       "1        0.220788      0.173720         0.029648        0.015426   \n",
       "2        0.418276      0.153319         0.061758        0.010992   \n",
       "3        0.487942      0.214513         0.061554        0.014134   \n",
       "4        0.322878      0.076468         0.053918        0.011732   \n",
       "5        0.272265      0.030051         0.046046        0.010587   \n",
       "6        0.206467      0.095098         0.044080        0.027309   \n",
       "7        0.271312      0.062948         0.038308        0.014991   \n",
       "8        0.331966      0.079762         0.040004        0.011076   \n",
       "9        0.453501      0.147809         0.046054        0.007631   \n",
       "10       0.386406      0.138167         0.042149        0.017252   \n",
       "11       0.385071      0.052941         0.047776        0.007506   \n",
       "12       0.414939      0.068393         0.069023        0.020411   \n",
       "13       0.312866      0.056189         0.067559        0.023231   \n",
       "14       0.355945      0.140808         0.051938        0.022336   \n",
       "15       0.445338      0.069065         0.075676        0.024825   \n",
       "16       0.429614      0.214215         0.062168        0.014386   \n",
       "17       0.273992      0.116231         0.049959        0.020581   \n",
       "18       0.265422      0.097044         0.052630        0.017250   \n",
       "19       0.445086      0.059750         0.052628        0.011413   \n",
       "20       0.518678      0.174374         0.069007        0.025861   \n",
       "21       0.442513      0.115612         0.044235        0.013838   \n",
       "\n",
       "   param_classifier__C param_classifier__fit_intercept  \\\n",
       "0                0.001                           False   \n",
       "1                  0.1                           False   \n",
       "2                  0.2                           False   \n",
       "3                  0.5                           False   \n",
       "4                    1                           False   \n",
       "5                  1.1                           False   \n",
       "6                  1.2                           False   \n",
       "7                  1.3                           False   \n",
       "8                  1.4                           False   \n",
       "9                  1.5                           False   \n",
       "10                 1.6                           False   \n",
       "11                   2                           False   \n",
       "12                   3                           False   \n",
       "13                   4                           False   \n",
       "14                   5                           False   \n",
       "15                   6                           False   \n",
       "16                   7                           False   \n",
       "17                   8                           False   \n",
       "18                   9                           False   \n",
       "19                  10                           False   \n",
       "20                 100                           False   \n",
       "21                1000                           False   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'classifier__C': 0.001, 'classifier__fit_inte...           0.620112   \n",
       "1   {'classifier__C': 0.1, 'classifier__fit_interc...           0.798883   \n",
       "2   {'classifier__C': 0.2, 'classifier__fit_interc...           0.804469   \n",
       "3   {'classifier__C': 0.5, 'classifier__fit_interc...           0.793296   \n",
       "4   {'classifier__C': 1, 'classifier__fit_intercep...           0.793296   \n",
       "5   {'classifier__C': 1.1, 'classifier__fit_interc...           0.793296   \n",
       "6   {'classifier__C': 1.2, 'classifier__fit_interc...           0.793296   \n",
       "7   {'classifier__C': 1.3, 'classifier__fit_interc...           0.793296   \n",
       "8   {'classifier__C': 1.4, 'classifier__fit_interc...           0.798883   \n",
       "9   {'classifier__C': 1.5, 'classifier__fit_interc...           0.798883   \n",
       "10  {'classifier__C': 1.6, 'classifier__fit_interc...           0.798883   \n",
       "11  {'classifier__C': 2, 'classifier__fit_intercep...           0.798883   \n",
       "12  {'classifier__C': 3, 'classifier__fit_intercep...           0.798883   \n",
       "13  {'classifier__C': 4, 'classifier__fit_intercep...           0.798883   \n",
       "14  {'classifier__C': 5, 'classifier__fit_intercep...           0.798883   \n",
       "15  {'classifier__C': 6, 'classifier__fit_intercep...           0.798883   \n",
       "16  {'classifier__C': 7, 'classifier__fit_intercep...           0.798883   \n",
       "17  {'classifier__C': 8, 'classifier__fit_intercep...           0.798883   \n",
       "18  {'classifier__C': 9, 'classifier__fit_intercep...           0.798883   \n",
       "19  {'classifier__C': 10, 'classifier__fit_interce...           0.793296   \n",
       "20  {'classifier__C': 100, 'classifier__fit_interc...           0.793296   \n",
       "21  {'classifier__C': 1000, 'classifier__fit_inter...           0.793296   \n",
       "\n",
       "    split1_test_score  split2_test_score  ...  mean_test_score  \\\n",
       "0            0.612360           0.623596  ...         0.618404   \n",
       "1            0.797753           0.797753  ...         0.796855   \n",
       "2            0.797753           0.797753  ...         0.799096   \n",
       "3            0.792135           0.803371  ...         0.801356   \n",
       "4            0.792135           0.803371  ...         0.801356   \n",
       "5            0.792135           0.797753  ...         0.799109   \n",
       "6            0.792135           0.792135  ...         0.797985   \n",
       "7            0.792135           0.797753  ...         0.800232   \n",
       "8            0.792135           0.792135  ...         0.797979   \n",
       "9            0.792135           0.797753  ...         0.799102   \n",
       "10           0.792135           0.803371  ...         0.800226   \n",
       "11           0.792135           0.803371  ...         0.799102   \n",
       "12           0.792135           0.797753  ...         0.797979   \n",
       "13           0.792135           0.792135  ...         0.795732   \n",
       "14           0.792135           0.792135  ...         0.796855   \n",
       "15           0.792135           0.786517  ...         0.794608   \n",
       "16           0.792135           0.786517  ...         0.796855   \n",
       "17           0.792135           0.803371  ...         0.799102   \n",
       "18           0.792135           0.792135  ...         0.795732   \n",
       "19           0.792135           0.786517  ...         0.794614   \n",
       "20           0.792135           0.792135  ...         0.796861   \n",
       "21           0.786517           0.792135  ...         0.794614   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         0.005097               22            0.617978            0.618513   \n",
       "1         0.016070               14            0.811798            0.820477   \n",
       "2         0.017968                9            0.818820            0.824684   \n",
       "3         0.016003                1            0.813202            0.824684   \n",
       "4         0.012951                1            0.808989            0.821879   \n",
       "5         0.013839                5            0.808989            0.821879   \n",
       "6         0.014128               10            0.808989            0.821879   \n",
       "7         0.012971                3            0.808989            0.823282   \n",
       "8         0.011793               11            0.808989            0.821879   \n",
       "9         0.011445                6            0.810393            0.823282   \n",
       "10        0.011533                4            0.808989            0.820477   \n",
       "11        0.012995                7            0.811798            0.823282   \n",
       "12        0.009412               11            0.811798            0.823282   \n",
       "13        0.011064               17            0.811798            0.823282   \n",
       "14        0.009702               14            0.811798            0.823282   \n",
       "15        0.008510               21            0.811798            0.823282   \n",
       "16        0.012540               14            0.810393            0.821879   \n",
       "17        0.012995                7            0.810393            0.823282   \n",
       "18        0.011064               17            0.811798            0.823282   \n",
       "19        0.010380               19            0.811798            0.823282   \n",
       "20        0.011918               13            0.808989            0.823282   \n",
       "21        0.010380               20            0.811798            0.824684   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0             0.615708            0.615708            0.618513   \n",
       "1             0.816269            0.805049            0.805049   \n",
       "2             0.813464            0.806452            0.805049   \n",
       "3             0.817672            0.814867            0.812062   \n",
       "4             0.826087            0.819074            0.809257   \n",
       "5             0.824684            0.819074            0.813464   \n",
       "6             0.824684            0.820477            0.814867   \n",
       "7             0.826087            0.820477            0.812062   \n",
       "8             0.826087            0.820477            0.810659   \n",
       "9             0.823282            0.820477            0.813464   \n",
       "10            0.824684            0.817672            0.813464   \n",
       "11            0.827489            0.821879            0.812062   \n",
       "12            0.827489            0.820477            0.809257   \n",
       "13            0.819074            0.820477            0.812062   \n",
       "14            0.821879            0.820477            0.810659   \n",
       "15            0.819074            0.819074            0.809257   \n",
       "16            0.823282            0.819074            0.807854   \n",
       "17            0.826087            0.821879            0.809257   \n",
       "18            0.824684            0.820477            0.813464   \n",
       "19            0.824684            0.820477            0.807854   \n",
       "20            0.823282            0.821879            0.813464   \n",
       "21            0.823282            0.819074            0.807854   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "0           0.617284         0.001301  \n",
       "1           0.811728         0.006106  \n",
       "2           0.813694         0.007407  \n",
       "3           0.816497         0.004508  \n",
       "4           0.817057         0.006853  \n",
       "5           0.817618         0.005691  \n",
       "6           0.818179         0.005599  \n",
       "7           0.818179         0.006569  \n",
       "8           0.817618         0.006647  \n",
       "9           0.818180         0.005295  \n",
       "10          0.817057         0.005445  \n",
       "11          0.819302         0.006297  \n",
       "12          0.818461         0.006898  \n",
       "13          0.817339         0.004620  \n",
       "14          0.817619         0.005305  \n",
       "15          0.816497         0.005174  \n",
       "16          0.816497         0.006223  \n",
       "17          0.818180         0.006964  \n",
       "18          0.818741         0.005196  \n",
       "19          0.817619         0.006624  \n",
       "20          0.818179         0.005873  \n",
       "21          0.817339         0.006526  \n",
       "\n",
       "[22 rows x 22 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"pd.DataFrame(grid_search.cv_results_)\";\n",
       "                var nbb_formatted_code = \"pd.DataFrame(grid_search.cv_results_)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bffa88cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"grid_search.best_estimator_[-1].C\";\n",
       "                var nbb_formatted_code = \"grid_search.best_estimator_[-1].C\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search.best_estimator_[-1].C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d9c0c",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e12ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42718704252838274\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"y_pred = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\\n\\nprint(log_loss(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\\n\\nprint(log_loss(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e5324",
   "metadata": {},
   "source": [
    "### Bayesian optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb1653",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02a0bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([-0.24125922])=336.7200337884654\n",
      "fun([-0.2588486])=336.71898691804455\n",
      "fun([-0.2588486])=336.7189446090333\n",
      "fun([-0.25880281])=336.7181532136365\n",
      "fun([-0.25880281])=336.7181532136365\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        preprocessor.fit_transform(X_train).to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_formatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        preprocessor.fit_transform(X_train).to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    func_with_bias,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        preprocessor.fit_transform(X_train).to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac_func_with_bias,\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d1cf754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.29537835])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"1 / np.exp(res.x)\";\n",
       "                var nbb_formatted_code = \"1 / np.exp(res.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 / np.exp(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8ba6f",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99c547a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capitaine/01_projects/2023/cr_model/venv_cr_model/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False),\n",
       "                                                  [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;,\n",
       "                                                   &#x27;CabinCat&#x27;]),\n",
       "                                                 (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer())]),\n",
       "                                                  [&#x27;Age&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(C=1.2953783459293147,\n",
       "                                    fit_intercept=False))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-101\" type=\"checkbox\" ><label for=\"sk-estimator-id-101\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False),\n",
       "                                                  [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;,\n",
       "                                                   &#x27;CabinCat&#x27;]),\n",
       "                                                 (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer())]),\n",
       "                                                  [&#x27;Age&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(C=1.2953783459293147,\n",
       "                                    fit_intercept=False))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-102\" type=\"checkbox\" ><label for=\"sk-estimator-id-102\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;categorical_features_wo_nan&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                               sparse_output=False),\n",
       "                                 [&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;, &#x27;CabinCat&#x27;]),\n",
       "                                (&#x27;numerical_features_w_nan&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer())]),\n",
       "                                 [&#x27;Age&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-103\" type=\"checkbox\" ><label for=\"sk-estimator-id-103\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_features_wo_nan</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Embarked&#x27;, &#x27;CabinCat&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-104\" type=\"checkbox\" ><label for=\"sk-estimator-id-104\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-105\" type=\"checkbox\" ><label for=\"sk-estimator-id-105\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_features_w_nan</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Age&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-106\" type=\"checkbox\" ><label for=\"sk-estimator-id-106\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-107\" type=\"checkbox\" ><label for=\"sk-estimator-id-107\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;SibSp&#x27;, &#x27;Parch&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-108\" type=\"checkbox\" ><label for=\"sk-estimator-id-108\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-109\" type=\"checkbox\" ><label for=\"sk-estimator-id-109\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1.2953783459293147, fit_intercept=False)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('categorical_features_wo_nan',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse_output=False),\n",
       "                                                  ['Pclass', 'Sex', 'Embarked',\n",
       "                                                   'CabinCat']),\n",
       "                                                 ('numerical_features_w_nan',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer())]),\n",
       "                                                  ['Age'])])),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.2953783459293147,\n",
       "                                    fit_intercept=False))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 49;\n",
       "                var nbb_unformatted_code = \"pipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\\"classifier\\\", LogisticRegression(fit_intercept=False, C=1 / np.exp(res.x[0]))),\\n    ]\\n)\\npipe.fit(X_train, y_train)\";\n",
       "                var nbb_formatted_code = \"pipe = Pipeline(\\n    [\\n        (\\\"preprocessing\\\", preprocessor),\\n        (\\\"classifier\\\", LogisticRegression(fit_intercept=False, C=1 / np.exp(res.x[0]))),\\n    ]\\n)\\npipe.fit(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(fit_intercept=False, C=1 / np.exp(res.x[0]))),\n",
    "    ]\n",
    ")\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5823562",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "708cb5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4406065074072889\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 50;\n",
       "                var nbb_unformatted_code = \"y_pred = pipe.predict_proba(X_test)[:, 1]\\n\\nprint(log_loss(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = pipe.predict_proba(X_test)[:, 1]\\n\\nprint(log_loss(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769d024",
   "metadata": {},
   "source": [
    "## Multiple sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ef593",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"categorical_features_wo_nan\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            [\"Pclass\", \"Sex\", \"Embarked\", \"CabinCat\"],\n",
    "        ),\n",
    "        (\n",
    "            \"numerical_features_w_nan\",\n",
    "            Pipeline(\n",
    "                [\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                ]\n",
    "            ),\n",
    "            [\"Age\"],\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(fit_intercept=False)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9e9c2",
   "metadata": {},
   "source": [
    "___\n",
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba1c154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun={intermediate_result.fun}\\\")\";\n",
       "                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun={intermediate_result.fun}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun={intermediate_result.fun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "795cacd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun=416.015700488111\n",
      "fun=415.7457521852242\n",
      "fun=415.7028985462279\n",
      "fun=415.70233681909383\n",
      "fun=415.70095393097137\n",
      "fun=415.7002621046415\n",
      "fun=415.6994301589399\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        preprocessor.fit_transform(X).to_numpy(),\\n        y.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_formatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        preprocessor.fit_transform(X).to_numpy(),\\n        y.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    func_with_bias,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        preprocessor.fit_transform(X).to_numpy(),\n",
    "        y.to_numpy(),\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac_func_with_bias,\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f4076e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: ABNORMAL_TERMINATION_IN_LNSRCH\n",
       "  success: False\n",
       "   status: 2\n",
       "      fun: 415.7002601196556\n",
       "        x: [-3.364e-01]\n",
       "      nit: 7\n",
       "      jac: [ 7.452e-05]\n",
       "     nfev: 64\n",
       "     njev: 64\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"res\";\n",
       "                var nbb_formatted_code = \"res\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f60875b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.39996636])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"1/np.exp(res.x)\";\n",
       "                var nbb_formatted_code = \"1 / np.exp(res.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cc7c7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"X1 = preprocessor.fit_transform(X).to_numpy()\\ny1 = y.to_numpy()\";\n",
       "                var nbb_formatted_code = \"X1 = preprocessor.fit_transform(X).to_numpy()\\ny1 = y.to_numpy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1 = preprocessor.fit_transform(X).to_numpy()\n",
    "y1 = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "def9650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun=917.3936949762424\n",
      "fun=909.2815416894933\n",
      "fun=908.7238866764843\n",
      "fun=908.5830502425201\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 182;\n",
       "                var nbb_unformatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        X1,\\n        y1,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_formatted_code = \"res = minimize(\\n    func_with_bias,\\n    np.array([0.0]),\\n    args=(\\n        X1,\\n        y1,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac_func_with_bias,\\n    callback=callback,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    func_with_bias,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        X1,\n",
    "        y1,\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac_func_with_bias,\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c404cc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 908.5830502425201\n",
       "        x: [-3.176e+00]\n",
       "      nit: 4\n",
       "      jac: [ 6.802e-06]\n",
       "     nfev: 8\n",
       "     njev: 8\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 183;\n",
       "                var nbb_unformatted_code = \"res\";\n",
       "                var nbb_formatted_code = \"res\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7db450de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04176261])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 184;\n",
       "                var nbb_unformatted_code = \"np.exp(res.x)\";\n",
       "                var nbb_formatted_code = \"np.exp(res.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.exp(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2e5c6bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianRidge(alpha_1=1000000.0, alpha_2=1000000.0, fit_intercept=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianRidge</label><div class=\"sk-toggleable__content\"><pre>BayesianRidge(alpha_1=1000000.0, alpha_2=1000000.0, fit_intercept=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianRidge(alpha_1=1000000.0, alpha_2=1000000.0, fit_intercept=False)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 185;\n",
       "                var nbb_unformatted_code = \"model = BayesianRidge(\\n    alpha_1=1 / 1 * 1e6,\\n    alpha_2=1e6,\\n    lambda_1=1e-6,\\n    lambda_2=1e-6,\\n    fit_intercept=False,\\n)\\nmodel.fit(X1, y1)\";\n",
       "                var nbb_formatted_code = \"model = BayesianRidge(\\n    alpha_1=1 / 1 * 1e6,\\n    alpha_2=1e6,\\n    lambda_1=1e-6,\\n    lambda_2=1e-6,\\n    fit_intercept=False,\\n)\\nmodel.fit(X1, y1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BayesianRidge(\n",
    "    alpha_1=1 / 1 * 1e6,\n",
    "    alpha_2=1e6,\n",
    "    lambda_1=1e-6,\n",
    "    lambda_2=1e-6,\n",
    "    fit_intercept=False,\n",
    ")\n",
    "model.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0d915f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.189134422067746"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 191;\n",
       "                var nbb_unformatted_code = \"np.log(1 / model.lambda_)\";\n",
       "                var nbb_formatted_code = \"np.log(1 / model.lambda_)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(1 / model.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54a8bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.2375065318942\n",
      "447.80239593850894\n",
      "426.7367949669429\n",
      "417.1934379524614\n",
      "416.017499735816\n",
      "419.8408348902286\n",
      "426.16091279686424\n",
      "433.5910798852872\n",
      "441.47220323542643\n",
      "449.58914179809045\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"for log_sigma2 in [\\n    -4,\\n    -3,-2,-1,0,1,2,3,4,5,\\n]:\\n    print(\\n        func_with_bias(\\n            np.array([log_sigma2]),\\n            X1,\\n            y1,\\n        )\\n    )\";\n",
       "                var nbb_formatted_code = \"for log_sigma2 in [\\n    -4,\\n    -3,\\n    -2,\\n    -1,\\n    0,\\n    1,\\n    2,\\n    3,\\n    4,\\n    5,\\n]:\\n    print(\\n        func_with_bias(\\n            np.array([log_sigma2]),\\n            X1,\\n            y1,\\n        )\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for log_sigma2 in [\n",
    "    -4,\n",
    "    -3,\n",
    "    -2,\n",
    "    -1,\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "]:\n",
    "    print(\n",
    "        func_with_bias(\n",
    "            np.array([log_sigma2]),\n",
    "            X1,\n",
    "            y1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3620fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.81102697e-22])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"1/np.exp(res.x)\";\n",
       "                var nbb_formatted_code = \"1 / np.exp(res.x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 / np.exp(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f9f6a8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 329.1250092965352\n",
       "        x: [ 4.932e+01]\n",
       "      nit: 3\n",
       "      jac: [ 0.000e+00]\n",
       "     nfev: 6\n",
       "     njev: 6\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 127;\n",
       "                var nbb_unformatted_code = \"res\";\n",
       "                var nbb_formatted_code = \"res\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "380eb6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17095.442744711374"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 130;\n",
       "                var nbb_unformatted_code = \"loss(np.array([1.0] * 21), preprocessor.fit_transform(X), y, np.array([2.0] * 21))\";\n",
       "                var nbb_formatted_code = \"loss(np.array([1.0] * 21), preprocessor.fit_transform(X), y, np.array([2.0] * 21))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss(np.array([1.0] * 21), preprocessor.fit_transform(X), y, np.array([2.0] * 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea00ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00532150e-01, 0.00000000e+00, 0.00000000e+00, 1.23977661e-04,\n",
       "        4.08172607e-04, 0.00000000e+00, 0.00000000e+00, 5.32150269e-04,\n",
       "        0.00000000e+00, 4.57763672e-05, 0.00000000e+00, 4.86373901e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.62939453e-04, 4.86373901e-04,\n",
       "        1.06430054e-03],\n",
       "       [0.00000000e+00, 5.05719185e-01, 0.00000000e+00, 4.68254089e-04,\n",
       "        5.24997711e-03, 9.55581665e-04, 0.00000000e+00, 4.76360321e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.10623169e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.90856171e-03, 6.50882721e-03, 4.06169891e-03,\n",
       "        7.76195526e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 5.09937286e-01, 4.88662720e-03,\n",
       "        5.05065918e-03, 6.29043579e-03, 1.90734863e-05, 3.62777710e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.52587891e-05, 0.00000000e+00, 9.57489014e-04,\n",
       "        0.00000000e+00, 8.96453857e-03, 1.07955933e-02, 3.83377075e-03,\n",
       "        1.11389160e-02],\n",
       "       [1.23977661e-04, 4.69207764e-04, 4.88662720e-03, 5.05477905e-01,\n",
       "        0.00000000e+00, 1.94549561e-03, 0.00000000e+00, 3.53145599e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.23977661e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.71661377e-05, 9.56535339e-04,\n",
       "        0.00000000e+00, 4.38213348e-03, 9.17148590e-03, 3.40652466e-03,\n",
       "        6.51168823e-03],\n",
       "       [4.11987305e-04, 5.25283813e-03, 5.05065918e-03, 0.00000000e+00,\n",
       "        5.10711670e-01, 5.30242920e-03, 2.28881836e-05, 5.39398193e-03,\n",
       "        0.00000000e+00, 4.57763672e-05, 0.00000000e+00, 3.66210938e-04,\n",
       "        0.00000000e+00, 1.52587891e-05, 8.01086426e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.49096680e-03, 8.90350342e-03, 4.98199463e-03,\n",
       "        1.34582520e-02],\n",
       "       [0.00000000e+00, 9.55581665e-04, 6.28852844e-03, 1.94549561e-03,\n",
       "        5.29861450e-03, 5.07244110e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.24411011e-03, 4.97627258e-03, 1.77860260e-03,\n",
       "        9.10949707e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.90734863e-05, 0.00000000e+00,\n",
       "        1.90734863e-05, 0.00000000e+00, 5.00019073e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.90734863e-05, 4.38690186e-05, 7.62939453e-05,\n",
       "        1.90734863e-05],\n",
       "       [5.34057617e-04, 4.76074219e-03, 3.62396240e-03, 3.53240967e-03,\n",
       "        5.38635254e-03, 0.00000000e+00, 0.00000000e+00, 5.08922577e-01,\n",
       "        0.00000000e+00, 4.57763672e-05, 0.00000000e+00, 4.88281250e-04,\n",
       "        0.00000000e+00, 1.52587891e-05, 8.08715820e-04, 9.53674316e-04,\n",
       "        0.00000000e+00, 6.60705566e-03, 1.30462646e-02, 6.52694702e-03,\n",
       "        1.08337402e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [4.54187393e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.54187393e-05, 0.00000000e+00, 0.00000000e+00, 4.54187393e-05,\n",
       "        0.00000000e+00, 5.00045419e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.81555748e-04, 0.00000000e+00,\n",
       "        9.08374786e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00000000e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.19209290e-07, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [4.86373901e-04, 0.00000000e+00, 0.00000000e+00, 1.23262405e-04,\n",
       "        3.63111496e-04, 0.00000000e+00, 0.00000000e+00, 4.86373901e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00486374e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.80787659e-04, 4.86373901e-04,\n",
       "        9.72986221e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.66893005e-05, 0.00000000e+00,\n",
       "        1.66893005e-05, 0.00000000e+00, 0.00000000e+00, 1.66893005e-05,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.00016689e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00135803e-04, 0.00000000e+00,\n",
       "        1.66893005e-05],\n",
       "       [0.00000000e+00, 8.10503960e-04, 0.00000000e+00, 1.66893005e-05,\n",
       "        7.93874264e-04, 0.00000000e+00, 0.00000000e+00, 8.10503960e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00810504e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.44267082e-03, 1.16252899e-03,\n",
       "        8.10503960e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 9.55611467e-04, 9.55611467e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.55611467e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00955611e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.00203061e-03, 4.53889370e-05,\n",
       "        9.55611467e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [0.00000000e+00, 4.90951538e-03, 8.96453857e-03, 4.38308716e-03,\n",
       "        9.49096680e-03, 7.24792480e-03, 1.90734863e-05, 6.60705566e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.13874054e-01, 1.37634277e-02, 6.69097900e-03,\n",
       "        1.71203613e-02],\n",
       "       [9.76562500e-04, 6.59179688e-03, 1.09863281e-02, 9.27734375e-03,\n",
       "        8.78906250e-03, 4.88281250e-03, 0.00000000e+00, 1.31835938e-02,\n",
       "        0.00000000e+00, 2.44140625e-04, 0.00000000e+00, 4.88281250e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.46484375e-03, 1.95312500e-03,\n",
       "        0.00000000e+00, 1.36718750e-02, 5.32714844e-01, 1.02539062e-02,\n",
       "        2.17285156e-02],\n",
       "       [4.84466553e-04, 4.06265259e-03, 3.83377075e-03, 3.40652466e-03,\n",
       "        4.97436523e-03, 1.77764893e-03, 7.62939453e-05, 6.52694702e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.84466553e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.16348267e-03, 4.57763672e-05,\n",
       "        0.00000000e+00, 6.69097900e-03, 1.00784302e-02, 5.12523651e-01,\n",
       "        9.38415527e-03],\n",
       "       [1.06430054e-03, 7.76290894e-03, 1.11351013e-02, 6.50978088e-03,\n",
       "        1.34487152e-02, 9.10949707e-03, 1.90734863e-05, 1.08318329e-02,\n",
       "        0.00000000e+00, 8.96453857e-05, 0.00000000e+00, 9.72747803e-04,\n",
       "        0.00000000e+00, 1.52587891e-05, 8.08715820e-04, 9.55581665e-04,\n",
       "        0.00000000e+00, 1.71165466e-02, 2.17056274e-02, 9.38034058e-03,\n",
       "        5.27763367e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"approx_fprime(\\n    np.array([1.0] * 21),\\n    jac,\\n    1.4901161193847656e-08,\\n    *(preprocessor.fit_transform(X), y, np.array([2.0] * 21)),\\n)\";\n",
       "                var nbb_formatted_code = \"approx_fprime(\\n    np.array([1.0] * 21),\\n    jac,\\n    1.4901161193847656e-08,\\n    *(preprocessor.fit_transform(X), y, np.array([2.0] * 21)),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "approx_fprime(\n",
    "    np.array([1.0] * 21),\n",
    "    jac,\n",
    "    1.4901161193847656e-08,\n",
    "    *(preprocessor.fit_transform(X), y, np.array([2.0] * 21)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c6074a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"r = 1\\nbeta = np.array([1.0] * 21)\\nm = np.array([0.0] * 21)\\nq = 1/np.array([2.0] * 21)\\n\\nweights = np.ones(y.shape)\\nweights[y == 0] = 1 / r\\n\\nlinear_pred = np.dot(preprocessor.fit_transform(X), beta)\\ny_pred = BayesianLogisticRegression().link._inv_link(linear_pred)\\n\\nhess = np.diag(q) + np.matmul(\\n    np.matmul(\\n        preprocessor.fit_transform(X).T.to_numpy(),\\n        np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n    ),\\n    preprocessor.fit_transform(X).to_numpy(),\\n)\";\n",
       "                var nbb_formatted_code = \"r = 1\\nbeta = np.array([1.0] * 21)\\nm = np.array([0.0] * 21)\\nq = 1 / np.array([2.0] * 21)\\n\\nweights = np.ones(y.shape)\\nweights[y == 0] = 1 / r\\n\\nlinear_pred = np.dot(preprocessor.fit_transform(X), beta)\\ny_pred = BayesianLogisticRegression().link._inv_link(linear_pred)\\n\\nhess = np.diag(q) + np.matmul(\\n    np.matmul(\\n        preprocessor.fit_transform(X).T.to_numpy(),\\n        np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n    ),\\n    preprocessor.fit_transform(X).to_numpy(),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = 1\n",
    "beta = np.array([1.0] * 21)\n",
    "m = np.array([0.0] * 21)\n",
    "q = 1 / np.array([2.0] * 21)\n",
    "\n",
    "weights = np.ones(y.shape)\n",
    "weights[y == 0] = 1 / r\n",
    "\n",
    "linear_pred = np.dot(preprocessor.fit_transform(X), beta)\n",
    "y_pred = BayesianLogisticRegression().link._inv_link(linear_pred)\n",
    "\n",
    "hess = np.diag(q) + np.matmul(\n",
    "    np.matmul(\n",
    "        preprocessor.fit_transform(X).T.to_numpy(),\n",
    "        np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\n",
    "    ),\n",
    "    preprocessor.fit_transform(X).to_numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abef22ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00531936e-01, 0.00000000e+00, 0.00000000e+00, 1.23386632e-04,\n",
       "        4.08549642e-04, 1.39942169e-09, 0.00000000e+00, 5.31934874e-04,\n",
       "        0.00000000e+00, 4.53958078e-05, 2.12966051e-08, 4.86518289e-04,\n",
       "        8.30318036e-10, 3.97397670e-11, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.03952402e-11, 7.62715736e-04, 4.86534540e-04,\n",
       "        1.06386289e-03],\n",
       "       [0.00000000e+00, 5.05719221e-01, 0.00000000e+00, 4.68690204e-04,\n",
       "        5.25053032e-03, 9.55623353e-04, 3.99680289e-15, 4.76359717e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.87960758e-12, 3.68594044e-14, 8.10555835e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.90866468e-03, 6.50974333e-03, 4.06259720e-03,\n",
       "        7.76258607e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 5.09934095e-01, 4.88596690e-03,\n",
       "        5.04812852e-03, 6.28940733e-03, 1.91256391e-05, 3.62556246e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.67011429e-05, 1.02875708e-10, 9.55616988e-04,\n",
       "        0.00000000e+00, 8.96177719e-03, 1.07949735e-02, 3.83242776e-03,\n",
       "        1.11363759e-02],\n",
       "       [1.23386632e-04, 4.68690204e-04, 4.88596690e-03, 5.05478044e-01,\n",
       "        0.00000000e+00, 1.94625626e-03, 1.01257133e-08, 3.53177736e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.06558358e-09, 1.23379735e-04,\n",
       "        7.88604514e-10, 3.96831457e-11, 1.67011436e-05, 9.55616988e-04,\n",
       "        0.00000000e+00, 4.38233898e-03, 9.17135383e-03, 3.40562015e-03,\n",
       "        6.51163716e-03],\n",
       "       [4.08549642e-04, 5.25053032e-03, 5.04812852e-03, 0.00000000e+00,\n",
       "        5.10707208e-01, 5.29877583e-03, 1.91155134e-05, 5.38931714e-03,\n",
       "        0.00000000e+00, 4.53958078e-05, 1.52310215e-08, 3.63138554e-04,\n",
       "        4.35931291e-11, 1.67011430e-05, 7.93854794e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.48810291e-03, 8.89607869e-03, 4.97593935e-03,\n",
       "        1.34511877e-02],\n",
       "       [1.39942169e-09, 9.55623353e-04, 6.28940733e-03, 1.94625626e-03,\n",
       "        5.29877583e-03, 5.07245032e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.04485811e-10, 5.88289195e-10,\n",
       "        3.29736238e-12, 9.52571355e-14, 2.22044605e-16, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.24503069e-03, 4.97701225e-03, 1.77953626e-03,\n",
       "        9.11045725e-03],\n",
       "       [0.00000000e+00, 3.99680289e-15, 1.91256391e-05, 1.01257133e-08,\n",
       "        1.91155134e-05, 0.00000000e+00, 5.00019126e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.22044605e-15, 2.22044605e-15, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.91256391e-05, 4.37221281e-05, 7.64615865e-05,\n",
       "        1.91153966e-05],\n",
       "       [5.31934874e-04, 4.76359717e-03, 3.62556246e-03, 3.53177736e-03,\n",
       "        5.38931714e-03, 0.00000000e+00, 0.00000000e+00, 5.08921094e-01,\n",
       "        0.00000000e+00, 4.53958078e-05, 2.04921193e-08, 4.86517700e-04,\n",
       "        8.28900281e-10, 1.67011826e-05, 8.10555938e-04, 9.55616988e-04,\n",
       "        0.00000000e+00, 6.60628556e-03, 1.30466981e-02, 6.52556165e-03,\n",
       "        1.08332522e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [4.53958078e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.53958078e-05, 0.00000000e+00, 0.00000000e+00, 4.53958078e-05,\n",
       "        0.00000000e+00, 5.00045396e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.81583232e-04, 0.00000000e+00,\n",
       "        9.07916155e-05],\n",
       "       [2.12966051e-08, 0.00000000e+00, 0.00000000e+00, 6.06558358e-09,\n",
       "        1.52310215e-08, 8.04485811e-10, 0.00000000e+00, 2.04921193e-08,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00000021e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.61753451e-07, 1.63157554e-08,\n",
       "        3.48082449e-08],\n",
       "       [4.86518289e-04, 0.00000000e+00, 0.00000000e+00, 1.23379735e-04,\n",
       "        3.63138554e-04, 5.88289195e-10, 0.00000000e+00, 4.86517700e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00486518e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.80856252e-04, 4.86518186e-04,\n",
       "        9.73035602e-04],\n",
       "       [8.30318036e-10, 1.87960758e-12, 0.00000000e+00, 7.88604514e-10,\n",
       "        4.35931291e-11, 3.29736238e-12, 0.00000000e+00, 8.28900281e-10,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000001e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.35883917e-08, 3.84439147e-11,\n",
       "        7.91832599e-10],\n",
       "       [3.97397670e-11, 3.68594044e-14, 1.67011429e-05, 3.96831457e-11,\n",
       "        1.67011430e-05, 9.52571355e-14, 2.22044605e-15, 1.67011826e-05,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.00016701e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00207582e-04, 1.06137321e-13,\n",
       "        1.67012203e-05],\n",
       "       [0.00000000e+00, 8.10555835e-04, 1.02875708e-10, 1.67011436e-05,\n",
       "        7.93854794e-04, 2.22044605e-16, 2.22044605e-15, 8.10555938e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.00810556e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.44265761e-03, 1.16249465e-03,\n",
       "        8.10555834e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 9.55616988e-04, 9.55616988e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.55616988e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.00955617e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.00202559e-03, 4.53958077e-05,\n",
       "        9.55616988e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.00000000e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [1.03952402e-11, 4.90866468e-03, 8.96177719e-03, 4.38233898e-03,\n",
       "        9.48810291e-03, 7.24503069e-03, 1.91256391e-05, 6.60628556e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.13870442e-01, 1.37598269e-02, 6.68713450e-03,\n",
       "        1.71160880e-02],\n",
       "       [7.62715736e-04, 6.50974333e-03, 1.07949735e-02, 9.17135383e-03,\n",
       "        8.89607869e-03, 4.97701225e-03, 4.37221281e-05, 1.30466981e-02,\n",
       "        0.00000000e+00, 1.81583232e-04, 2.61753451e-07, 5.80856252e-04,\n",
       "        1.35883917e-08, 1.00207582e-04, 1.44265761e-03, 2.00202559e-03,\n",
       "        0.00000000e+00, 1.37598269e-02, 5.32815243e-01, 1.00782545e-02,\n",
       "        2.17071454e-02],\n",
       "       [4.86534540e-04, 4.06259720e-03, 3.83242776e-03, 3.40562015e-03,\n",
       "        4.97593935e-03, 1.77953626e-03, 7.64615865e-05, 6.52556165e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.63157554e-08, 4.86518186e-04,\n",
       "        3.84439147e-11, 1.06137321e-13, 1.16249465e-03, 4.53958077e-05,\n",
       "        0.00000000e+00, 6.68713450e-03, 1.00782545e-02, 5.12521273e-01,\n",
       "        9.38123843e-03],\n",
       "       [1.06386289e-03, 7.76258607e-03, 1.11363759e-02, 6.51163716e-03,\n",
       "        1.34511877e-02, 9.11045725e-03, 1.91153966e-05, 1.08332522e-02,\n",
       "        0.00000000e+00, 9.07916155e-05, 3.48082449e-08, 9.73035602e-04,\n",
       "        7.91832599e-10, 1.67012203e-05, 8.10555834e-04, 9.55616988e-04,\n",
       "        0.00000000e+00, 1.71160880e-02, 2.17071454e-02, 9.38123843e-03,\n",
       "        5.27765661e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"hess\";\n",
       "                var nbb_formatted_code = \"hess\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e5ce16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01217139e-01, 1.00152907e-01, 2.69488563e+02, 4.15603116e+00,\n",
       "       3.49809805e-02])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 52;\n",
       "                var nbb_unformatted_code = \"1/np.exp(np.array([2.29048718, 2.30105719, -5.59652595, -1.42456057, 3.35295078]))\";\n",
       "                var nbb_formatted_code = \"1 / np.exp(np.array([2.29048718, 2.30105719, -5.59652595, -1.42456057, 3.35295078]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 / np.exp(np.array([2.29048718, 2.30105719, -5.59652595, -1.42456057, 3.35295078]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360bc83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cr_model_research",
   "language": "python",
   "name": "cr_model_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
