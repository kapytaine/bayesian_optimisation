{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3911da83",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Packages</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Link\" data-toc-modified-id=\"Link-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Link</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#Logit\" data-toc-modified-id=\"Logit-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Logit</a></span></li></ul></li><li><span><a href=\"#Bayesian-GLM\" data-toc-modified-id=\"Bayesian-GLM-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Bayesian GLM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#BayesianLogisticRegression\" data-toc-modified-id=\"BayesianLogisticRegression-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>BayesianLogisticRegression</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"#Miscellenaous\" data-toc-modified-id=\"Miscellenaous-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Miscellenaous</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read\" data-toc-modified-id=\"Read-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Read</a></span></li><li><span><a href=\"#Split-train-test\" data-toc-modified-id=\"Split-train-test-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Split train test</a></span></li></ul></li><li><span><a href=\"#Studies\" data-toc-modified-id=\"Studies-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Studies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-sigma2\" data-toc-modified-id=\"Single-sigma2-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Single sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-model\" data-toc-modified-id=\"Naive-model-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Naive model</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.1.2.1\"><span class=\"toc-item-num\">4.1.2.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.1.2.2\"><span class=\"toc-item-num\">4.1.2.2&nbsp;&nbsp;</span>Test</a></span></li></ul></li><li><span><a href=\"#Bayesian-optimisation\" data-toc-modified-id=\"Bayesian-optimisation-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Bayesian optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.1.3.1\"><span class=\"toc-item-num\">4.1.3.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-4.1.3.2\"><span class=\"toc-item-num\">4.1.3.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.1.3.3\"><span class=\"toc-item-num\">4.1.3.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li><li><span><a href=\"#Multiple-sigma2\" data-toc-modified-id=\"Multiple-sigma2-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Multiple sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocess-data\" data-toc-modified-id=\"Preprocess-data-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Preprocess data</a></span></li><li><span><a href=\"#Bayesian-optimization\" data-toc-modified-id=\"Bayesian-optimization-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Bayesian optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-4.2.2.1\"><span class=\"toc-item-num\">4.2.2.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-4.2.2.2\"><span class=\"toc-item-num\">4.2.2.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4.2.2.3\"><span class=\"toc-item-num\">4.2.2.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Bayesian-optimization\" data-toc-modified-id=\"Bayesian-optimization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Bayesian optimization</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95e967",
   "metadata": {},
   "source": [
    "___\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8298c767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nfrom __future__ import annotations\\n\\nfrom abc import ABCMeta, abstractmethod\\nimport itertools\\nimport logging\\nfrom typing import Any, Tuple, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy.special import expit, logit\\nfrom scipy import optimize\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, make_scorer\\nfrom sklearn.linear_model import (\\n    BayesianRidge,\\n    LinearRegression,\\n    LogisticRegression,\\n    Ridge,\\n)\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.base import BaseEstimator\\nfrom scipy.optimize import approx_fprime, minimize\\nfrom scipy.special import expit\\nfrom scipy.stats import multivariate_normal\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nfrom __future__ import annotations\\n\\nfrom abc import ABCMeta, abstractmethod\\nimport itertools\\nimport logging\\nfrom typing import Any, Tuple, Union\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy.special import expit, logit\\nfrom scipy import optimize\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, make_scorer\\nfrom sklearn.linear_model import (\\n    BayesianRidge,\\n    LinearRegression,\\n    LogisticRegression,\\n    Ridge,\\n)\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.base import BaseEstimator\\nfrom scipy.optimize import approx_fprime, minimize\\nfrom scipy.special import expit\\nfrom scipy.stats import multivariate_normal\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import itertools\n",
    "import logging\n",
    "from typing import Any, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit, logit\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, make_scorer\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import approx_fprime, minimize\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a63f4e",
   "metadata": {},
   "source": [
    "___\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda78f10",
   "metadata": {},
   "source": [
    "## Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c24f2",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b8ffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"class Link(metaclass=ABCMeta):\\n    @abstractmethod\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return self._inv_jac_link(self._inv_link(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return np.multiply(\\n            -self._hess_link(self._inv_link(x)),\\n            np.power(self._jac_inv_link(x), 3),\\n        )\\n\\n    @abstractmethod\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        This method has been chosen to avoid numerical error.\\n        When working with _jac_link, it may be close to 1/0 close to the\\n        optimum resulting in an zero division error.\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\";\n",
       "                var nbb_formatted_code = \"class Link(metaclass=ABCMeta):\\n    @abstractmethod\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return self._inv_jac_link(self._inv_link(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return np.multiply(\\n            -self._hess_link(self._inv_link(x)),\\n            np.power(self._jac_inv_link(x), 3),\\n        )\\n\\n    @abstractmethod\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        This method has been chosen to avoid numerical error.\\n        When working with _jac_link, it may be close to 1/0 close to the\\n        optimum resulting in an zero division error.\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Link(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def _inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self._inv_jac_link(self._inv_link(x))\n",
    "\n",
    "    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.multiply(\n",
    "            -self._hess_link(self._inv_link(x)),\n",
    "            np.power(self._jac_inv_link(x), 3),\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        This method has been chosen to avoid numerical error.\n",
    "        When working with _jac_link, it may be close to 1/0 close to the\n",
    "        optimum resulting in an zero division error.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _hess_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f111bf5",
   "metadata": {},
   "source": [
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811f567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"class Logit(Link):\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x)\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\\n\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        return logit(y)\\n\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        return y * (1 - y)\\n\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        return (2 * y - 1) / (y * (1 - y)) ** 2\";\n",
       "                var nbb_formatted_code = \"class Logit(Link):\\n    def _inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x)\\n\\n    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x))\\n\\n    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\\n        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\\n\\n    def _link(self, y: np.ndarray) -> np.ndarray:\\n        return logit(y)\\n\\n    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\\n        return y * (1 - y)\\n\\n    def _hess_link(self, y: np.ndarray) -> np.ndarray:\\n        return (2 * y - 1) / (y * (1 - y)) ** 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Logit(Link):\n",
    "    def _inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x)\n",
    "\n",
    "    def _jac_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x) * (1 - expit(x))\n",
    "\n",
    "    def _hess_inv_link(self, x: np.ndarray) -> np.ndarray:\n",
    "        return expit(x) * (1 - expit(x)) * (1 - 2 * expit(x))\n",
    "\n",
    "    def _link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return logit(y)\n",
    "\n",
    "    def _inv_jac_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return y * (1 - y)\n",
    "\n",
    "    def _hess_link(self, y: np.ndarray) -> np.ndarray:\n",
    "        return (2 * y - 1) / (y * (1 - y)) ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec15fa",
   "metadata": {},
   "source": [
    "## Bayesian GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0861d",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff1ffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\\n    def __init__(\\n        self,\\n        prior_parameters: dict = {},\\n        default_parameters: dict = {\\\"m\\\": 0, \\\"p\\\": 5},\\n        optimize_kwargs: dict = {},\\n        optimize_method: str = \\\"L-BFGS-B\\\",\\n    ) -> None:\\n        self.prior_parameters = prior_parameters\\n        self.default_parameters = default_parameters\\n        self.optimize_kwargs = optimize_kwargs\\n        self.optimize_method = optimize_method\\n\\n    def _check_distribution_params(self, params: dict) -> None:\\n        if not (\\n            isinstance(params, dict)\\n            and set(params.keys()) == set([\\\"m\\\", \\\"p\\\"])\\n            and (\\n                isinstance(params.get(\\\"m\\\"), Union[int, float])  # type: ignore\\n                and not np.isnan(params.get(\\\"m\\\"))  # type: ignore\\n            )\\n            and (\\n                isinstance(params.get(\\\"p\\\"), Union[int, float])  # type: ignore\\n                and params.get(\\\"p\\\") >= 0  # type: ignore\\n            )\\n        ):\\n            raise ValueError\\n\\n    @abstractmethod\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\\n        m = np.array([])\\n        p = np.array([])\\n        for col in feat_names:\\n            m = np.append(\\n                m,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"],\\n            )\\n            p = np.append(\\n                p,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"],\\n            )\\n        return m, p\\n\\n    def _update_m_p(\\n        self,\\n        res: optimize.OptimizeResult,\\n        X: pd.DataFrame,\\n        y: pd.Series,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> None:\\n        # get new updates values\\n        m_updated = res.x\\n        p_updated = self._diag_hess(\\n            res.x,\\n            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n        )\\n        feat_names = X.columns.to_list()\\n\\n        # check hessian positiveness\\n        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\\n            raise NotPositiveHessian\\n\\n        # update parameters\\n        self.posterior_parameters_ = dict(\\n            self.prior_parameters,\\n            **{\\n                col: {\\\"m\\\": m, \\\"p\\\": p}\\n                for col, m, p in zip(feat_names, m_updated, p_updated)\\n            },\\n        )\\n\\n        # check parameters\\n        for _, v in self.posterior_parameters_.items():\\n            self._check_distribution_params(v)\\n\\n    @abstractmethod\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\\n        logging.info(f\\\"Shape of the training dataset: {X.shape}.\\\")\\n\\n        # get parameter prior distribution parameters (mean and precision)\\n        m, p = self._get_prior_m_p(X.columns.to_list())\\n        logging.info(\\\"Prior distribution parameters obtained.\\\")\\n\\n        # optimize\\n        res = optimize.minimize(\\n            fun=self._loss,\\n            x0=m,\\n            jac=self._jac,\\n            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n            method=self.optimize_method,\\n            **self.optimize_kwargs,\\n        )\\n\\n        # log optimization result\\n        initial_loss = self._loss(\\n            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\\n        )\\n        log_message = (\\n            f\\\"initial fun: {initial_loss}\\\\n\\\"\\n            \\\"The OptimizeResult instance is:\\\\n\\\"\\n            f\\\"{res}\\\"\\n        )\\n\\n        if res.success:\\n            logging.info(f\\\"Log likelihood optimized successfully.\\\\n{log_message}\\\")\\n\\n            # calculate parameter posterior distribution parameters\\n            self._update_m_p(res, X, y, m, p, **kwargs)\\n            logging.info(\\\"Posterior distribution parameters computed.\\\")\\n        else:\\n            logging.warning(f\\\"Log likelihood optimization failed.\\\\n{log_message}\\\")\\n            if res.fun < initial_loss:\\n                # calculate parameter posterior distribution parameters\\n                self._update_m_p(res, X, y, m, p, **kwargs)\\n                logging.info(\\n                    \\\"Posterior distribution parameters computed even \\\"\\n                    \\\"though the optimization failed.\\\"\\n                )\\n            else:\\n                raise OptimizationError\\n\\n        return self\\n\\n    @abstractmethod\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def get_params(self, deep=True) -> dict:\\n        return {\\n            \\\"prior_parameters\\\": self.prior_parameters,\\n            \\\"default_parameters\\\": self.default_parameters,\\n            \\\"optimize_kwargs\\\": self.optimize_kwargs,\\n            \\\"optimize_method\\\": self.optimize_method,\\n        }\\n\\n    def set_params(self, **parameters) -> BayesianInterface:\\n        for parameter, value in parameters.items():\\n            setattr(self, parameter, value)\\n        return self\";\n",
       "                var nbb_formatted_code = \"class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\\n    def __init__(\\n        self,\\n        prior_parameters: dict = {},\\n        default_parameters: dict = {\\\"m\\\": 0, \\\"p\\\": 5},\\n        optimize_kwargs: dict = {},\\n        optimize_method: str = \\\"L-BFGS-B\\\",\\n    ) -> None:\\n        self.prior_parameters = prior_parameters\\n        self.default_parameters = default_parameters\\n        self.optimize_kwargs = optimize_kwargs\\n        self.optimize_method = optimize_method\\n\\n    def _check_distribution_params(self, params: dict) -> None:\\n        if not (\\n            isinstance(params, dict)\\n            and set(params.keys()) == set([\\\"m\\\", \\\"p\\\"])\\n            and (\\n                isinstance(params.get(\\\"m\\\"), Union[int, float])  # type: ignore\\n                and not np.isnan(params.get(\\\"m\\\"))  # type: ignore\\n            )\\n            and (\\n                isinstance(params.get(\\\"p\\\"), Union[int, float])  # type: ignore\\n                and params.get(\\\"p\\\") >= 0  # type: ignore\\n            )\\n        ):\\n            raise ValueError\\n\\n    @abstractmethod\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs: Any,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\\n        m = np.array([])\\n        p = np.array([])\\n        for col in feat_names:\\n            m = np.append(\\n                m,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"],\\n            )\\n            p = np.append(\\n                p,\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"],\\n            )\\n        return m, p\\n\\n    def _update_m_p(\\n        self,\\n        res: optimize.OptimizeResult,\\n        X: pd.DataFrame,\\n        y: pd.Series,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> None:\\n        # get new updates values\\n        m_updated = res.x\\n        p_updated = self._diag_hess(\\n            res.x,\\n            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n        )\\n        feat_names = X.columns.to_list()\\n\\n        # check hessian positiveness\\n        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\\n            raise NotPositiveHessian\\n\\n        # update parameters\\n        self.posterior_parameters_ = dict(\\n            self.prior_parameters,\\n            **{\\n                col: {\\\"m\\\": m, \\\"p\\\": p}\\n                for col, m, p in zip(feat_names, m_updated, p_updated)\\n            },\\n        )\\n\\n        # check parameters\\n        for _, v in self.posterior_parameters_.items():\\n            self._check_distribution_params(v)\\n\\n    @abstractmethod\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\\n        logging.info(f\\\"Shape of the training dataset: {X.shape}.\\\")\\n\\n        # get parameter prior distribution parameters (mean and precision)\\n        m, p = self._get_prior_m_p(X.columns.to_list())\\n        logging.info(\\\"Prior distribution parameters obtained.\\\")\\n\\n        # optimize\\n        res = optimize.minimize(\\n            fun=self._loss,\\n            x0=m,\\n            jac=self._jac,\\n            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\\n            method=self.optimize_method,\\n            **self.optimize_kwargs,\\n        )\\n\\n        # log optimization result\\n        initial_loss = self._loss(\\n            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\\n        )\\n        log_message = (\\n            f\\\"initial fun: {initial_loss}\\\\n\\\"\\n            \\\"The OptimizeResult instance is:\\\\n\\\"\\n            f\\\"{res}\\\"\\n        )\\n\\n        if res.success:\\n            logging.info(f\\\"Log likelihood optimized successfully.\\\\n{log_message}\\\")\\n\\n            # calculate parameter posterior distribution parameters\\n            self._update_m_p(res, X, y, m, p, **kwargs)\\n            logging.info(\\\"Posterior distribution parameters computed.\\\")\\n        else:\\n            logging.warning(f\\\"Log likelihood optimization failed.\\\\n{log_message}\\\")\\n            if res.fun < initial_loss:\\n                # calculate parameter posterior distribution parameters\\n                self._update_m_p(res, X, y, m, p, **kwargs)\\n                logging.info(\\n                    \\\"Posterior distribution parameters computed even \\\"\\n                    \\\"though the optimization failed.\\\"\\n                )\\n            else:\\n                raise OptimizationError\\n\\n        return self\\n\\n    @abstractmethod\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        \\\"\\\"\\\"Placeholder for train.\\n        Subclasses should implement this method!\\n        \\\"\\\"\\\"\\n\\n    def get_params(self, deep=True) -> dict:\\n        return {\\n            \\\"prior_parameters\\\": self.prior_parameters,\\n            \\\"default_parameters\\\": self.default_parameters,\\n            \\\"optimize_kwargs\\\": self.optimize_kwargs,\\n            \\\"optimize_method\\\": self.optimize_method,\\n        }\\n\\n    def set_params(self, **parameters) -> BayesianInterface:\\n        for parameter, value in parameters.items():\\n            setattr(self, parameter, value)\\n        return self\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BayesianInterface(BaseEstimator, metaclass=ABCMeta):\n",
    "    def __init__(\n",
    "        self,\n",
    "        prior_parameters: dict = {},\n",
    "        default_parameters: dict = {\"m\": 0, \"p\": 5},\n",
    "        optimize_kwargs: dict = {},\n",
    "        optimize_method: str = \"L-BFGS-B\",\n",
    "    ) -> None:\n",
    "        self.prior_parameters = prior_parameters\n",
    "        self.default_parameters = default_parameters\n",
    "        self.optimize_kwargs = optimize_kwargs\n",
    "        self.optimize_method = optimize_method\n",
    "\n",
    "    def _check_distribution_params(self, params: dict) -> None:\n",
    "        if not (\n",
    "            isinstance(params, dict)\n",
    "            and set(params.keys()) == set([\"m\", \"p\"])\n",
    "            and (\n",
    "                isinstance(params.get(\"m\"), Union[int, float])  # type: ignore\n",
    "                and not np.isnan(params.get(\"m\"))  # type: ignore\n",
    "            )\n",
    "            and (\n",
    "                isinstance(params.get(\"p\"), Union[int, float])  # type: ignore\n",
    "                and params.get(\"p\") >= 0  # type: ignore\n",
    "            )\n",
    "        ):\n",
    "            raise ValueError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _loss(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> float:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _jac(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _diag_hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs: Any,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def _get_prior_m_p(self, feat_names: list[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        m = np.array([])\n",
    "        p = np.array([])\n",
    "        for col in feat_names:\n",
    "            m = np.append(\n",
    "                m,\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"m\"],\n",
    "            )\n",
    "            p = np.append(\n",
    "                p,\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"p\"],\n",
    "            )\n",
    "        return m, p\n",
    "\n",
    "    def _update_m_p(\n",
    "        self,\n",
    "        res: optimize.OptimizeResult,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        # get new updates values\n",
    "        m_updated = res.x\n",
    "        p_updated = self._diag_hess(\n",
    "            res.x,\n",
    "            *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\n",
    "        )\n",
    "        feat_names = X.columns.to_list()\n",
    "\n",
    "        # check hessian positiveness\n",
    "        if not (np.all(np.isfinite(p_updated)) and np.all(p_updated >= 0)):\n",
    "            raise NotPositiveHessian\n",
    "\n",
    "        # update parameters\n",
    "        self.posterior_parameters_ = dict(\n",
    "            self.prior_parameters,\n",
    "            **{\n",
    "                col: {\"m\": m, \"p\": p}\n",
    "                for col, m, p in zip(feat_names, m_updated, p_updated)\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # check parameters\n",
    "        for _, v in self.posterior_parameters_.items():\n",
    "            self._check_distribution_params(v)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_args(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> BayesianInterface:\n",
    "        logging.info(f\"Shape of the training dataset: {X.shape}.\")\n",
    "\n",
    "        # get parameter prior distribution parameters (mean and precision)\n",
    "        m, p = self._get_prior_m_p(X.columns.to_list())\n",
    "        logging.info(\"Prior distribution parameters obtained.\")\n",
    "\n",
    "        # optimize\n",
    "        res = optimize.minimize(\n",
    "            fun=self._loss,\n",
    "            x0=m,\n",
    "            jac=self._jac,\n",
    "            args=self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs),\n",
    "            method=self.optimize_method,\n",
    "            **self.optimize_kwargs,\n",
    "        )\n",
    "\n",
    "        # log optimization result\n",
    "        initial_loss = self._loss(\n",
    "            m, *self._get_args(X.to_numpy(), y.to_numpy(), m, p, **kwargs)\n",
    "        )\n",
    "        log_message = (\n",
    "            f\"initial fun: {initial_loss}\\n\"\n",
    "            \"The OptimizeResult instance is:\\n\"\n",
    "            f\"{res}\"\n",
    "        )\n",
    "\n",
    "        if res.success:\n",
    "            logging.info(f\"Log likelihood optimized successfully.\\n{log_message}\")\n",
    "\n",
    "            # calculate parameter posterior distribution parameters\n",
    "            self._update_m_p(res, X, y, m, p, **kwargs)\n",
    "            logging.info(\"Posterior distribution parameters computed.\")\n",
    "        else:\n",
    "            logging.warning(f\"Log likelihood optimization failed.\\n{log_message}\")\n",
    "            if res.fun < initial_loss:\n",
    "                # calculate parameter posterior distribution parameters\n",
    "                self._update_m_p(res, X, y, m, p, **kwargs)\n",
    "                logging.info(\n",
    "                    \"Posterior distribution parameters computed even \"\n",
    "                    \"though the optimization failed.\"\n",
    "                )\n",
    "            else:\n",
    "                raise OptimizationError\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        \"\"\"Placeholder for train.\n",
    "        Subclasses should implement this method!\n",
    "        \"\"\"\n",
    "\n",
    "    def get_params(self, deep=True) -> dict:\n",
    "        return {\n",
    "            \"prior_parameters\": self.prior_parameters,\n",
    "            \"default_parameters\": self.default_parameters,\n",
    "            \"optimize_kwargs\": self.optimize_kwargs,\n",
    "            \"optimize_method\": self.optimize_method,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters) -> BayesianInterface:\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef38757",
   "metadata": {},
   "source": [
    "### BayesianLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69813b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"class BayesianLogisticRegression(BayesianInterface):\\n    def __init__(self, link: Link = Logit(), **kwargs):\\n        self.link = link\\n        super().__init__(**kwargs)\\n\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> float:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :param r: regularization parameter\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        loss = (\\n            log_loss(y, y_pred, sample_weight=weights, normalize=False)\\n            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\\n            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\\n        )\\n\\n        return loss\\n\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        jac = np.add(\\n            -np.dot(weights * (y - y_pred), X),\\n            np.multiply(p, np.subtract(beta, m)),\\n        )\\n\\n        return jac\\n\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        diag_hess = p + np.dot(\\n            np.power(X, 2).T,\\n            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\\n        )\\n        return diag_hess\\n\\n    def _hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        hess = np.diag(p) + np.matmul(\\n            np.matmul(\\n                X.T,\\n                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n            ),\\n            X,\\n        )\\n        return hess\\n\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        m_posterior = np.array(\\n            [\\n                self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        m_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        p_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\\n\\n    def get_params(self, deep=True):\\n        shared_params = super().get_params()\\n        return dict(shared_params, **{\\\"link\\\": self.link})\\n\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        return (X, y, m, p)\\n\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        m = [\\n            self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n            for col in X.columns\\n        ]\\n        linear_pred = np.dot(X.to_numpy(), np.array(m))\\n        return self.link._inv_link(linear_pred)\";\n",
       "                var nbb_formatted_code = \"class BayesianLogisticRegression(BayesianInterface):\\n    def __init__(self, link: Link = Logit(), **kwargs):\\n        self.link = link\\n        super().__init__(**kwargs)\\n\\n    def _loss(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> float:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :param r: regularization parameter\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        loss = (\\n            log_loss(y, y_pred, sample_weight=weights, normalize=False)\\n            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\\n            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\\n        )\\n\\n        return loss\\n\\n    def _jac(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        \\\"\\\"\\\"\\n        LogLikelihood defines the regularized loss function\\n        :param omega: vector to optimize\\n        :param y: responses (1/-1) of training data\\n        :param X: dimensions of training data\\n        :param m: previous vector of means\\n        :param p: previous vector of inverse variances\\n        :return out: value of loss function\\n        :return grad: gradient of loss function\\n        \\\"\\\"\\\"\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        jac = np.add(\\n            -np.dot(weights * (y - y_pred), X),\\n            np.multiply(p, np.subtract(beta, m)),\\n        )\\n\\n        return jac\\n\\n    def _diag_hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        diag_hess = p + np.dot(\\n            np.power(X, 2).T,\\n            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\\n        )\\n        return diag_hess\\n\\n    def _hess(\\n        self,\\n        beta: np.ndarray,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n    ) -> np.ndarray:\\n        linear_pred = np.dot(X, beta)\\n        y_pred = self.link._inv_link(linear_pred)\\n\\n        weights = np.ones(y.shape)\\n\\n        hess = np.diag(p) + np.matmul(\\n            np.matmul(\\n                X.T,\\n                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\\n            ),\\n            X,\\n        )\\n        return hess\\n\\n    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\\n        m_posterior = np.array(\\n            [\\n                self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        m_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"m\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        p_prior = np.array(\\n            [\\n                self.prior_parameters.get(col, self.default_parameters)[\\\"p\\\"]\\n                for col in X.columns\\n            ]\\n        )\\n        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\\n\\n    def get_params(self, deep=True):\\n        shared_params = super().get_params()\\n        return dict(shared_params, **{\\\"link\\\": self.link})\\n\\n    def _get_args(\\n        self,\\n        X: np.ndarray,\\n        y: np.ndarray,\\n        m: np.ndarray,\\n        p: np.ndarray,\\n        **kwargs,\\n    ) -> tuple:\\n        return (X, y, m, p)\\n\\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\\n        m = [\\n            self.posterior_parameters_.get(col, self.default_parameters)[\\\"m\\\"]\\n            for col in X.columns\\n        ]\\n        linear_pred = np.dot(X.to_numpy(), np.array(m))\\n        return self.link._inv_link(linear_pred)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BayesianLogisticRegression(BayesianInterface):\n",
    "    def __init__(self, link: Link = Logit(), **kwargs):\n",
    "        self.link = link\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _loss(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        LogLikelihood defines the regularized loss function\n",
    "        :param omega: vector to optimize\n",
    "        :param y: responses (1/-1) of training data\n",
    "        :param X: dimensions of training data\n",
    "        :param m: previous vector of means\n",
    "        :param p: previous vector of inverse variances\n",
    "        :param r: regularization parameter\n",
    "        :return out: value of loss function\n",
    "        :return grad: gradient of loss function\n",
    "        \"\"\"\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        loss = (\n",
    "            log_loss(y, y_pred, sample_weight=weights, normalize=False)\n",
    "            + 0.5 * np.dot(np.multiply(p, np.subtract(beta, m)), np.subtract(beta, m))\n",
    "            + 0.5 * (np.sum(np.log(1 / p)) + len(p) * np.log(2 * np.pi))\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _jac(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        LogLikelihood defines the regularized loss function\n",
    "        :param omega: vector to optimize\n",
    "        :param y: responses (1/-1) of training data\n",
    "        :param X: dimensions of training data\n",
    "        :param m: previous vector of means\n",
    "        :param p: previous vector of inverse variances\n",
    "        :return out: value of loss function\n",
    "        :return grad: gradient of loss function\n",
    "        \"\"\"\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        jac = np.add(\n",
    "            -np.dot(weights * (y - y_pred), X),\n",
    "            np.multiply(p, np.subtract(beta, m)),\n",
    "        )\n",
    "\n",
    "        return jac\n",
    "\n",
    "    def _diag_hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> np.ndarray:\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        diag_hess = p + np.dot(\n",
    "            np.power(X, 2).T,\n",
    "            np.multiply(weights, np.multiply(y_pred, 1 - y_pred)),\n",
    "        )\n",
    "        return diag_hess\n",
    "\n",
    "    def _hess(\n",
    "        self,\n",
    "        beta: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        linear_pred = np.dot(X, beta)\n",
    "        y_pred = self.link._inv_link(linear_pred)\n",
    "\n",
    "        weights = np.ones(y.shape)\n",
    "\n",
    "        hess = np.diag(p) + np.matmul(\n",
    "            np.matmul(\n",
    "                X.T,\n",
    "                np.diag(np.multiply(weights, np.multiply(y_pred, 1 - y_pred))),\n",
    "            ),\n",
    "            X,\n",
    "        )\n",
    "        return hess\n",
    "\n",
    "    def score(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        m_posterior = np.array(\n",
    "            [\n",
    "                self.posterior_parameters_.get(col, self.default_parameters)[\"m\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        m_prior = np.array(\n",
    "            [\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"m\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        p_prior = np.array(\n",
    "            [\n",
    "                self.prior_parameters.get(col, self.default_parameters)[\"p\"]\n",
    "                for col in X.columns\n",
    "            ]\n",
    "        )\n",
    "        return -self._loss(m_posterior, X.to_numpy(), y.to_numpy(), m_prior, p_prior)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        shared_params = super().get_params()\n",
    "        return dict(shared_params, **{\"link\": self.link})\n",
    "\n",
    "    def _get_args(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        m: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        **kwargs,\n",
    "    ) -> tuple:\n",
    "        return (X, y, m, p)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        m = [\n",
    "            self.posterior_parameters_.get(col, self.default_parameters)[\"m\"]\n",
    "            for col in X.columns\n",
    "        ]\n",
    "        linear_pred = np.dot(X.to_numpy(), np.array(m))\n",
    "        return self.link._inv_link(linear_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2f408",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf96e6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"beta = np.random.normal(size=4)\\nX = np.random.normal(size=(10, 4))\\ny = np.random.choice([0, 1], size=10)\\nm = np.random.normal(size=4)\\nq = np.exp(np.random.normal(size=4))\";\n",
       "                var nbb_formatted_code = \"beta = np.random.normal(size=4)\\nX = np.random.normal(size=(10, 4))\\ny = np.random.choice([0, 1], size=10)\\nm = np.random.normal(size=4)\\nq = np.exp(np.random.normal(size=4))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = np.random.normal(size=4)\n",
    "X = np.random.normal(size=(10, 4))\n",
    "y = np.random.choice([0, 1], size=10)\n",
    "m = np.random.normal(size=4)\n",
    "q = np.exp(np.random.normal(size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65efbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._loss,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._jac(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_formatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._loss,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._jac(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.testing.assert_allclose(\n",
    "    approx_fprime(\n",
    "        beta,\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        1.4901161193847656e-08,\n",
    "        *(X, y, m, q),\n",
    "    ),\n",
    "    BayesianLogisticRegression()._jac(beta, X, y, m, q),\n",
    "    atol=1e-6,\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0faa7a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._jac,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._hess(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_formatted_code = \"np.testing.assert_allclose(\\n    approx_fprime(\\n        beta,\\n        BayesianLogisticRegression()._jac,\\n        1.4901161193847656e-08,\\n        *(X, y, m, q),\\n    ),\\n    BayesianLogisticRegression()._hess(beta, X, y, m, q),\\n    atol=1e-6,\\n    rtol=1e-6,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.testing.assert_allclose(\n",
    "    approx_fprime(\n",
    "        beta,\n",
    "        BayesianLogisticRegression()._jac,\n",
    "        1.4901161193847656e-08,\n",
    "        *(X, y, m, q),\n",
    "    ),\n",
    "    BayesianLogisticRegression()._hess(beta, X, y, m, q),\n",
    "    atol=1e-6,\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196c5e4",
   "metadata": {},
   "source": [
    "## Miscellenaous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63db1a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"def get_col_to_id(cols, cats):\\n    out = {}\\n    for col in cols:\\n        for ii, cat in enumerate(cats):\\n            if isinstance(cat, str):\\n                if cat in col:\\n                    out[col] = ii\\n            else:\\n                for cc in cat:\\n                    if cc in col:\\n                        out[col] = ii\\n    return out\";\n",
       "                var nbb_formatted_code = \"def get_col_to_id(cols, cats):\\n    out = {}\\n    for col in cols:\\n        for ii, cat in enumerate(cats):\\n            if isinstance(cat, str):\\n                if cat in col:\\n                    out[col] = ii\\n            else:\\n                for cc in cat:\\n                    if cc in col:\\n                        out[col] = ii\\n    return out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_col_to_id(cols, cats):\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        for ii, cat in enumerate(cats):\n",
    "            if isinstance(cat, str):\n",
    "                if cat in col:\n",
    "                    out[col] = ii\n",
    "            else:\n",
    "                for cc in cat:\n",
    "                    if cc in col:\n",
    "                        out[col] = ii\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f72ff",
   "metadata": {},
   "source": [
    "___\n",
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf333b60",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab81624b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 43;\n",
       "                var nbb_unformatted_code = \"n_samples = 200\\ncats = [\\\"a\\\", \\\"b\\\", \\\"c\\\", \\\"d\\\", \\\"e\\\", \\\"f\\\"]\\nX = pd.DataFrame(\\n    data={\\n        \\\"col1\\\": np.random.choice(cats, size=n_samples),\\n        \\\"col2\\\": np.random.choice(cats, size=n_samples),\\n        \\\"col3\\\": np.random.choice(cats, size=n_samples),\\n    }\\n)\\nX_preprocessed = pd.get_dummies(X)\\n\\ntheta = np.random.multivariate_normal(\\n    np.zeros(len(cats) * X.shape[1]),\\n    np.diag(np.array([1e-1] * len(cats) + [1] * len(cats) + [1e1] * len(cats))),\\n)\\n\\ny = pd.Series(\\n    data=np.random.binomial(1, expit(np.dot(X_preprocessed.to_numpy(), theta))),\\n    index=X_preprocessed.index,\\n)\";\n",
       "                var nbb_formatted_code = \"n_samples = 200\\ncats = [\\\"a\\\", \\\"b\\\", \\\"c\\\", \\\"d\\\", \\\"e\\\", \\\"f\\\"]\\nX = pd.DataFrame(\\n    data={\\n        \\\"col1\\\": np.random.choice(cats, size=n_samples),\\n        \\\"col2\\\": np.random.choice(cats, size=n_samples),\\n        \\\"col3\\\": np.random.choice(cats, size=n_samples),\\n    }\\n)\\nX_preprocessed = pd.get_dummies(X)\\n\\ntheta = np.random.multivariate_normal(\\n    np.zeros(len(cats) * X.shape[1]),\\n    np.diag(np.array([1e-1] * len(cats) + [1] * len(cats) + [1e1] * len(cats))),\\n)\\n\\ny = pd.Series(\\n    data=np.random.binomial(1, expit(np.dot(X_preprocessed.to_numpy(), theta))),\\n    index=X_preprocessed.index,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 200\n",
    "cats = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "X = pd.DataFrame(\n",
    "    data={\n",
    "        \"col1\": np.random.choice(cats, size=n_samples),\n",
    "        \"col2\": np.random.choice(cats, size=n_samples),\n",
    "        \"col3\": np.random.choice(cats, size=n_samples),\n",
    "    }\n",
    ")\n",
    "X_preprocessed = pd.get_dummies(X)\n",
    "\n",
    "theta = np.random.multivariate_normal(\n",
    "    np.zeros(len(cats) * X.shape[1]),\n",
    "    np.diag(np.array([1e-1] * len(cats) + [1] * len(cats) + [1e1] * len(cats))),\n",
    ")\n",
    "\n",
    "y = pd.Series(\n",
    "    data=np.random.binomial(1, expit(np.dot(X_preprocessed.to_numpy(), theta))),\n",
    "    index=X_preprocessed.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ded10b",
   "metadata": {},
   "source": [
    "## Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b91bb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"X_train, X_test, y_train, y_test = train_test_split(\\n    X_preprocessed, y, test_size=0.2, stratify=y\\n)\";\n",
       "                var nbb_formatted_code = \"X_train, X_test, y_train, y_test = train_test_split(\\n    X_preprocessed, y, test_size=0.2, stratify=y\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_preprocessed, y, test_size=0.2, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfce23",
   "metadata": {},
   "source": [
    "___\n",
    "# Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e400bbf",
   "metadata": {},
   "source": [
    "## Single sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422bdfc",
   "metadata": {},
   "source": [
    "### Naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4580a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5623351446188083\n",
      "0.5\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"y_pred = np.array([np.mean(y_train)] * len(y_test))\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = np.array([np.mean(y_train)] * len(y_test))\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = np.array([np.mean(y_train)] * len(y_test))\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab81dd",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616d7ea",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0b57a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_default_parameters</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031313</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>{'m': 0, 'p': 0.001}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.001}}</td>\n",
       "      <td>-0.311680</td>\n",
       "      <td>-0.432727</td>\n",
       "      <td>-0.624367</td>\n",
       "      <td>-0.275618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377357</td>\n",
       "      <td>0.139255</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.189113</td>\n",
       "      <td>-0.190769</td>\n",
       "      <td>-0.169377</td>\n",
       "      <td>-0.200170</td>\n",
       "      <td>-0.203862</td>\n",
       "      <td>-0.190658</td>\n",
       "      <td>0.012001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027904</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>{'m': 0, 'p': 0.0017782794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0017782...</td>\n",
       "      <td>-0.312519</td>\n",
       "      <td>-0.416230</td>\n",
       "      <td>-0.594905</td>\n",
       "      <td>-0.275720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368316</td>\n",
       "      <td>0.127459</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.189337</td>\n",
       "      <td>-0.191047</td>\n",
       "      <td>-0.169708</td>\n",
       "      <td>-0.200395</td>\n",
       "      <td>-0.204093</td>\n",
       "      <td>-0.190916</td>\n",
       "      <td>0.011964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026473</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>{'m': 0, 'p': 0.0031622776601683794}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0031622...</td>\n",
       "      <td>-0.313839</td>\n",
       "      <td>-0.400069</td>\n",
       "      <td>-0.565815</td>\n",
       "      <td>-0.275980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359529</td>\n",
       "      <td>0.115838</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.189701</td>\n",
       "      <td>-0.191501</td>\n",
       "      <td>-0.170236</td>\n",
       "      <td>-0.200761</td>\n",
       "      <td>-0.204472</td>\n",
       "      <td>-0.191334</td>\n",
       "      <td>0.011910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024158</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>{'m': 0, 'p': 0.005623413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0056234...</td>\n",
       "      <td>-0.315778</td>\n",
       "      <td>-0.383827</td>\n",
       "      <td>-0.537204</td>\n",
       "      <td>-0.276387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350936</td>\n",
       "      <td>0.104452</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.190292</td>\n",
       "      <td>-0.192236</td>\n",
       "      <td>-0.171084</td>\n",
       "      <td>-0.201352</td>\n",
       "      <td>-0.205086</td>\n",
       "      <td>-0.192010</td>\n",
       "      <td>0.011825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>{'m': 0, 'p': 0.01}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.01}}</td>\n",
       "      <td>-0.318500</td>\n",
       "      <td>-0.367465</td>\n",
       "      <td>-0.508722</td>\n",
       "      <td>-0.277033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342501</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.191247</td>\n",
       "      <td>-0.193426</td>\n",
       "      <td>-0.172432</td>\n",
       "      <td>-0.202309</td>\n",
       "      <td>-0.206084</td>\n",
       "      <td>-0.193100</td>\n",
       "      <td>0.011697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>{'m': 0, 'p': 0.01778279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0177827...</td>\n",
       "      <td>-0.322193</td>\n",
       "      <td>-0.351113</td>\n",
       "      <td>-0.480661</td>\n",
       "      <td>-0.278165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334384</td>\n",
       "      <td>0.082382</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.192780</td>\n",
       "      <td>-0.195322</td>\n",
       "      <td>-0.174564</td>\n",
       "      <td>-0.203833</td>\n",
       "      <td>-0.207694</td>\n",
       "      <td>-0.194839</td>\n",
       "      <td>0.011504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>{'m': 0, 'p': 0.03162277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0316227...</td>\n",
       "      <td>-0.326720</td>\n",
       "      <td>-0.334859</td>\n",
       "      <td>-0.453479</td>\n",
       "      <td>-0.280108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326743</td>\n",
       "      <td>0.072223</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.195209</td>\n",
       "      <td>-0.198308</td>\n",
       "      <td>-0.177902</td>\n",
       "      <td>-0.206232</td>\n",
       "      <td>-0.210266</td>\n",
       "      <td>-0.197583</td>\n",
       "      <td>0.011217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.014102</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>{'m': 0, 'p': 0.056234132519034905}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0562341...</td>\n",
       "      <td>-0.331744</td>\n",
       "      <td>-0.319195</td>\n",
       "      <td>-0.428453</td>\n",
       "      <td>-0.283286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320023</td>\n",
       "      <td>0.063349</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.198977</td>\n",
       "      <td>-0.202914</td>\n",
       "      <td>-0.183007</td>\n",
       "      <td>-0.209928</td>\n",
       "      <td>-0.214304</td>\n",
       "      <td>-0.201826</td>\n",
       "      <td>0.010815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016554</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>{'m': 0, 'p': 0.09999999999999999}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0999999...</td>\n",
       "      <td>-0.336485</td>\n",
       "      <td>-0.304739</td>\n",
       "      <td>-0.407270</td>\n",
       "      <td>-0.288190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314785</td>\n",
       "      <td>0.056295</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.204686</td>\n",
       "      <td>-0.209814</td>\n",
       "      <td>-0.190532</td>\n",
       "      <td>-0.215465</td>\n",
       "      <td>-0.220488</td>\n",
       "      <td>-0.208197</td>\n",
       "      <td>0.010304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>{'m': 0, 'p': 0.1778279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.1778279...</td>\n",
       "      <td>-0.340192</td>\n",
       "      <td>-0.292514</td>\n",
       "      <td>-0.392050</td>\n",
       "      <td>-0.295155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311855</td>\n",
       "      <td>0.051269</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.213149</td>\n",
       "      <td>-0.219822</td>\n",
       "      <td>-0.201120</td>\n",
       "      <td>-0.223533</td>\n",
       "      <td>-0.229618</td>\n",
       "      <td>-0.217448</td>\n",
       "      <td>0.009756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>{'m': 0, 'p': 0.31622776601683794}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.3162277...</td>\n",
       "      <td>-0.343172</td>\n",
       "      <td>-0.283906</td>\n",
       "      <td>-0.384348</td>\n",
       "      <td>-0.304565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312331</td>\n",
       "      <td>0.047855</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.225453</td>\n",
       "      <td>-0.233898</td>\n",
       "      <td>-0.215366</td>\n",
       "      <td>-0.235032</td>\n",
       "      <td>-0.242562</td>\n",
       "      <td>-0.230462</td>\n",
       "      <td>0.009296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>{'m': 0, 'p': 0.5623413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.5623413...</td>\n",
       "      <td>-0.347524</td>\n",
       "      <td>-0.280987</td>\n",
       "      <td>-0.384889</td>\n",
       "      <td>-0.317184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317767</td>\n",
       "      <td>0.045357</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.242944</td>\n",
       "      <td>-0.253189</td>\n",
       "      <td>-0.233969</td>\n",
       "      <td>-0.251172</td>\n",
       "      <td>-0.260270</td>\n",
       "      <td>-0.248309</td>\n",
       "      <td>0.009050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>{'m': 0, 'p': 1.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1.0}}</td>\n",
       "      <td>-0.356604</td>\n",
       "      <td>-0.286597</td>\n",
       "      <td>-0.393833</td>\n",
       "      <td>-0.334331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330119</td>\n",
       "      <td>0.043041</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.267094</td>\n",
       "      <td>-0.279034</td>\n",
       "      <td>-0.258058</td>\n",
       "      <td>-0.273492</td>\n",
       "      <td>-0.283907</td>\n",
       "      <td>-0.272317</td>\n",
       "      <td>0.009071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008785</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>{'m': 0, 'p': 1.7782794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1.7782794...</td>\n",
       "      <td>-0.373814</td>\n",
       "      <td>-0.303805</td>\n",
       "      <td>-0.411297</td>\n",
       "      <td>-0.357686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351396</td>\n",
       "      <td>0.040194</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.299296</td>\n",
       "      <td>-0.312775</td>\n",
       "      <td>-0.289279</td>\n",
       "      <td>-0.303701</td>\n",
       "      <td>-0.314827</td>\n",
       "      <td>-0.303976</td>\n",
       "      <td>0.009308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>{'m': 0, 'p': 3.162277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 3.1622776...</td>\n",
       "      <td>-0.401304</td>\n",
       "      <td>-0.334826</td>\n",
       "      <td>-0.437415</td>\n",
       "      <td>-0.388691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.382953</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.340451</td>\n",
       "      <td>-0.355258</td>\n",
       "      <td>-0.329256</td>\n",
       "      <td>-0.343193</td>\n",
       "      <td>-0.354085</td>\n",
       "      <td>-0.344449</td>\n",
       "      <td>0.009572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.006890</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>{'m': 0, 'p': 5.62341325190349}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 5.6234132...</td>\n",
       "      <td>-0.438960</td>\n",
       "      <td>-0.379632</td>\n",
       "      <td>-0.471534</td>\n",
       "      <td>-0.427760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.390191</td>\n",
       "      <td>-0.405968</td>\n",
       "      <td>-0.378408</td>\n",
       "      <td>-0.392096</td>\n",
       "      <td>-0.401450</td>\n",
       "      <td>-0.393623</td>\n",
       "      <td>0.009584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>{'m': 0, 'p': 10.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 10.0}}</td>\n",
       "      <td>-0.483969</td>\n",
       "      <td>-0.434967</td>\n",
       "      <td>-0.511277</td>\n",
       "      <td>-0.473538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473051</td>\n",
       "      <td>0.025157</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.446060</td>\n",
       "      <td>-0.462086</td>\n",
       "      <td>-0.434727</td>\n",
       "      <td>-0.448107</td>\n",
       "      <td>-0.454600</td>\n",
       "      <td>-0.449116</td>\n",
       "      <td>0.009118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>{'m': 0, 'p': 17.78279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 17.782794...</td>\n",
       "      <td>-0.531612</td>\n",
       "      <td>-0.494612</td>\n",
       "      <td>-0.552766</td>\n",
       "      <td>-0.522577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523968</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.503552</td>\n",
       "      <td>-0.518569</td>\n",
       "      <td>-0.493673</td>\n",
       "      <td>-0.506201</td>\n",
       "      <td>-0.509344</td>\n",
       "      <td>-0.506268</td>\n",
       "      <td>0.008085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.006147</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>{'m': 0, 'p': 31.622776601683796}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 31.622776...</td>\n",
       "      <td>-0.576682</td>\n",
       "      <td>-0.551140</td>\n",
       "      <td>-0.591772</td>\n",
       "      <td>-0.569786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571728</td>\n",
       "      <td>0.013115</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.557156</td>\n",
       "      <td>-0.569756</td>\n",
       "      <td>-0.549364</td>\n",
       "      <td>-0.560084</td>\n",
       "      <td>-0.560646</td>\n",
       "      <td>-0.559401</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>{'m': 0, 'p': 56.23413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 56.234132...</td>\n",
       "      <td>-0.614727</td>\n",
       "      <td>-0.598364</td>\n",
       "      <td>-0.624705</td>\n",
       "      <td>-0.610040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611712</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.601958</td>\n",
       "      <td>-0.611370</td>\n",
       "      <td>-0.596374</td>\n",
       "      <td>-0.604565</td>\n",
       "      <td>-0.603871</td>\n",
       "      <td>-0.603628</td>\n",
       "      <td>0.004822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.005628</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>{'m': 0, 'p': 100.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 100.0}}</td>\n",
       "      <td>-0.643393</td>\n",
       "      <td>-0.633448</td>\n",
       "      <td>-0.649609</td>\n",
       "      <td>-0.640455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641630</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.635466</td>\n",
       "      <td>-0.641835</td>\n",
       "      <td>-0.631778</td>\n",
       "      <td>-0.637421</td>\n",
       "      <td>-0.636462</td>\n",
       "      <td>-0.636592</td>\n",
       "      <td>0.003245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>{'m': 0, 'p': 177.82794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 177.82794...</td>\n",
       "      <td>-0.662965</td>\n",
       "      <td>-0.657117</td>\n",
       "      <td>-0.666683</td>\n",
       "      <td>-0.661210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.661955</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.658226</td>\n",
       "      <td>-0.662240</td>\n",
       "      <td>-0.655934</td>\n",
       "      <td>-0.659532</td>\n",
       "      <td>-0.658739</td>\n",
       "      <td>-0.658934</td>\n",
       "      <td>0.002041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>{'m': 0, 'p': 316.2277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 316.22776...</td>\n",
       "      <td>-0.675383</td>\n",
       "      <td>-0.672011</td>\n",
       "      <td>-0.677551</td>\n",
       "      <td>-0.674364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674810</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.672620</td>\n",
       "      <td>-0.675034</td>\n",
       "      <td>-0.671252</td>\n",
       "      <td>-0.673431</td>\n",
       "      <td>-0.672888</td>\n",
       "      <td>-0.673045</td>\n",
       "      <td>0.001227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>{'m': 0, 'p': 562.341325190349}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 562.34132...</td>\n",
       "      <td>-0.682888</td>\n",
       "      <td>-0.680965</td>\n",
       "      <td>-0.684132</td>\n",
       "      <td>-0.682304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.682565</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.681301</td>\n",
       "      <td>-0.682713</td>\n",
       "      <td>-0.680504</td>\n",
       "      <td>-0.681784</td>\n",
       "      <td>-0.681444</td>\n",
       "      <td>-0.681549</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>{'m': 0, 'p': 1000.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1000.0}}</td>\n",
       "      <td>-0.687289</td>\n",
       "      <td>-0.686199</td>\n",
       "      <td>-0.687997</td>\n",
       "      <td>-0.686957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.687107</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.686386</td>\n",
       "      <td>-0.687198</td>\n",
       "      <td>-0.685929</td>\n",
       "      <td>-0.686666</td>\n",
       "      <td>-0.686464</td>\n",
       "      <td>-0.686528</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.031313      0.002291         0.000839        0.000047   \n",
       "1        0.027904      0.002463         0.000908        0.000059   \n",
       "2        0.026473      0.002976         0.000908        0.000038   \n",
       "3        0.024158      0.003380         0.000918        0.000232   \n",
       "4        0.020368      0.001990         0.000767        0.000016   \n",
       "5        0.018054      0.001623         0.000806        0.000030   \n",
       "6        0.016542      0.000949         0.000795        0.000027   \n",
       "7        0.014102      0.001220         0.000799        0.000043   \n",
       "8        0.016554      0.003581         0.001007        0.000225   \n",
       "9        0.012117      0.000650         0.000840        0.000099   \n",
       "10       0.010755      0.000348         0.000772        0.000020   \n",
       "11       0.010590      0.001100         0.000906        0.000089   \n",
       "12       0.009805      0.000603         0.000890        0.000036   \n",
       "13       0.008785      0.000387         0.000873        0.000024   \n",
       "14       0.008291      0.000209         0.000863        0.000058   \n",
       "15       0.006890      0.000330         0.000817        0.000008   \n",
       "16       0.006114      0.000136         0.000789        0.000007   \n",
       "17       0.006213      0.000301         0.000826        0.000047   \n",
       "18       0.006147      0.000166         0.000807        0.000040   \n",
       "19       0.005532      0.000237         0.000838        0.000071   \n",
       "20       0.005628      0.000574         0.000837        0.000155   \n",
       "21       0.005244      0.000092         0.000781        0.000034   \n",
       "22       0.004629      0.000072         0.000775        0.000010   \n",
       "23       0.004661      0.000172         0.000791        0.000045   \n",
       "24       0.005227      0.000374         0.000854        0.000108   \n",
       "\n",
       "                param_default_parameters  \\\n",
       "0                   {'m': 0, 'p': 0.001}   \n",
       "1   {'m': 0, 'p': 0.0017782794100389228}   \n",
       "2   {'m': 0, 'p': 0.0031622776601683794}   \n",
       "3    {'m': 0, 'p': 0.005623413251903491}   \n",
       "4                    {'m': 0, 'p': 0.01}   \n",
       "5     {'m': 0, 'p': 0.01778279410038923}   \n",
       "6     {'m': 0, 'p': 0.03162277660168379}   \n",
       "7    {'m': 0, 'p': 0.056234132519034905}   \n",
       "8     {'m': 0, 'p': 0.09999999999999999}   \n",
       "9      {'m': 0, 'p': 0.1778279410038923}   \n",
       "10    {'m': 0, 'p': 0.31622776601683794}   \n",
       "11     {'m': 0, 'p': 0.5623413251903491}   \n",
       "12                    {'m': 0, 'p': 1.0}   \n",
       "13     {'m': 0, 'p': 1.7782794100389228}   \n",
       "14      {'m': 0, 'p': 3.162277660168379}   \n",
       "15       {'m': 0, 'p': 5.62341325190349}   \n",
       "16                   {'m': 0, 'p': 10.0}   \n",
       "17      {'m': 0, 'p': 17.78279410038923}   \n",
       "18     {'m': 0, 'p': 31.622776601683796}   \n",
       "19      {'m': 0, 'p': 56.23413251903491}   \n",
       "20                  {'m': 0, 'p': 100.0}   \n",
       "21     {'m': 0, 'p': 177.82794100389228}   \n",
       "22      {'m': 0, 'p': 316.2277660168379}   \n",
       "23       {'m': 0, 'p': 562.341325190349}   \n",
       "24                 {'m': 0, 'p': 1000.0}   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0        {'default_parameters': {'m': 0, 'p': 0.001}}          -0.311680   \n",
       "1   {'default_parameters': {'m': 0, 'p': 0.0017782...          -0.312519   \n",
       "2   {'default_parameters': {'m': 0, 'p': 0.0031622...          -0.313839   \n",
       "3   {'default_parameters': {'m': 0, 'p': 0.0056234...          -0.315778   \n",
       "4         {'default_parameters': {'m': 0, 'p': 0.01}}          -0.318500   \n",
       "5   {'default_parameters': {'m': 0, 'p': 0.0177827...          -0.322193   \n",
       "6   {'default_parameters': {'m': 0, 'p': 0.0316227...          -0.326720   \n",
       "7   {'default_parameters': {'m': 0, 'p': 0.0562341...          -0.331744   \n",
       "8   {'default_parameters': {'m': 0, 'p': 0.0999999...          -0.336485   \n",
       "9   {'default_parameters': {'m': 0, 'p': 0.1778279...          -0.340192   \n",
       "10  {'default_parameters': {'m': 0, 'p': 0.3162277...          -0.343172   \n",
       "11  {'default_parameters': {'m': 0, 'p': 0.5623413...          -0.347524   \n",
       "12         {'default_parameters': {'m': 0, 'p': 1.0}}          -0.356604   \n",
       "13  {'default_parameters': {'m': 0, 'p': 1.7782794...          -0.373814   \n",
       "14  {'default_parameters': {'m': 0, 'p': 3.1622776...          -0.401304   \n",
       "15  {'default_parameters': {'m': 0, 'p': 5.6234132...          -0.438960   \n",
       "16        {'default_parameters': {'m': 0, 'p': 10.0}}          -0.483969   \n",
       "17  {'default_parameters': {'m': 0, 'p': 17.782794...          -0.531612   \n",
       "18  {'default_parameters': {'m': 0, 'p': 31.622776...          -0.576682   \n",
       "19  {'default_parameters': {'m': 0, 'p': 56.234132...          -0.614727   \n",
       "20       {'default_parameters': {'m': 0, 'p': 100.0}}          -0.643393   \n",
       "21  {'default_parameters': {'m': 0, 'p': 177.82794...          -0.662965   \n",
       "22  {'default_parameters': {'m': 0, 'p': 316.22776...          -0.675383   \n",
       "23  {'default_parameters': {'m': 0, 'p': 562.34132...          -0.682888   \n",
       "24      {'default_parameters': {'m': 0, 'p': 1000.0}}          -0.687289   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           -0.432727          -0.624367          -0.275618  ...   \n",
       "1           -0.416230          -0.594905          -0.275720  ...   \n",
       "2           -0.400069          -0.565815          -0.275980  ...   \n",
       "3           -0.383827          -0.537204          -0.276387  ...   \n",
       "4           -0.367465          -0.508722          -0.277033  ...   \n",
       "5           -0.351113          -0.480661          -0.278165  ...   \n",
       "6           -0.334859          -0.453479          -0.280108  ...   \n",
       "7           -0.319195          -0.428453          -0.283286  ...   \n",
       "8           -0.304739          -0.407270          -0.288190  ...   \n",
       "9           -0.292514          -0.392050          -0.295155  ...   \n",
       "10          -0.283906          -0.384348          -0.304565  ...   \n",
       "11          -0.280987          -0.384889          -0.317184  ...   \n",
       "12          -0.286597          -0.393833          -0.334331  ...   \n",
       "13          -0.303805          -0.411297          -0.357686  ...   \n",
       "14          -0.334826          -0.437415          -0.388691  ...   \n",
       "15          -0.379632          -0.471534          -0.427760  ...   \n",
       "16          -0.434967          -0.511277          -0.473538  ...   \n",
       "17          -0.494612          -0.552766          -0.522577  ...   \n",
       "18          -0.551140          -0.591772          -0.569786  ...   \n",
       "19          -0.598364          -0.624705          -0.610040  ...   \n",
       "20          -0.633448          -0.649609          -0.640455  ...   \n",
       "21          -0.657117          -0.666683          -0.661210  ...   \n",
       "22          -0.672011          -0.677551          -0.674364  ...   \n",
       "23          -0.680965          -0.684132          -0.682304  ...   \n",
       "24          -0.686199          -0.687997          -0.686957  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         -0.377357        0.139255               14           -0.189113   \n",
       "1         -0.368316        0.127459               13           -0.189337   \n",
       "2         -0.359529        0.115838               12           -0.189701   \n",
       "3         -0.350936        0.104452               10           -0.190292   \n",
       "4         -0.342501        0.093232                9           -0.191247   \n",
       "5         -0.334384        0.082382                8           -0.192780   \n",
       "6         -0.326743        0.072223                6           -0.195209   \n",
       "7         -0.320023        0.063349                5           -0.198977   \n",
       "8         -0.314785        0.056295                3           -0.204686   \n",
       "9         -0.311855        0.051269                1           -0.213149   \n",
       "10        -0.312331        0.047855                2           -0.225453   \n",
       "11        -0.317767        0.045357                4           -0.242944   \n",
       "12        -0.330119        0.043041                7           -0.267094   \n",
       "13        -0.351396        0.040194               11           -0.299296   \n",
       "14        -0.382953        0.036270               15           -0.340451   \n",
       "15        -0.424439        0.031154               16           -0.390191   \n",
       "16        -0.473051        0.025157               17           -0.446060   \n",
       "17        -0.523968        0.018888               18           -0.503552   \n",
       "18        -0.571728        0.013115               19           -0.557156   \n",
       "19        -0.611712        0.008484               20           -0.601958   \n",
       "20        -0.641630        0.005201               21           -0.635466   \n",
       "21        -0.661955        0.003077               22           -0.658226   \n",
       "22        -0.674810        0.001782               23           -0.672620   \n",
       "23        -0.682565        0.001019               24           -0.681301   \n",
       "24        -0.687107        0.000578               25           -0.686386   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            -0.190769           -0.169377           -0.200170   \n",
       "1            -0.191047           -0.169708           -0.200395   \n",
       "2            -0.191501           -0.170236           -0.200761   \n",
       "3            -0.192236           -0.171084           -0.201352   \n",
       "4            -0.193426           -0.172432           -0.202309   \n",
       "5            -0.195322           -0.174564           -0.203833   \n",
       "6            -0.198308           -0.177902           -0.206232   \n",
       "7            -0.202914           -0.183007           -0.209928   \n",
       "8            -0.209814           -0.190532           -0.215465   \n",
       "9            -0.219822           -0.201120           -0.223533   \n",
       "10           -0.233898           -0.215366           -0.235032   \n",
       "11           -0.253189           -0.233969           -0.251172   \n",
       "12           -0.279034           -0.258058           -0.273492   \n",
       "13           -0.312775           -0.289279           -0.303701   \n",
       "14           -0.355258           -0.329256           -0.343193   \n",
       "15           -0.405968           -0.378408           -0.392096   \n",
       "16           -0.462086           -0.434727           -0.448107   \n",
       "17           -0.518569           -0.493673           -0.506201   \n",
       "18           -0.569756           -0.549364           -0.560084   \n",
       "19           -0.611370           -0.596374           -0.604565   \n",
       "20           -0.641835           -0.631778           -0.637421   \n",
       "21           -0.662240           -0.655934           -0.659532   \n",
       "22           -0.675034           -0.671252           -0.673431   \n",
       "23           -0.682713           -0.680504           -0.681784   \n",
       "24           -0.687198           -0.685929           -0.686666   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0            -0.203862         -0.190658         0.012001  \n",
       "1            -0.204093         -0.190916         0.011964  \n",
       "2            -0.204472         -0.191334         0.011910  \n",
       "3            -0.205086         -0.192010         0.011825  \n",
       "4            -0.206084         -0.193100         0.011697  \n",
       "5            -0.207694         -0.194839         0.011504  \n",
       "6            -0.210266         -0.197583         0.011217  \n",
       "7            -0.214304         -0.201826         0.010815  \n",
       "8            -0.220488         -0.208197         0.010304  \n",
       "9            -0.229618         -0.217448         0.009756  \n",
       "10           -0.242562         -0.230462         0.009296  \n",
       "11           -0.260270         -0.248309         0.009050  \n",
       "12           -0.283907         -0.272317         0.009071  \n",
       "13           -0.314827         -0.303976         0.009308  \n",
       "14           -0.354085         -0.344449         0.009572  \n",
       "15           -0.401450         -0.393623         0.009584  \n",
       "16           -0.454600         -0.449116         0.009118  \n",
       "17           -0.509344         -0.506268         0.008085  \n",
       "18           -0.560646         -0.559401         0.006558  \n",
       "19           -0.603871         -0.603628         0.004822  \n",
       "20           -0.636462         -0.636592         0.003245  \n",
       "21           -0.658739         -0.658934         0.002041  \n",
       "22           -0.672888         -0.673045         0.001227  \n",
       "23           -0.681444         -0.681549         0.000717  \n",
       "24           -0.686464         -0.686528         0.000413  \n",
       "\n",
       "[25 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1778279410038923\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 46;\n",
       "                var nbb_unformatted_code = \"model = BayesianLogisticRegression()\\nparam_grid = {\\\"default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 3, 25)]}\\ngrid_search = GridSearchCV(\\n    model,\\n    param_grid,\\n    scoring=make_scorer(log_loss, greater_is_better=False),\\n    return_train_score=True,\\n)\\ngrid_search.fit(X_train, y_train)\\ndisplay(pd.DataFrame(grid_search.cv_results_))\\nprint(grid_search.best_estimator_.default_parameters[\\\"p\\\"])\";\n",
       "                var nbb_formatted_code = \"model = BayesianLogisticRegression()\\nparam_grid = {\\\"default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 3, 25)]}\\ngrid_search = GridSearchCV(\\n    model,\\n    param_grid,\\n    scoring=make_scorer(log_loss, greater_is_better=False),\\n    return_train_score=True,\\n)\\ngrid_search.fit(X_train, y_train)\\ndisplay(pd.DataFrame(grid_search.cv_results_))\\nprint(grid_search.best_estimator_.default_parameters[\\\"p\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BayesianLogisticRegression()\n",
    "param_grid = {\"default_parameters\": [{\"m\": 0, \"p\": p} for p in np.logspace(-3, 3, 25)]}\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(log_loss, greater_is_better=False),\n",
    "    return_train_score=True,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "display(pd.DataFrame(grid_search.cv_results_))\n",
    "print(grid_search.best_estimator_.default_parameters[\"p\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d9c0c",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e12ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3897214331971086\n",
      "0.83\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 47;\n",
       "                var nbb_unformatted_code = \"y_pred = grid_search.best_estimator_.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = grid_search.best_estimator_.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e5324",
   "metadata": {},
   "source": [
    "### Bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fc8e5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 48;\n",
       "                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "def loss(log_sigma2, X, y):\n",
    "    n_feat = X.shape[1]\n",
    "    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\n",
    "    diag_sigma2 = np.exp(log_sigma2)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=BayesianLogisticRegression()._jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = loss(xk, X, y)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = loss(xk, X, y)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb1653",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02a0bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([1.])=61.060407669288374\n",
      "fun([1.43303518])=60.64547121441359\n",
      "fun([1.50461444])=60.63622982158609\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 60.63622982158609\n",
      "        x: [ 1.505e+00]\n",
      "      nit: 3\n",
      "      jac: [-1.711e-06]\n",
      "     nfev: 4\n",
      "     njev: 4\n",
      " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
      "[0.22210291]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 49;\n",
       "                var nbb_unformatted_code = \"res = minimize(\\n    loss,\\n    np.array([0.0]),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n",
       "                var nbb_formatted_code = \"res = minimize(\\n    loss,\\n    np.array([0.0]),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    loss,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        X_train.to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac,\n",
    "    callback=callback,\n",
    ")\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8ba6f",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99c547a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                               &#x27;p&#x27;: array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianLogisticRegression</label><div class=\"sk-toggleable__content\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                               &#x27;p&#x27;: array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianLogisticRegression(default_parameters={'m': 0,\n",
       "                                               'p': array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method='L-BFGS-B',\n",
       "                           prior_parameters={})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 50;\n",
       "                var nbb_unformatted_code = \"model = BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1 / np.exp(res.x)})\\nmodel.fit(X_train, y_train)\";\n",
       "                var nbb_formatted_code = \"model = BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1 / np.exp(res.x)})\\nmodel.fit(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BayesianLogisticRegression(default_parameters={\"m\": 0, \"p\": 1 / np.exp(res.x)})\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5823562",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "708cb5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.382668628864105\n",
      "0.8366666666666667\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 51;\n",
       "                var nbb_unformatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769d024",
   "metadata": {},
   "source": [
    "## Multiple sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b99d86de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 52;\n",
       "                var nbb_unformatted_code = \"cats = list(X.columns)\";\n",
       "                var nbb_formatted_code = \"cats = list(X.columns)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cats = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc63f684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 53;\n",
       "                var nbb_unformatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\";\n",
       "                var nbb_formatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_id = get_col_to_id(X_train.columns, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d024e1e",
   "metadata": {},
   "source": [
    "### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e9e2b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y, cols, col_to_id):\\n    n_feat = X.shape[1]\\n    sigma2_list = []\\n    for col in cols:\\n        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\\n    diag_sigma2 = np.array(sigma2_list)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y, cols, col_to_id):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y, cols, col_to_id)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y, cols, col_to_id)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y, cols, col_to_id):\\n    n_feat = X.shape[1]\\n    sigma2_list = []\\n    for col in cols:\\n        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\\n    diag_sigma2 = np.array(sigma2_list)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y, cols, col_to_id):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y, cols, col_to_id)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y, cols, col_to_id)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "def loss(log_sigma2, X, y, cols, col_to_id):\n",
    "    n_feat = X.shape[1]\n",
    "    sigma2_list = []\n",
    "    for col in cols:\n",
    "        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\n",
    "    diag_sigma2 = np.array(sigma2_list)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=BayesianLogisticRegression()._jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y, cols, col_to_id):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = loss(xk, X, y, cols, col_to_id)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = loss(xk, X, y, cols, col_to_id)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b16238",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce721f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([-0.12615174 -0.02922013  0.99158052])=59.20723001635101\n",
      "fun([-0.65596357 -0.09487296  2.58736172])=55.51144842183872\n",
      "fun([-0.92151598 -0.04330354  2.7817241 ])=55.354216890488715\n",
      "fun([-1.45481759  0.00776873  2.92284706])=55.22032802613284\n",
      "fun([-1.91015232 -0.0504136   2.87268155])=55.15229109268441\n",
      "fun([-2.32653384  0.08854265  2.77926044])=55.127431462018016\n",
      "fun([-2.83233246 -0.01662156  2.74486042])=55.10404821297138\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 55.10404821297138\n",
      "        x: [-2.832e+00 -1.662e-02  2.745e+00]\n",
      "      nit: 7\n",
      "      jac: [ 2.489e-06  5.180e-06 -3.672e-06]\n",
      "     nfev: 11\n",
      "     njev: 11\n",
      " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
      "[16.98503158  1.01676047  0.06425727]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 55;\n",
       "                var nbb_unformatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\\nres = minimize(\\n    loss,\\n    np.array([0.0] * len(cats)),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n        X_train.columns,\\n        col_to_id,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n",
       "                var nbb_formatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\\nres = minimize(\\n    loss,\\n    np.array([0.0] * len(cats)),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n        X_train.columns,\\n        col_to_id,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_id = get_col_to_id(X_train.columns, cats)\n",
    "res = minimize(\n",
    "    loss,\n",
    "    np.array([0.0] * len(cats)),\n",
    "    args=(\n",
    "        X_train.to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "        X_train.columns,\n",
    "        col_to_id,\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac,\n",
    "    callback=callback,\n",
    ")\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cbcd9",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2ad250f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0, &#x27;p&#x27;: 5},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={&#x27;col1_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985...\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col3_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562}})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianLogisticRegression</label><div class=\"sk-toggleable__content\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0, &#x27;p&#x27;: 5},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={&#x27;col1_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985...\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col3_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562}})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianLogisticRegression(default_parameters={'m': 0, 'p': 5},\n",
       "                           optimize_kwargs={}, optimize_method='L-BFGS-B',\n",
       "                           prior_parameters={'col1_a': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_b': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_c': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_d': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_e': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_f': {'m': 0,\n",
       "                                                        'p': 16.985...\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col2_e': {'m': 0,\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col2_f': {'m': 0,\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col3_a': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_b': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_c': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_d': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_e': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_f': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 56;\n",
       "                var nbb_unformatted_code = \"prior_parameters = {}\\n\\nfor col in X_train.columns:\\n    prior_parameters[col] = {\\\"m\\\": 0, \\\"p\\\": (1 / np.exp(res.x))[col_to_id[col]]}\\n\\nmodel = BayesianLogisticRegression(prior_parameters=prior_parameters)\\nmodel.fit(X_train, y_train)\";\n",
       "                var nbb_formatted_code = \"prior_parameters = {}\\n\\nfor col in X_train.columns:\\n    prior_parameters[col] = {\\\"m\\\": 0, \\\"p\\\": (1 / np.exp(res.x))[col_to_id[col]]}\\n\\nmodel = BayesianLogisticRegression(prior_parameters=prior_parameters)\\nmodel.fit(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prior_parameters = {}\n",
    "\n",
    "for col in X_train.columns:\n",
    "    prior_parameters[col] = {\"m\": 0, \"p\": (1 / np.exp(res.x))[col_to_id[col]]}\n",
    "\n",
    "model = BayesianLogisticRegression(prior_parameters=prior_parameters)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c5079",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a067a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2996493214889956\n",
      "0.8833333333333333\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 57;\n",
       "                var nbb_unformatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cr_model_research",
   "language": "python",
   "name": "cr_model_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
