{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3911da83",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Packages</a></span></li><li><span><a href=\"#Doc\" data-toc-modified-id=\"Doc-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Doc</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayesian-model\" data-toc-modified-id=\"Bayesian-model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Bayesian model</a></span></li><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Optimization</a></span></li></ul></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Link\" data-toc-modified-id=\"Link-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Link</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#Logit\" data-toc-modified-id=\"Logit-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Logit</a></span></li></ul></li><li><span><a href=\"#Bayesian-GLM\" data-toc-modified-id=\"Bayesian-GLM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Bayesian GLM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#BayesianLogisticRegression\" data-toc-modified-id=\"BayesianLogisticRegression-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>BayesianLogisticRegression</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"#Miscellenaous\" data-toc-modified-id=\"Miscellenaous-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Miscellenaous</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read\" data-toc-modified-id=\"Read-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Read</a></span></li><li><span><a href=\"#Split-train-test\" data-toc-modified-id=\"Split-train-test-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Split train test</a></span></li></ul></li><li><span><a href=\"#Studies\" data-toc-modified-id=\"Studies-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Studies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-sigma2\" data-toc-modified-id=\"Single-sigma2-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Single sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-model\" data-toc-modified-id=\"Naive-model-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Naive model</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.1.2.1\"><span class=\"toc-item-num\">5.1.2.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.1.2.2\"><span class=\"toc-item-num\">5.1.2.2&nbsp;&nbsp;</span>Test</a></span></li></ul></li><li><span><a href=\"#Bayesian-optimisation\" data-toc-modified-id=\"Bayesian-optimisation-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Bayesian optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.1.3.1\"><span class=\"toc-item-num\">5.1.3.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-5.1.3.2\"><span class=\"toc-item-num\">5.1.3.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.1.3.3\"><span class=\"toc-item-num\">5.1.3.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li><li><span><a href=\"#Multiple-sigma2\" data-toc-modified-id=\"Multiple-sigma2-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Multiple sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayesian-optimization\" data-toc-modified-id=\"Bayesian-optimization-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Bayesian optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.2.1.1\"><span class=\"toc-item-num\">5.2.1.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-5.2.1.2\"><span class=\"toc-item-num\">5.2.1.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.2.1.3\"><span class=\"toc-item-num\">5.2.1.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95e967",
   "metadata": {},
   "source": [
    "___\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8298c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import itertools\n",
    "import logging\n",
    "from typing import Any, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit, logit\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, make_scorer\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression\n",
    "from scipy.optimize import approx_fprime, minimize\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from module.bayesian_model import BayesianLogisticRegression, BayesianNormalRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8b504",
   "metadata": {},
   "source": [
    "___\n",
    "# Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00b1b9",
   "metadata": {},
   "source": [
    "## Bayesian model\n",
    "The purpose of bayesian optimization is to model the parameters as random variables. For instance, in the use case that I am going to investigate, a binary classification, we can model the law of the dependent variable $Y_{i}$ conditioned on the parameters $\\boldsymbol{\\Theta}$ as a Bernoulli random variable depending on observed independent variables $\\boldsymbol{X_{i}}$ and the parameters as a multivariate normal random variable with a diagonal covariance matrix (parameters of $\\boldsymbol{\\sigma^{2}}$, which is a vector in my notation, may differ). Vectors are written in bold format in the following equations and random variable in uppercase.\n",
    "$$\n",
    "\\begin{align} \n",
    "Y_{i} \\mid \\boldsymbol{\\Theta} & \\sim \\text{Bern}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta}) \\\\\n",
    "\\boldsymbol{\\Theta} & \\sim \\mathcal{N}(0,\\boldsymbol{\\sigma^{2}}I)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It is possible to get one step further by modelling the diagonal elements of the covariance matrix, $\\boldsymbol{\\sigma^{2}}$, as a random variable as well, $\\boldsymbol{\\Sigma^{2}}$, with $Y_{i}$ and $\\boldsymbol{\\sigma^{2}}$ conditionally independent given $\\boldsymbol{\\Theta}$:\n",
    "$$\n",
    "\\begin{align} \n",
    "Y_{i} \\mid \\boldsymbol{\\Theta} & \\sim \\text{Bern}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta}) \\\\\n",
    "\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma} & \\sim \\mathcal{N}(0,\\boldsymbol{\\Sigma^{2}}I) \\\\\n",
    "\\Sigma_{i} & \\sim \\Gamma(\\alpha_{i}, \\beta_{i})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Optimization\n",
    "I am going to focus only on the 2nd model as it is more general. The purpose is to maximize $f_{\\Sigma \\mid \\boldsymbol{Y}}(\\sigma \\mid \\boldsymbol{y})$, $\\boldsymbol{Y}$ being the vectors of all the $Y_{i}$ random variables. Unfortunately, there is no closed form for the law of $Y_{i} \\mid \\boldsymbol{\\Sigma}$ contrary to the normal linear case (when $Y_{i} \\mid \\boldsymbol{\\Theta} \\sim \\mathcal{N}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta},\\sigma^{2}I)$). That's being said, it is possible to approximate the density function of $\\Sigma \\mid \\boldsymbol{Y}$ thanks to the conditional independence and Bayes theorem:\n",
    "$$\n",
    "\\begin{align} \n",
    "f_{\\boldsymbol{\\Sigma} \\mid Y_{i}}(\\boldsymbol{\\sigma} \\mid y_{i})\n",
    "& = \\frac{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} \\mid \\boldsymbol{\\Sigma} = \\sigma ) f_{\\Sigma}(\\sigma)\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\\\\n",
    "& = \\frac{ \n",
    "    f_{Y_{i}, \\boldsymbol{\\Theta}, \\boldsymbol{\\Sigma}}(y_{i}, \\boldsymbol{\\theta}, \\sigma)\n",
    "}{\n",
    "    f_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta} \\mid y_{i}, \\boldsymbol{\\sigma})\n",
    "}\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} \\mid \\boldsymbol{\\Theta} = \\theta )\n",
    "    f_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\sigma})\n",
    "    f_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\sigma})\n",
    "}{\n",
    "    f_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\sigma}}(\\boldsymbol{\\theta} \\mid y_{i}, \\boldsymbol{\\sigma})\n",
    "}\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We would like to find $\\boldsymbol{\\sigma}$ that maximize the previous density function given the observations $\\boldsymbol{y}$. The numerator can be evaluated provided that we know $\\boldsymbol{\\theta}$. The second element of the denominator does not depend on $\\boldsymbol{\\sigma}$ so we can discard it when trying to maximize our objective function. Finally, the first element of the denominator is unknown. This is why we need to approximate it by means of Laplace approximation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\boldsymbol{\\sigma} ; \\boldsymbol{y})\n",
    "&= \\sum \n",
    "    log\\left(\\mathbb{P}\\left( Y_{i} = y_{i} \\mid \\boldsymbol{\\Theta} = \\theta^{*} \\right)\\right)\n",
    "    + log(f_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta^{*}} \\mid \\boldsymbol{\\sigma}))\n",
    "    + log(f_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\sigma}))\n",
    "    - log(\\tilde{f}_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\sigma}}(\\boldsymbol{\\theta^{*}} \\mid y_{i}, \\boldsymbol{\\sigma}))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a63f4e",
   "metadata": {},
   "source": [
    "___\n",
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32c426",
   "metadata": {},
   "source": [
    "## loss and jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399ed157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_single_sigma2(X, y, baysian_model, prior_dist_logpdf=None):\n",
    "    def callback(intermediate_result):\n",
    "        print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "    def loss(log_sigma2, X, y, baysian_model, prior_dist_logpdf = None):\n",
    "        n_feat = X.shape[1]\n",
    "        log_sigma2 = np.array([log_sigma2[0]] * n_feat)\n",
    "        diag_sigma2 = np.exp(log_sigma2)\n",
    "\n",
    "        m = np.array([0] * n_feat)\n",
    "        p = 1 / diag_sigma2\n",
    "\n",
    "        res = minimize(\n",
    "            baysian_model._loss,\n",
    "            np.array([0] * n_feat),\n",
    "            args=(X, y, m, p),\n",
    "            method=\"L-BFGS-B\",\n",
    "            jac=baysian_model._jac,\n",
    "        )\n",
    "\n",
    "        theta_star = res.x\n",
    "\n",
    "        H = baysian_model._hess(theta_star, X, y, m, p)\n",
    "\n",
    "        out = (\n",
    "            baysian_model._loss(theta_star, X, y, m, p)\n",
    "            - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "            + 0.5 * np.linalg.slogdet(H)[1]\n",
    "        )\n",
    "\n",
    "        if prior_dist_logpdf:\n",
    "            out += - np.sum(prior_dist_logpdf(p))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def jac(log_sigma2, X, y, baysian_model, prior_dist_logpdf = None):\n",
    "        h = np.log(1 + 1e-2)\n",
    "        jac_list = []\n",
    "        for ii in range(len(log_sigma2)):\n",
    "            xk = np.copy(log_sigma2)\n",
    "            xk[ii] += h\n",
    "            fk_plus_h = loss(xk, X, y, baysian_model, prior_dist_logpdf)\n",
    "            xk[ii] -= 2 * h\n",
    "            fk_minus_h = loss(xk, X, y, baysian_model, prior_dist_logpdf)\n",
    "            jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "        return np.array(jac_list)\n",
    "\n",
    "    return minimize(\n",
    "        loss,\n",
    "        np.array([0.0]),\n",
    "        args=(\n",
    "            X.to_numpy(),\n",
    "            y.to_numpy(),\n",
    "            baysian_model,\n",
    "            prior_dist_logpdf\n",
    "        ),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=jac,\n",
    "        callback=callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4cc92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_to_id(cols, cats):\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        for ii, cat in enumerate(cats):\n",
    "            if isinstance(cat, str):\n",
    "                if cat in col:\n",
    "                    out[col] = ii\n",
    "            else:\n",
    "                for cc in cat:\n",
    "                    if cc in col:\n",
    "                        out[col] = ii\n",
    "    return out\n",
    "\n",
    "def solve_single_multiple_sigma2(X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "    def callback(intermediate_result):\n",
    "        print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "    def loss(log_sigma2, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "        n_feat = X.shape[1]\n",
    "        sigma2_list = []\n",
    "        for col in cols:\n",
    "            sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\n",
    "        diag_sigma2 = np.array(sigma2_list)\n",
    "\n",
    "        m = np.array([0] * n_feat)\n",
    "        p = 1 / diag_sigma2\n",
    "\n",
    "        res = minimize(\n",
    "            baysian_model._loss,\n",
    "            np.array([0] * n_feat),\n",
    "            args=(X, y, m, p),\n",
    "            method=\"L-BFGS-B\",\n",
    "            jac=baysian_model._jac,\n",
    "        )\n",
    "\n",
    "        theta_star = res.x\n",
    "\n",
    "        H = baysian_model._hess(theta_star, X, y, m, p)\n",
    "\n",
    "        out = (\n",
    "            baysian_model._loss(theta_star, X, y, m, p)\n",
    "            - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "            + 0.5 * np.linalg.slogdet(H)[1]\n",
    "        )\n",
    "\n",
    "        if prior_dist_logpdf:\n",
    "            out += - np.sum(prior_dist_logpdf(p))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def jac(log_sigma2, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "        h = np.log(1 + 1e-2)\n",
    "        jac_list = []\n",
    "        for ii in range(len(log_sigma2)):\n",
    "            xk = np.copy(log_sigma2)\n",
    "            xk[ii] += h\n",
    "            fk_plus_h = loss(xk, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf)\n",
    "            xk[ii] -= 2 * h\n",
    "            fk_minus_h = loss(xk, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf)\n",
    "            jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "        return np.array(jac_list)\n",
    "\n",
    "    return minimize(\n",
    "        loss,\n",
    "        np.array([0.0]),\n",
    "        args=(\n",
    "            X.to_numpy(),\n",
    "            y.to_numpy(),\n",
    "            cols,\n",
    "            col_to_id,\n",
    "            baysian_model,\n",
    "            prior_dist_logpdf\n",
    "        ),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=jac,\n",
    "        callback=callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f72ff",
   "metadata": {},
   "source": [
    "___\n",
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf333b60",
   "metadata": {},
   "source": [
    "## Create artificial dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031edb61",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab81624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_dataset():\n",
    "    n_samples = 200\n",
    "    cats = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "    X = pd.DataFrame(\n",
    "        data={\n",
    "            \"col1\": np.random.choice(cats, size=n_samples),\n",
    "            \"col2\": np.random.choice(cats, size=n_samples),\n",
    "            \"col3\": np.random.choice(cats, size=n_samples),\n",
    "        }\n",
    "    )\n",
    "    X_preprocessed = pd.get_dummies(X)\n",
    "\n",
    "    theta = np.random.multivariate_normal(\n",
    "        np.zeros(len(cats) * X.shape[1]),\n",
    "        np.diag(np.array([1e-1] * len(cats) + [1] * len(cats) + [1e1] * len(cats))),\n",
    "    )\n",
    "\n",
    "    y = pd.Series(\n",
    "        data=np.random.binomial(1, expit(np.dot(X_preprocessed.to_numpy(), theta))),\n",
    "        index=X_preprocessed.index,\n",
    "    )\n",
    "    return X_preprocessed, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1971cfb",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4aed85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_dataset():\n",
    "    n_samples = 200\n",
    "    sigma2 = 3\n",
    "    cats = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "    X = pd.DataFrame(\n",
    "        data={\n",
    "            \"col1\": np.random.choice(cats, size=n_samples),\n",
    "            \"col2\": np.random.choice(cats, size=n_samples),\n",
    "            \"col3\": np.random.choice(cats, size=n_samples),\n",
    "        }\n",
    "    )\n",
    "    X_preprocessed = pd.get_dummies(X)\n",
    "\n",
    "    theta = np.random.multivariate_normal(\n",
    "        np.zeros(len(cats) * X.shape[1]),\n",
    "        np.diag(np.array([1e-1] * len(cats) + [1] * len(cats) + [1e1] * len(cats))),\n",
    "    )\n",
    "\n",
    "    y = pd.Series(\n",
    "        data=np.random.multivariate_normal(np.dot(X_preprocessed.to_numpy(), theta), sigma2*np.eye(n_samples)),\n",
    "        index=X_preprocessed.index,\n",
    "    )\n",
    "    return X_preprocessed, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfce23",
   "metadata": {},
   "source": [
    "___\n",
    "# Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166fdea",
   "metadata": {},
   "source": [
    "## Linear case vs scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916dd7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([1.])=356.6935481590551\n",
      "fun([1.26663693])=355.8719510553316\n",
      "fun([1.43913281])=355.6963535034688\n",
      "fun([1.47525614])=355.6903681055224\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 355.6903681055224\n",
      "        x: [ 1.475e+00]\n",
      "      nit: 4\n",
      "      jac: [-2.687e-06]\n",
      "     nfev: 5\n",
      "     njev: 5\n",
      " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
      "[0.22872014]\n"
     ]
    }
   ],
   "source": [
    "X, y = get_regression_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")\n",
    "res = solve_single_sigma2(X_train, y_train, BayesianNormalRegression())\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33e94f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianRidge(fit_intercept=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;BayesianRidge<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.BayesianRidge.html\">?<span>Documentation for BayesianRidge</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>BayesianRidge(fit_intercept=False)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "BayesianRidge(fit_intercept=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BayesianRidge(fit_intercept=False, lambda_1=1e-6, lambda_2=1e-6, alpha_1=1e-6, alpha_2=1e-6)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9734fac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_iter': 300,\n",
       " 'tol': 0.001,\n",
       " 'alpha_1': 1e-06,\n",
       " 'alpha_2': 1e-06,\n",
       " 'lambda_1': 1e-06,\n",
       " 'lambda_2': 1e-06,\n",
       " 'alpha_init': None,\n",
       " 'lambda_init': None,\n",
       " 'compute_score': False,\n",
       " 'fit_intercept': False,\n",
       " 'copy_X': True,\n",
       " 'verbose': False,\n",
       " 'feature_names_in_': array(['col1_a', 'col1_b', 'col1_c', 'col1_d', 'col1_e', 'col1_f',\n",
       "        'col2_a', 'col2_b', 'col2_c', 'col2_d', 'col2_e', 'col2_f',\n",
       "        'col3_a', 'col3_b', 'col3_c', 'col3_d', 'col3_e', 'col3_f'],\n",
       "       dtype=object),\n",
       " 'n_features_in_': 18,\n",
       " 'X_offset_': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " 'X_scale_': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.]),\n",
       " 'scores_': [],\n",
       " 'n_iter_': 5,\n",
       " 'alpha_': np.float64(0.28787256812755047),\n",
       " 'lambda_': np.float64(0.22734519455979785),\n",
       " 'coef_': array([-1.63045126,  0.02702687,  0.18224659,  0.01348441, -0.84220924,\n",
       "        -1.07400614,  1.57511541, -1.70697957, -0.91117184, -0.7762855 ,\n",
       "        -1.81334485,  0.30875758, -2.84135377,  2.75743459, -2.66436576,\n",
       "         0.55856515,  3.29264464, -4.42683362]),\n",
       " 'sigma_': array([[ 0.59081034,  0.47269253,  0.4678105 ,  0.47044423,  0.4715389 ,\n",
       "          0.47066798, -0.23431561, -0.24191114, -0.2557771 , -0.24538873,\n",
       "         -0.23048463, -0.24675577, -0.23511716, -0.24060745, -0.24192247,\n",
       "         -0.23693222, -0.25322105, -0.24683264],\n",
       "        [ 0.47269253,  0.60143217,  0.46742394,  0.46763406,  0.47002235,\n",
       "          0.46955281, -0.23461666, -0.24558999, -0.24781341, -0.2395086 ,\n",
       "         -0.23202693, -0.25028404, -0.2459096 , -0.2468849 , -0.23902005,\n",
       "         -0.23836293, -0.23955994, -0.2401022 ],\n",
       "        [ 0.4678105 ,  0.46742394,  0.61214091,  0.46509981,  0.46850812,\n",
       "          0.47089096, -0.23854099, -0.25256307, -0.23392494, -0.24110866,\n",
       "         -0.24718868, -0.23339692, -0.2292747 , -0.24766046, -0.24848441,\n",
       "         -0.24966918, -0.23231268, -0.23932181],\n",
       "        [ 0.47044423,  0.46763406,  0.46509981,  0.61401577,  0.46624126,\n",
       "          0.47133458, -0.2615817 , -0.23423568, -0.24455687, -0.24700723,\n",
       "         -0.22496907, -0.23147722, -0.24501234, -0.24156758, -0.24218631,\n",
       "         -0.22851602, -0.23996307, -0.24658244],\n",
       "        [ 0.4715389 ,  0.47002235,  0.46850812,  0.46624126,  0.59745315,\n",
       "          0.46722356, -0.23720915, -0.24240232, -0.24680587, -0.23530748,\n",
       "         -0.25244158, -0.24344373, -0.25167546, -0.22888882, -0.23446821,\n",
       "         -0.25832434, -0.24370919, -0.24054411],\n",
       "        [ 0.47066798,  0.46955281,  0.47089096,  0.47133458,  0.46722356,\n",
       "          0.59758825, -0.25129548, -0.24657215, -0.22893718, -0.24559849,\n",
       "         -0.24292404, -0.23601202, -0.23536948, -0.25626098, -0.24216412,\n",
       "         -0.22929464, -0.24472316, -0.24352697],\n",
       "        [-0.23431561, -0.23461666, -0.23854099, -0.2615817 , -0.23720915,\n",
       "         -0.25129548,  0.5948603 ,  0.47211241,  0.47165038,  0.47332233,\n",
       "          0.46540289,  0.4636896 , -0.23590384, -0.24659927, -0.23962821,\n",
       "         -0.2549599 , -0.23906395, -0.24140442],\n",
       "        [-0.24191114, -0.24558999, -0.25256307, -0.23423568, -0.24240232,\n",
       "         -0.24657215,  0.47211241,  0.58356778,  0.47285386,  0.47061585,\n",
       "          0.46592798,  0.47024526, -0.26296556, -0.23403189, -0.24625341,\n",
       "         -0.24300647, -0.23340587, -0.24361115],\n",
       "        [-0.2557771 , -0.24781341, -0.23392494, -0.24455687, -0.24680587,\n",
       "         -0.22893718,  0.47165038,  0.47285386,  0.58787889,  0.47313944,\n",
       "          0.46425066,  0.47100888, -0.23463283, -0.24544403, -0.24876378,\n",
       "         -0.2401888 , -0.24449106, -0.24429488],\n",
       "        [-0.24538873, -0.2395086 , -0.24110866, -0.24700723, -0.23530748,\n",
       "         -0.24559849,  0.47332233,  0.47061585,  0.47313944,  0.59694445,\n",
       "          0.46323245,  0.46742378, -0.23733309, -0.2516107 , -0.23575804,\n",
       "         -0.23874453, -0.25659113, -0.23388169],\n",
       "        [-0.23048463, -0.23202693, -0.24718868, -0.22496907, -0.25244158,\n",
       "         -0.24292404,  0.46540289,  0.46592798,  0.46425066,  0.46323245,\n",
       "          0.65317153,  0.45657704, -0.22540075, -0.24873592, -0.22579   ,\n",
       "         -0.23951536, -0.23261482, -0.25797807],\n",
       "        [-0.24675577, -0.25028404, -0.23339692, -0.23147722, -0.24344373,\n",
       "         -0.23601202,  0.4636896 ,  0.47024526,  0.47100888,  0.46742378,\n",
       "          0.45657704,  0.62828322, -0.24612269, -0.23544838, -0.25205215,\n",
       "         -0.22468426, -0.24732226, -0.23573996],\n",
       "        [-0.23511716, -0.2459096 , -0.2292747 , -0.24501234, -0.25167546,\n",
       "         -0.23536948, -0.23590384, -0.26296556, -0.23463283, -0.23733309,\n",
       "         -0.22540075, -0.24612269,  0.62058719,  0.46607198,  0.46837887,\n",
       "          0.46578201,  0.46748552,  0.46793315],\n",
       "        [-0.24060745, -0.2468849 , -0.24766046, -0.24156758, -0.22888882,\n",
       "         -0.25626098, -0.24659927, -0.23403189, -0.24544403, -0.2516107 ,\n",
       "         -0.24873592, -0.23544838,  0.46607198,  0.58342943,  0.4711533 ,\n",
       "          0.46804731,  0.47407495,  0.47395031],\n",
       "        [-0.24192247, -0.23902005, -0.24848441, -0.24218631, -0.23446821,\n",
       "         -0.24216412, -0.23962821, -0.24625341, -0.24876378, -0.23575804,\n",
       "         -0.22579   , -0.25205215,  0.46837887,  0.4711533 ,  0.60662184,\n",
       "          0.46494671,  0.46989293,  0.46935823],\n",
       "        [-0.23693222, -0.23836293, -0.24966918, -0.22851602, -0.25832434,\n",
       "         -0.22929464, -0.2549599 , -0.24300647, -0.2401888 , -0.23874453,\n",
       "         -0.23951536, -0.22468426,  0.46578201,  0.46804731,  0.46494671,\n",
       "          0.62470619,  0.46554105,  0.46847489],\n",
       "        [-0.25322105, -0.23955994, -0.23231268, -0.23996307, -0.24370919,\n",
       "         -0.24472316, -0.23906395, -0.23340587, -0.24449106, -0.25659113,\n",
       "         -0.23261482, -0.24732226,  0.46748552,  0.47407495,  0.46989293,\n",
       "          0.46554105,  0.59751825,  0.47059569],\n",
       "        [-0.24683264, -0.2401022 , -0.23932181, -0.24658244, -0.24054411,\n",
       "         -0.24352697, -0.24140442, -0.24361115, -0.24429488, -0.23388169,\n",
       "         -0.25797807, -0.23573996,  0.46793315,  0.47395031,  0.46935823,\n",
       "          0.46847489,  0.47059569,  0.59137505]]),\n",
       " 'intercept_': 0.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e400bbf",
   "metadata": {},
   "source": [
    "## Single sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422bdfc",
   "metadata": {},
   "source": [
    "### Naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4580a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5623351446188083\n",
      "0.5\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 45;\n                var nbb_unformatted_code = \"y_pred = np.array([np.mean(y_train)] * len(y_test))\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_formatted_code = \"y_pred = np.array([np.mean(y_train)] * len(y_test))\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = np.array([np.mean(y_train)] * len(y_test))\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab81dd",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616d7ea",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0b57a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_default_parameters</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031313</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>{'m': 0, 'p': 0.001}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.001}}</td>\n",
       "      <td>-0.311680</td>\n",
       "      <td>-0.432727</td>\n",
       "      <td>-0.624367</td>\n",
       "      <td>-0.275618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377357</td>\n",
       "      <td>0.139255</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.189113</td>\n",
       "      <td>-0.190769</td>\n",
       "      <td>-0.169377</td>\n",
       "      <td>-0.200170</td>\n",
       "      <td>-0.203862</td>\n",
       "      <td>-0.190658</td>\n",
       "      <td>0.012001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027904</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>{'m': 0, 'p': 0.0017782794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0017782...</td>\n",
       "      <td>-0.312519</td>\n",
       "      <td>-0.416230</td>\n",
       "      <td>-0.594905</td>\n",
       "      <td>-0.275720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368316</td>\n",
       "      <td>0.127459</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.189337</td>\n",
       "      <td>-0.191047</td>\n",
       "      <td>-0.169708</td>\n",
       "      <td>-0.200395</td>\n",
       "      <td>-0.204093</td>\n",
       "      <td>-0.190916</td>\n",
       "      <td>0.011964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026473</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>{'m': 0, 'p': 0.0031622776601683794}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0031622...</td>\n",
       "      <td>-0.313839</td>\n",
       "      <td>-0.400069</td>\n",
       "      <td>-0.565815</td>\n",
       "      <td>-0.275980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359529</td>\n",
       "      <td>0.115838</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.189701</td>\n",
       "      <td>-0.191501</td>\n",
       "      <td>-0.170236</td>\n",
       "      <td>-0.200761</td>\n",
       "      <td>-0.204472</td>\n",
       "      <td>-0.191334</td>\n",
       "      <td>0.011910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024158</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>{'m': 0, 'p': 0.005623413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0056234...</td>\n",
       "      <td>-0.315778</td>\n",
       "      <td>-0.383827</td>\n",
       "      <td>-0.537204</td>\n",
       "      <td>-0.276387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350936</td>\n",
       "      <td>0.104452</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.190292</td>\n",
       "      <td>-0.192236</td>\n",
       "      <td>-0.171084</td>\n",
       "      <td>-0.201352</td>\n",
       "      <td>-0.205086</td>\n",
       "      <td>-0.192010</td>\n",
       "      <td>0.011825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>{'m': 0, 'p': 0.01}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.01}}</td>\n",
       "      <td>-0.318500</td>\n",
       "      <td>-0.367465</td>\n",
       "      <td>-0.508722</td>\n",
       "      <td>-0.277033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342501</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.191247</td>\n",
       "      <td>-0.193426</td>\n",
       "      <td>-0.172432</td>\n",
       "      <td>-0.202309</td>\n",
       "      <td>-0.206084</td>\n",
       "      <td>-0.193100</td>\n",
       "      <td>0.011697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>{'m': 0, 'p': 0.01778279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0177827...</td>\n",
       "      <td>-0.322193</td>\n",
       "      <td>-0.351113</td>\n",
       "      <td>-0.480661</td>\n",
       "      <td>-0.278165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334384</td>\n",
       "      <td>0.082382</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.192780</td>\n",
       "      <td>-0.195322</td>\n",
       "      <td>-0.174564</td>\n",
       "      <td>-0.203833</td>\n",
       "      <td>-0.207694</td>\n",
       "      <td>-0.194839</td>\n",
       "      <td>0.011504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>{'m': 0, 'p': 0.03162277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0316227...</td>\n",
       "      <td>-0.326720</td>\n",
       "      <td>-0.334859</td>\n",
       "      <td>-0.453479</td>\n",
       "      <td>-0.280108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326743</td>\n",
       "      <td>0.072223</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.195209</td>\n",
       "      <td>-0.198308</td>\n",
       "      <td>-0.177902</td>\n",
       "      <td>-0.206232</td>\n",
       "      <td>-0.210266</td>\n",
       "      <td>-0.197583</td>\n",
       "      <td>0.011217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.014102</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>{'m': 0, 'p': 0.056234132519034905}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0562341...</td>\n",
       "      <td>-0.331744</td>\n",
       "      <td>-0.319195</td>\n",
       "      <td>-0.428453</td>\n",
       "      <td>-0.283286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320023</td>\n",
       "      <td>0.063349</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.198977</td>\n",
       "      <td>-0.202914</td>\n",
       "      <td>-0.183007</td>\n",
       "      <td>-0.209928</td>\n",
       "      <td>-0.214304</td>\n",
       "      <td>-0.201826</td>\n",
       "      <td>0.010815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016554</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>{'m': 0, 'p': 0.09999999999999999}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0999999...</td>\n",
       "      <td>-0.336485</td>\n",
       "      <td>-0.304739</td>\n",
       "      <td>-0.407270</td>\n",
       "      <td>-0.288190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314785</td>\n",
       "      <td>0.056295</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.204686</td>\n",
       "      <td>-0.209814</td>\n",
       "      <td>-0.190532</td>\n",
       "      <td>-0.215465</td>\n",
       "      <td>-0.220488</td>\n",
       "      <td>-0.208197</td>\n",
       "      <td>0.010304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>{'m': 0, 'p': 0.1778279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.1778279...</td>\n",
       "      <td>-0.340192</td>\n",
       "      <td>-0.292514</td>\n",
       "      <td>-0.392050</td>\n",
       "      <td>-0.295155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311855</td>\n",
       "      <td>0.051269</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.213149</td>\n",
       "      <td>-0.219822</td>\n",
       "      <td>-0.201120</td>\n",
       "      <td>-0.223533</td>\n",
       "      <td>-0.229618</td>\n",
       "      <td>-0.217448</td>\n",
       "      <td>0.009756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>{'m': 0, 'p': 0.31622776601683794}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.3162277...</td>\n",
       "      <td>-0.343172</td>\n",
       "      <td>-0.283906</td>\n",
       "      <td>-0.384348</td>\n",
       "      <td>-0.304565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312331</td>\n",
       "      <td>0.047855</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.225453</td>\n",
       "      <td>-0.233898</td>\n",
       "      <td>-0.215366</td>\n",
       "      <td>-0.235032</td>\n",
       "      <td>-0.242562</td>\n",
       "      <td>-0.230462</td>\n",
       "      <td>0.009296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>{'m': 0, 'p': 0.5623413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.5623413...</td>\n",
       "      <td>-0.347524</td>\n",
       "      <td>-0.280987</td>\n",
       "      <td>-0.384889</td>\n",
       "      <td>-0.317184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317767</td>\n",
       "      <td>0.045357</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.242944</td>\n",
       "      <td>-0.253189</td>\n",
       "      <td>-0.233969</td>\n",
       "      <td>-0.251172</td>\n",
       "      <td>-0.260270</td>\n",
       "      <td>-0.248309</td>\n",
       "      <td>0.009050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>{'m': 0, 'p': 1.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1.0}}</td>\n",
       "      <td>-0.356604</td>\n",
       "      <td>-0.286597</td>\n",
       "      <td>-0.393833</td>\n",
       "      <td>-0.334331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330119</td>\n",
       "      <td>0.043041</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.267094</td>\n",
       "      <td>-0.279034</td>\n",
       "      <td>-0.258058</td>\n",
       "      <td>-0.273492</td>\n",
       "      <td>-0.283907</td>\n",
       "      <td>-0.272317</td>\n",
       "      <td>0.009071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008785</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>{'m': 0, 'p': 1.7782794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1.7782794...</td>\n",
       "      <td>-0.373814</td>\n",
       "      <td>-0.303805</td>\n",
       "      <td>-0.411297</td>\n",
       "      <td>-0.357686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351396</td>\n",
       "      <td>0.040194</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.299296</td>\n",
       "      <td>-0.312775</td>\n",
       "      <td>-0.289279</td>\n",
       "      <td>-0.303701</td>\n",
       "      <td>-0.314827</td>\n",
       "      <td>-0.303976</td>\n",
       "      <td>0.009308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>{'m': 0, 'p': 3.162277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 3.1622776...</td>\n",
       "      <td>-0.401304</td>\n",
       "      <td>-0.334826</td>\n",
       "      <td>-0.437415</td>\n",
       "      <td>-0.388691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.382953</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.340451</td>\n",
       "      <td>-0.355258</td>\n",
       "      <td>-0.329256</td>\n",
       "      <td>-0.343193</td>\n",
       "      <td>-0.354085</td>\n",
       "      <td>-0.344449</td>\n",
       "      <td>0.009572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.006890</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>{'m': 0, 'p': 5.62341325190349}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 5.6234132...</td>\n",
       "      <td>-0.438960</td>\n",
       "      <td>-0.379632</td>\n",
       "      <td>-0.471534</td>\n",
       "      <td>-0.427760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.390191</td>\n",
       "      <td>-0.405968</td>\n",
       "      <td>-0.378408</td>\n",
       "      <td>-0.392096</td>\n",
       "      <td>-0.401450</td>\n",
       "      <td>-0.393623</td>\n",
       "      <td>0.009584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>{'m': 0, 'p': 10.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 10.0}}</td>\n",
       "      <td>-0.483969</td>\n",
       "      <td>-0.434967</td>\n",
       "      <td>-0.511277</td>\n",
       "      <td>-0.473538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473051</td>\n",
       "      <td>0.025157</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.446060</td>\n",
       "      <td>-0.462086</td>\n",
       "      <td>-0.434727</td>\n",
       "      <td>-0.448107</td>\n",
       "      <td>-0.454600</td>\n",
       "      <td>-0.449116</td>\n",
       "      <td>0.009118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>{'m': 0, 'p': 17.78279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 17.782794...</td>\n",
       "      <td>-0.531612</td>\n",
       "      <td>-0.494612</td>\n",
       "      <td>-0.552766</td>\n",
       "      <td>-0.522577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523968</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.503552</td>\n",
       "      <td>-0.518569</td>\n",
       "      <td>-0.493673</td>\n",
       "      <td>-0.506201</td>\n",
       "      <td>-0.509344</td>\n",
       "      <td>-0.506268</td>\n",
       "      <td>0.008085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.006147</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>{'m': 0, 'p': 31.622776601683796}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 31.622776...</td>\n",
       "      <td>-0.576682</td>\n",
       "      <td>-0.551140</td>\n",
       "      <td>-0.591772</td>\n",
       "      <td>-0.569786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571728</td>\n",
       "      <td>0.013115</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.557156</td>\n",
       "      <td>-0.569756</td>\n",
       "      <td>-0.549364</td>\n",
       "      <td>-0.560084</td>\n",
       "      <td>-0.560646</td>\n",
       "      <td>-0.559401</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>{'m': 0, 'p': 56.23413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 56.234132...</td>\n",
       "      <td>-0.614727</td>\n",
       "      <td>-0.598364</td>\n",
       "      <td>-0.624705</td>\n",
       "      <td>-0.610040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611712</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.601958</td>\n",
       "      <td>-0.611370</td>\n",
       "      <td>-0.596374</td>\n",
       "      <td>-0.604565</td>\n",
       "      <td>-0.603871</td>\n",
       "      <td>-0.603628</td>\n",
       "      <td>0.004822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.005628</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>{'m': 0, 'p': 100.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 100.0}}</td>\n",
       "      <td>-0.643393</td>\n",
       "      <td>-0.633448</td>\n",
       "      <td>-0.649609</td>\n",
       "      <td>-0.640455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641630</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.635466</td>\n",
       "      <td>-0.641835</td>\n",
       "      <td>-0.631778</td>\n",
       "      <td>-0.637421</td>\n",
       "      <td>-0.636462</td>\n",
       "      <td>-0.636592</td>\n",
       "      <td>0.003245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>{'m': 0, 'p': 177.82794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 177.82794...</td>\n",
       "      <td>-0.662965</td>\n",
       "      <td>-0.657117</td>\n",
       "      <td>-0.666683</td>\n",
       "      <td>-0.661210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.661955</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.658226</td>\n",
       "      <td>-0.662240</td>\n",
       "      <td>-0.655934</td>\n",
       "      <td>-0.659532</td>\n",
       "      <td>-0.658739</td>\n",
       "      <td>-0.658934</td>\n",
       "      <td>0.002041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>{'m': 0, 'p': 316.2277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 316.22776...</td>\n",
       "      <td>-0.675383</td>\n",
       "      <td>-0.672011</td>\n",
       "      <td>-0.677551</td>\n",
       "      <td>-0.674364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674810</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.672620</td>\n",
       "      <td>-0.675034</td>\n",
       "      <td>-0.671252</td>\n",
       "      <td>-0.673431</td>\n",
       "      <td>-0.672888</td>\n",
       "      <td>-0.673045</td>\n",
       "      <td>0.001227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>{'m': 0, 'p': 562.341325190349}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 562.34132...</td>\n",
       "      <td>-0.682888</td>\n",
       "      <td>-0.680965</td>\n",
       "      <td>-0.684132</td>\n",
       "      <td>-0.682304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.682565</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.681301</td>\n",
       "      <td>-0.682713</td>\n",
       "      <td>-0.680504</td>\n",
       "      <td>-0.681784</td>\n",
       "      <td>-0.681444</td>\n",
       "      <td>-0.681549</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>{'m': 0, 'p': 1000.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1000.0}}</td>\n",
       "      <td>-0.687289</td>\n",
       "      <td>-0.686199</td>\n",
       "      <td>-0.687997</td>\n",
       "      <td>-0.686957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.687107</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.686386</td>\n",
       "      <td>-0.687198</td>\n",
       "      <td>-0.685929</td>\n",
       "      <td>-0.686666</td>\n",
       "      <td>-0.686464</td>\n",
       "      <td>-0.686528</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.031313      0.002291         0.000839        0.000047   \n",
       "1        0.027904      0.002463         0.000908        0.000059   \n",
       "2        0.026473      0.002976         0.000908        0.000038   \n",
       "3        0.024158      0.003380         0.000918        0.000232   \n",
       "4        0.020368      0.001990         0.000767        0.000016   \n",
       "5        0.018054      0.001623         0.000806        0.000030   \n",
       "6        0.016542      0.000949         0.000795        0.000027   \n",
       "7        0.014102      0.001220         0.000799        0.000043   \n",
       "8        0.016554      0.003581         0.001007        0.000225   \n",
       "9        0.012117      0.000650         0.000840        0.000099   \n",
       "10       0.010755      0.000348         0.000772        0.000020   \n",
       "11       0.010590      0.001100         0.000906        0.000089   \n",
       "12       0.009805      0.000603         0.000890        0.000036   \n",
       "13       0.008785      0.000387         0.000873        0.000024   \n",
       "14       0.008291      0.000209         0.000863        0.000058   \n",
       "15       0.006890      0.000330         0.000817        0.000008   \n",
       "16       0.006114      0.000136         0.000789        0.000007   \n",
       "17       0.006213      0.000301         0.000826        0.000047   \n",
       "18       0.006147      0.000166         0.000807        0.000040   \n",
       "19       0.005532      0.000237         0.000838        0.000071   \n",
       "20       0.005628      0.000574         0.000837        0.000155   \n",
       "21       0.005244      0.000092         0.000781        0.000034   \n",
       "22       0.004629      0.000072         0.000775        0.000010   \n",
       "23       0.004661      0.000172         0.000791        0.000045   \n",
       "24       0.005227      0.000374         0.000854        0.000108   \n",
       "\n",
       "                param_default_parameters  \\\n",
       "0                   {'m': 0, 'p': 0.001}   \n",
       "1   {'m': 0, 'p': 0.0017782794100389228}   \n",
       "2   {'m': 0, 'p': 0.0031622776601683794}   \n",
       "3    {'m': 0, 'p': 0.005623413251903491}   \n",
       "4                    {'m': 0, 'p': 0.01}   \n",
       "5     {'m': 0, 'p': 0.01778279410038923}   \n",
       "6     {'m': 0, 'p': 0.03162277660168379}   \n",
       "7    {'m': 0, 'p': 0.056234132519034905}   \n",
       "8     {'m': 0, 'p': 0.09999999999999999}   \n",
       "9      {'m': 0, 'p': 0.1778279410038923}   \n",
       "10    {'m': 0, 'p': 0.31622776601683794}   \n",
       "11     {'m': 0, 'p': 0.5623413251903491}   \n",
       "12                    {'m': 0, 'p': 1.0}   \n",
       "13     {'m': 0, 'p': 1.7782794100389228}   \n",
       "14      {'m': 0, 'p': 3.162277660168379}   \n",
       "15       {'m': 0, 'p': 5.62341325190349}   \n",
       "16                   {'m': 0, 'p': 10.0}   \n",
       "17      {'m': 0, 'p': 17.78279410038923}   \n",
       "18     {'m': 0, 'p': 31.622776601683796}   \n",
       "19      {'m': 0, 'p': 56.23413251903491}   \n",
       "20                  {'m': 0, 'p': 100.0}   \n",
       "21     {'m': 0, 'p': 177.82794100389228}   \n",
       "22      {'m': 0, 'p': 316.2277660168379}   \n",
       "23       {'m': 0, 'p': 562.341325190349}   \n",
       "24                 {'m': 0, 'p': 1000.0}   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0        {'default_parameters': {'m': 0, 'p': 0.001}}          -0.311680   \n",
       "1   {'default_parameters': {'m': 0, 'p': 0.0017782...          -0.312519   \n",
       "2   {'default_parameters': {'m': 0, 'p': 0.0031622...          -0.313839   \n",
       "3   {'default_parameters': {'m': 0, 'p': 0.0056234...          -0.315778   \n",
       "4         {'default_parameters': {'m': 0, 'p': 0.01}}          -0.318500   \n",
       "5   {'default_parameters': {'m': 0, 'p': 0.0177827...          -0.322193   \n",
       "6   {'default_parameters': {'m': 0, 'p': 0.0316227...          -0.326720   \n",
       "7   {'default_parameters': {'m': 0, 'p': 0.0562341...          -0.331744   \n",
       "8   {'default_parameters': {'m': 0, 'p': 0.0999999...          -0.336485   \n",
       "9   {'default_parameters': {'m': 0, 'p': 0.1778279...          -0.340192   \n",
       "10  {'default_parameters': {'m': 0, 'p': 0.3162277...          -0.343172   \n",
       "11  {'default_parameters': {'m': 0, 'p': 0.5623413...          -0.347524   \n",
       "12         {'default_parameters': {'m': 0, 'p': 1.0}}          -0.356604   \n",
       "13  {'default_parameters': {'m': 0, 'p': 1.7782794...          -0.373814   \n",
       "14  {'default_parameters': {'m': 0, 'p': 3.1622776...          -0.401304   \n",
       "15  {'default_parameters': {'m': 0, 'p': 5.6234132...          -0.438960   \n",
       "16        {'default_parameters': {'m': 0, 'p': 10.0}}          -0.483969   \n",
       "17  {'default_parameters': {'m': 0, 'p': 17.782794...          -0.531612   \n",
       "18  {'default_parameters': {'m': 0, 'p': 31.622776...          -0.576682   \n",
       "19  {'default_parameters': {'m': 0, 'p': 56.234132...          -0.614727   \n",
       "20       {'default_parameters': {'m': 0, 'p': 100.0}}          -0.643393   \n",
       "21  {'default_parameters': {'m': 0, 'p': 177.82794...          -0.662965   \n",
       "22  {'default_parameters': {'m': 0, 'p': 316.22776...          -0.675383   \n",
       "23  {'default_parameters': {'m': 0, 'p': 562.34132...          -0.682888   \n",
       "24      {'default_parameters': {'m': 0, 'p': 1000.0}}          -0.687289   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           -0.432727          -0.624367          -0.275618  ...   \n",
       "1           -0.416230          -0.594905          -0.275720  ...   \n",
       "2           -0.400069          -0.565815          -0.275980  ...   \n",
       "3           -0.383827          -0.537204          -0.276387  ...   \n",
       "4           -0.367465          -0.508722          -0.277033  ...   \n",
       "5           -0.351113          -0.480661          -0.278165  ...   \n",
       "6           -0.334859          -0.453479          -0.280108  ...   \n",
       "7           -0.319195          -0.428453          -0.283286  ...   \n",
       "8           -0.304739          -0.407270          -0.288190  ...   \n",
       "9           -0.292514          -0.392050          -0.295155  ...   \n",
       "10          -0.283906          -0.384348          -0.304565  ...   \n",
       "11          -0.280987          -0.384889          -0.317184  ...   \n",
       "12          -0.286597          -0.393833          -0.334331  ...   \n",
       "13          -0.303805          -0.411297          -0.357686  ...   \n",
       "14          -0.334826          -0.437415          -0.388691  ...   \n",
       "15          -0.379632          -0.471534          -0.427760  ...   \n",
       "16          -0.434967          -0.511277          -0.473538  ...   \n",
       "17          -0.494612          -0.552766          -0.522577  ...   \n",
       "18          -0.551140          -0.591772          -0.569786  ...   \n",
       "19          -0.598364          -0.624705          -0.610040  ...   \n",
       "20          -0.633448          -0.649609          -0.640455  ...   \n",
       "21          -0.657117          -0.666683          -0.661210  ...   \n",
       "22          -0.672011          -0.677551          -0.674364  ...   \n",
       "23          -0.680965          -0.684132          -0.682304  ...   \n",
       "24          -0.686199          -0.687997          -0.686957  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         -0.377357        0.139255               14           -0.189113   \n",
       "1         -0.368316        0.127459               13           -0.189337   \n",
       "2         -0.359529        0.115838               12           -0.189701   \n",
       "3         -0.350936        0.104452               10           -0.190292   \n",
       "4         -0.342501        0.093232                9           -0.191247   \n",
       "5         -0.334384        0.082382                8           -0.192780   \n",
       "6         -0.326743        0.072223                6           -0.195209   \n",
       "7         -0.320023        0.063349                5           -0.198977   \n",
       "8         -0.314785        0.056295                3           -0.204686   \n",
       "9         -0.311855        0.051269                1           -0.213149   \n",
       "10        -0.312331        0.047855                2           -0.225453   \n",
       "11        -0.317767        0.045357                4           -0.242944   \n",
       "12        -0.330119        0.043041                7           -0.267094   \n",
       "13        -0.351396        0.040194               11           -0.299296   \n",
       "14        -0.382953        0.036270               15           -0.340451   \n",
       "15        -0.424439        0.031154               16           -0.390191   \n",
       "16        -0.473051        0.025157               17           -0.446060   \n",
       "17        -0.523968        0.018888               18           -0.503552   \n",
       "18        -0.571728        0.013115               19           -0.557156   \n",
       "19        -0.611712        0.008484               20           -0.601958   \n",
       "20        -0.641630        0.005201               21           -0.635466   \n",
       "21        -0.661955        0.003077               22           -0.658226   \n",
       "22        -0.674810        0.001782               23           -0.672620   \n",
       "23        -0.682565        0.001019               24           -0.681301   \n",
       "24        -0.687107        0.000578               25           -0.686386   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            -0.190769           -0.169377           -0.200170   \n",
       "1            -0.191047           -0.169708           -0.200395   \n",
       "2            -0.191501           -0.170236           -0.200761   \n",
       "3            -0.192236           -0.171084           -0.201352   \n",
       "4            -0.193426           -0.172432           -0.202309   \n",
       "5            -0.195322           -0.174564           -0.203833   \n",
       "6            -0.198308           -0.177902           -0.206232   \n",
       "7            -0.202914           -0.183007           -0.209928   \n",
       "8            -0.209814           -0.190532           -0.215465   \n",
       "9            -0.219822           -0.201120           -0.223533   \n",
       "10           -0.233898           -0.215366           -0.235032   \n",
       "11           -0.253189           -0.233969           -0.251172   \n",
       "12           -0.279034           -0.258058           -0.273492   \n",
       "13           -0.312775           -0.289279           -0.303701   \n",
       "14           -0.355258           -0.329256           -0.343193   \n",
       "15           -0.405968           -0.378408           -0.392096   \n",
       "16           -0.462086           -0.434727           -0.448107   \n",
       "17           -0.518569           -0.493673           -0.506201   \n",
       "18           -0.569756           -0.549364           -0.560084   \n",
       "19           -0.611370           -0.596374           -0.604565   \n",
       "20           -0.641835           -0.631778           -0.637421   \n",
       "21           -0.662240           -0.655934           -0.659532   \n",
       "22           -0.675034           -0.671252           -0.673431   \n",
       "23           -0.682713           -0.680504           -0.681784   \n",
       "24           -0.687198           -0.685929           -0.686666   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0            -0.203862         -0.190658         0.012001  \n",
       "1            -0.204093         -0.190916         0.011964  \n",
       "2            -0.204472         -0.191334         0.011910  \n",
       "3            -0.205086         -0.192010         0.011825  \n",
       "4            -0.206084         -0.193100         0.011697  \n",
       "5            -0.207694         -0.194839         0.011504  \n",
       "6            -0.210266         -0.197583         0.011217  \n",
       "7            -0.214304         -0.201826         0.010815  \n",
       "8            -0.220488         -0.208197         0.010304  \n",
       "9            -0.229618         -0.217448         0.009756  \n",
       "10           -0.242562         -0.230462         0.009296  \n",
       "11           -0.260270         -0.248309         0.009050  \n",
       "12           -0.283907         -0.272317         0.009071  \n",
       "13           -0.314827         -0.303976         0.009308  \n",
       "14           -0.354085         -0.344449         0.009572  \n",
       "15           -0.401450         -0.393623         0.009584  \n",
       "16           -0.454600         -0.449116         0.009118  \n",
       "17           -0.509344         -0.506268         0.008085  \n",
       "18           -0.560646         -0.559401         0.006558  \n",
       "19           -0.603871         -0.603628         0.004822  \n",
       "20           -0.636462         -0.636592         0.003245  \n",
       "21           -0.658739         -0.658934         0.002041  \n",
       "22           -0.672888         -0.673045         0.001227  \n",
       "23           -0.681444         -0.681549         0.000717  \n",
       "24           -0.686464         -0.686528         0.000413  \n",
       "\n",
       "[25 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1778279410038923\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 46;\n                var nbb_unformatted_code = \"model = BayesianLogisticRegression()\\nparam_grid = {\\\"default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 3, 25)]}\\ngrid_search = GridSearchCV(\\n    model,\\n    param_grid,\\n    scoring=make_scorer(log_loss, greater_is_better=False),\\n    return_train_score=True,\\n)\\ngrid_search.fit(X_train, y_train)\\ndisplay(pd.DataFrame(grid_search.cv_results_))\\nprint(grid_search.best_estimator_.default_parameters[\\\"p\\\"])\";\n                var nbb_formatted_code = \"model = BayesianLogisticRegression()\\nparam_grid = {\\\"default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 3, 25)]}\\ngrid_search = GridSearchCV(\\n    model,\\n    param_grid,\\n    scoring=make_scorer(log_loss, greater_is_better=False),\\n    return_train_score=True,\\n)\\ngrid_search.fit(X_train, y_train)\\ndisplay(pd.DataFrame(grid_search.cv_results_))\\nprint(grid_search.best_estimator_.default_parameters[\\\"p\\\"])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BayesianLogisticRegression()\n",
    "param_grid = {\"default_parameters\": [{\"m\": 0, \"p\": p} for p in np.logspace(-3, 3, 25)]}\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(log_loss, greater_is_better=False),\n",
    "    return_train_score=True,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "display(pd.DataFrame(grid_search.cv_results_))\n",
    "print(grid_search.best_estimator_.default_parameters[\"p\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d9c0c",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e12ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3897214331971086\n",
      "0.83\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 47;\n                var nbb_unformatted_code = \"y_pred = grid_search.best_estimator_.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_formatted_code = \"y_pred = grid_search.best_estimator_.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e5324",
   "metadata": {},
   "source": [
    "### Bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fc8e5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 48;\n                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "def loss(log_sigma2, X, y):\n",
    "    n_feat = X.shape[1]\n",
    "    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\n",
    "    diag_sigma2 = np.exp(log_sigma2)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=BayesianLogisticRegression()._jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = loss(xk, X, y)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = loss(xk, X, y)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb1653",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02a0bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([1.])=61.060407669288374\n",
      "fun([1.43303518])=60.64547121441359\n",
      "fun([1.50461444])=60.63622982158609\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 60.63622982158609\n",
      "        x: [ 1.505e+00]\n",
      "      nit: 3\n",
      "      jac: [-1.711e-06]\n",
      "     nfev: 4\n",
      "     njev: 4\n",
      " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
      "[0.22210291]\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 49;\n                var nbb_unformatted_code = \"res = minimize(\\n    loss,\\n    np.array([0.0]),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n                var nbb_formatted_code = \"res = minimize(\\n    loss,\\n    np.array([0.0]),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    loss,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        X_train.to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac,\n",
    "    callback=callback,\n",
    ")\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8ba6f",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99c547a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                               &#x27;p&#x27;: array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianLogisticRegression</label><div class=\"sk-toggleable__content\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                               &#x27;p&#x27;: array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianLogisticRegression(default_parameters={'m': 0,\n",
       "                                               'p': array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method='L-BFGS-B',\n",
       "                           prior_parameters={})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 50;\n                var nbb_unformatted_code = \"model = BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1 / np.exp(res.x)})\\nmodel.fit(X_train, y_train)\";\n                var nbb_formatted_code = \"model = BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1 / np.exp(res.x)})\\nmodel.fit(X_train, y_train)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BayesianLogisticRegression(default_parameters={\"m\": 0, \"p\": 1 / np.exp(res.x)})\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5823562",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "708cb5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.382668628864105\n",
      "0.8366666666666667\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 51;\n                var nbb_unformatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_formatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769d024",
   "metadata": {},
   "source": [
    "## Multiple sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b99d86de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 52;\n                var nbb_unformatted_code = \"cats = list(X.columns)\";\n                var nbb_formatted_code = \"cats = list(X.columns)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cats = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc63f684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 53;\n                var nbb_unformatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\";\n                var nbb_formatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_id = get_col_to_id(X_train.columns, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d024e1e",
   "metadata": {},
   "source": [
    "### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e9e2b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 54;\n                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y, cols, col_to_id):\\n    n_feat = X.shape[1]\\n    sigma2_list = []\\n    for col in cols:\\n        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\\n    diag_sigma2 = np.array(sigma2_list)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y, cols, col_to_id):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y, cols, col_to_id)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y, cols, col_to_id)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y, cols, col_to_id):\\n    n_feat = X.shape[1]\\n    sigma2_list = []\\n    for col in cols:\\n        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\\n    diag_sigma2 = np.array(sigma2_list)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y, cols, col_to_id):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y, cols, col_to_id)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y, cols, col_to_id)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "def loss(log_sigma2, X, y, cols, col_to_id):\n",
    "    n_feat = X.shape[1]\n",
    "    sigma2_list = []\n",
    "    for col in cols:\n",
    "        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\n",
    "    diag_sigma2 = np.array(sigma2_list)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=BayesianLogisticRegression()._jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y, cols, col_to_id):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = loss(xk, X, y, cols, col_to_id)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = loss(xk, X, y, cols, col_to_id)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b16238",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce721f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([-0.12615174 -0.02922013  0.99158052])=59.20723001635101\n",
      "fun([-0.65596357 -0.09487296  2.58736172])=55.51144842183872\n",
      "fun([-0.92151598 -0.04330354  2.7817241 ])=55.354216890488715\n",
      "fun([-1.45481759  0.00776873  2.92284706])=55.22032802613284\n",
      "fun([-1.91015232 -0.0504136   2.87268155])=55.15229109268441\n",
      "fun([-2.32653384  0.08854265  2.77926044])=55.127431462018016\n",
      "fun([-2.83233246 -0.01662156  2.74486042])=55.10404821297138\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 55.10404821297138\n",
      "        x: [-2.832e+00 -1.662e-02  2.745e+00]\n",
      "      nit: 7\n",
      "      jac: [ 2.489e-06  5.180e-06 -3.672e-06]\n",
      "     nfev: 11\n",
      "     njev: 11\n",
      " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
      "[16.98503158  1.01676047  0.06425727]\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 55;\n                var nbb_unformatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\\nres = minimize(\\n    loss,\\n    np.array([0.0] * len(cats)),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n        X_train.columns,\\n        col_to_id,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n                var nbb_formatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\\nres = minimize(\\n    loss,\\n    np.array([0.0] * len(cats)),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n        X_train.columns,\\n        col_to_id,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_id = get_col_to_id(X_train.columns, cats)\n",
    "res = minimize(\n",
    "    loss,\n",
    "    np.array([0.0] * len(cats)),\n",
    "    args=(\n",
    "        X_train.to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "        X_train.columns,\n",
    "        col_to_id,\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac,\n",
    "    callback=callback,\n",
    ")\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cbcd9",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2ad250f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0, &#x27;p&#x27;: 5},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={&#x27;col1_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985...\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col3_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562}})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianLogisticRegression</label><div class=\"sk-toggleable__content\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0, &#x27;p&#x27;: 5},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={&#x27;col1_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985...\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col3_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562}})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianLogisticRegression(default_parameters={'m': 0, 'p': 5},\n",
       "                           optimize_kwargs={}, optimize_method='L-BFGS-B',\n",
       "                           prior_parameters={'col1_a': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_b': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_c': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_d': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_e': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_f': {'m': 0,\n",
       "                                                        'p': 16.985...\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col2_e': {'m': 0,\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col2_f': {'m': 0,\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col3_a': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_b': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_c': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_d': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_e': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_f': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 56;\n                var nbb_unformatted_code = \"prior_parameters = {}\\n\\nfor col in X_train.columns:\\n    prior_parameters[col] = {\\\"m\\\": 0, \\\"p\\\": (1 / np.exp(res.x))[col_to_id[col]]}\\n\\nmodel = BayesianLogisticRegression(prior_parameters=prior_parameters)\\nmodel.fit(X_train, y_train)\";\n                var nbb_formatted_code = \"prior_parameters = {}\\n\\nfor col in X_train.columns:\\n    prior_parameters[col] = {\\\"m\\\": 0, \\\"p\\\": (1 / np.exp(res.x))[col_to_id[col]]}\\n\\nmodel = BayesianLogisticRegression(prior_parameters=prior_parameters)\\nmodel.fit(X_train, y_train)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prior_parameters = {}\n",
    "\n",
    "for col in X_train.columns:\n",
    "    prior_parameters[col] = {\"m\": 0, \"p\": (1 / np.exp(res.x))[col_to_id[col]]}\n",
    "\n",
    "model = BayesianLogisticRegression(prior_parameters=prior_parameters)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c5079",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a067a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2996493214889956\n",
      "0.8833333333333333\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 57;\n                var nbb_unformatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_formatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bay_opt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
