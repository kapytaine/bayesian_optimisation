{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3911da83",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Packages</a></span></li><li><span><a href=\"#Doc\" data-toc-modified-id=\"Doc-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Doc</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayesian-model\" data-toc-modified-id=\"Bayesian-model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Bayesian model</a></span></li><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Optimization</a></span></li></ul></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Link\" data-toc-modified-id=\"Link-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Link</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#Logit\" data-toc-modified-id=\"Logit-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Logit</a></span></li></ul></li><li><span><a href=\"#Bayesian-GLM\" data-toc-modified-id=\"Bayesian-GLM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Bayesian GLM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#BayesianLogisticRegression\" data-toc-modified-id=\"BayesianLogisticRegression-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>BayesianLogisticRegression</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"#Miscellenaous\" data-toc-modified-id=\"Miscellenaous-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Miscellenaous</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read\" data-toc-modified-id=\"Read-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Read</a></span></li><li><span><a href=\"#Split-train-test\" data-toc-modified-id=\"Split-train-test-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Split train test</a></span></li></ul></li><li><span><a href=\"#Studies\" data-toc-modified-id=\"Studies-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Studies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-sigma2\" data-toc-modified-id=\"Single-sigma2-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Single sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-model\" data-toc-modified-id=\"Naive-model-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Naive model</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.1.2.1\"><span class=\"toc-item-num\">5.1.2.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.1.2.2\"><span class=\"toc-item-num\">5.1.2.2&nbsp;&nbsp;</span>Test</a></span></li></ul></li><li><span><a href=\"#Bayesian-optimisation\" data-toc-modified-id=\"Bayesian-optimisation-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Bayesian optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.1.3.1\"><span class=\"toc-item-num\">5.1.3.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-5.1.3.2\"><span class=\"toc-item-num\">5.1.3.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.1.3.3\"><span class=\"toc-item-num\">5.1.3.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li><li><span><a href=\"#Multiple-sigma2\" data-toc-modified-id=\"Multiple-sigma2-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Multiple sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayesian-optimization\" data-toc-modified-id=\"Bayesian-optimization-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Bayesian optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.2.1.1\"><span class=\"toc-item-num\">5.2.1.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-5.2.1.2\"><span class=\"toc-item-num\">5.2.1.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.2.1.3\"><span class=\"toc-item-num\">5.2.1.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95e967",
   "metadata": {},
   "source": [
    "___\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8298c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import itertools\n",
    "import logging\n",
    "from typing import Any, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit, logit\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, make_scorer\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression\n",
    "from scipy.optimize import approx_fprime, minimize, OptimizeResult\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal, gamma, norm\n",
    "\n",
    "from module.bayesian_model import BayesianLogisticRegression, BayesianNormalRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8b504",
   "metadata": {},
   "source": [
    "___\n",
    "# Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00b1b9",
   "metadata": {},
   "source": [
    "## Bayesian model\n",
    "The purpose of bayesian optimization is to model the parameters as random variables. For instance, in the use case that I am going to investigate, a binary classification, we can model the law of the dependent variable $Y_{i}$ conditioned on the parameters $\\boldsymbol{\\Theta}$ as a Bernoulli random variable depending on observed independent variables $\\boldsymbol{X_{i}}$ and the parameters as a multivariate normal random variable with a diagonal covariance matrix (parameters of $\\boldsymbol{\\sigma^{2}}$, which is a vector in my notation, may differ). Vectors are written in bold format in the following equations and random variable in uppercase.\n",
    "$$\n",
    "\\begin{align} \n",
    "Y_{i} \\mid \\boldsymbol{\\Theta} & \\sim \\text{Bern}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta}) \\\\\n",
    "\\boldsymbol{\\Theta} & \\sim \\mathcal{N}(0,\\boldsymbol{\\sigma^{2}}I)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It is possible to get one step further by modelling the diagonal elements of the covariance matrix, $\\boldsymbol{\\sigma^{2}}$, as a random variable as well, $\\boldsymbol{\\Sigma^{2}}$, with $Y_{i}$ and $\\boldsymbol{\\sigma^{2}}$ conditionally independent given $\\boldsymbol{\\Theta}$:\n",
    "$$\n",
    "\\begin{align} \n",
    "Y_{i} \\mid \\boldsymbol{\\Theta} & \\sim \\text{Bern}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta}) \\\\\n",
    "\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma} & \\sim \\mathcal{N}(0,\\boldsymbol{\\Sigma^{2}}I) \\\\\n",
    "\\Sigma_{i} & \\sim \\Gamma(\\alpha_{i}, \\beta_{i})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Optimization\n",
    "I am going to focus only on the 2nd model as it is more general. The purpose is to maximize $f_{\\Sigma \\mid \\boldsymbol{Y}}(\\sigma \\mid \\boldsymbol{y})$, $\\boldsymbol{Y}$ being the vectors of all the $Y_{i}$ random variables. Unfortunately, there is no closed form for the law of $Y_{i} \\mid \\boldsymbol{\\Sigma}$ contrary to the normal linear case (when $Y_{i} \\mid \\boldsymbol{\\Theta} \\sim \\mathcal{N}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta},\\sigma^{2}I)$). That's being said, it is possible to approximate the density function of $\\Sigma \\mid \\boldsymbol{Y}$ thanks to the conditional independence and Bayes theorem:\n",
    "$$\n",
    "\\begin{align} \n",
    "f_{\\boldsymbol{\\Sigma} \\mid Y_{i}}(\\boldsymbol{\\sigma} \\mid y_{i})\n",
    "& = \\frac{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} \\mid \\boldsymbol{\\Sigma} = \\sigma ) f_{\\Sigma}(\\sigma)\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\\\\n",
    "& = \\frac{ \n",
    "    f_{Y_{i}, \\boldsymbol{\\Theta}, \\boldsymbol{\\Sigma}}(y_{i}, \\boldsymbol{\\theta}, \\sigma)\n",
    "}{\n",
    "    f_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta} \\mid y_{i}, \\boldsymbol{\\sigma})\n",
    "}\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} \\mid \\boldsymbol{\\Theta} = \\theta )\n",
    "    f_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\sigma})\n",
    "    f_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\sigma})\n",
    "}{\n",
    "    f_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\sigma}}(\\boldsymbol{\\theta} \\mid y_{i}, \\boldsymbol{\\sigma})\n",
    "}\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We would like to find $\\boldsymbol{\\sigma}$ that maximize the previous density function given the observations $\\boldsymbol{y}$. The numerator can be evaluated provided that we know $\\boldsymbol{\\theta}$. The second element of the denominator does not depend on $\\boldsymbol{\\sigma}$ so we can discard it when trying to maximize our objective function. Finally, the first element of the denominator is unknown. This is why we need to approximate it by means of Laplace approximation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\boldsymbol{\\sigma} ; \\boldsymbol{y})\n",
    "&= \\sum \n",
    "    log\\left(\\mathbb{P}\\left( Y_{i} = y_{i} \\mid \\boldsymbol{\\Theta} = \\theta^{*} \\right)\\right)\n",
    "    + log(f_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta^{*}} \\mid \\boldsymbol{\\sigma}))\n",
    "    + log(f_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\sigma}))\n",
    "    - log(\\tilde{f}_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\sigma}}(\\boldsymbol{\\theta^{*}} \\mid y_{i}, \\boldsymbol{\\sigma}))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a63f4e",
   "metadata": {},
   "source": [
    "___\n",
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beca00",
   "metadata": {},
   "source": [
    "## adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fa5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(\n",
    "    fun,\n",
    "    x0,\n",
    "    jac,\n",
    "    args=(),\n",
    "    learning_rate=0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    eps=1e-8,\n",
    "    startiter=0,\n",
    "    maxiter=1000,\n",
    "    callback=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"``scipy.optimize.minimize`` compatible implementation of ADAM -\n",
    "    [http://arxiv.org/pdf/1412.6980.pdf].\n",
    "    Adapted from ``autograd/misc/optimizers.py``.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "\n",
    "    for i in range(startiter, startiter + maxiter):\n",
    "        g = jac(x, *args)\n",
    "\n",
    "        if callback and callback(x):\n",
    "            break\n",
    "\n",
    "        m = (1 - beta1) * g + beta1 * m  # first  moment estimate.\n",
    "        v = (1 - beta2) * (g**2) + beta2 * v  # second moment estimate.\n",
    "        mhat = m / (1 - beta1**(i + 1))  # bias correction.\n",
    "        vhat = v / (1 - beta2**(i + 1))\n",
    "        x = x - learning_rate * mhat / (np.sqrt(vhat) + eps)\n",
    "\n",
    "    i += 1\n",
    "    return OptimizeResult(x=x, fun=fun(x, *args), jac=g, nit=i, nfev=i, success=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32c426",
   "metadata": {},
   "source": [
    "## loss and jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "399ed157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_single_sigma2(X, y, baysian_model, prior_dist_logpdf=None):\n",
    "    def callback(intermediate_result):\n",
    "        print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "    def loss(log_sigma2, X, y, baysian_model, prior_dist_logpdf = None):\n",
    "        n_feat = X.shape[1]\n",
    "        single_p = np.exp(-log_sigma2[0])\n",
    "        p = np.array([single_p] * n_feat)\n",
    "        m = np.array([0] * n_feat)\n",
    "\n",
    "        res = minimize(\n",
    "            baysian_model._loss,\n",
    "            np.array([0] * n_feat),\n",
    "            args=(X, y, m, p),\n",
    "            method=\"BFGS\",\n",
    "            jac=baysian_model._jac,\n",
    "        )\n",
    "\n",
    "        theta_star = res.x\n",
    "\n",
    "        H = baysian_model._hess(theta_star, X, y, m, p)\n",
    "\n",
    "        out = (\n",
    "            baysian_model._loss(theta_star, X, y, m, p)\n",
    "            - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "            + 0.5 * np.linalg.slogdet(H)[1]\n",
    "        )\n",
    "\n",
    "        if prior_dist_logpdf:\n",
    "            out += - prior_dist_logpdf(single_p)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def jac(log_sigma2, X, y, baysian_model, prior_dist_logpdf = None):\n",
    "        h = np.log(1 + 1e-2)\n",
    "        jac_list = []\n",
    "        for ii in range(len(log_sigma2)):\n",
    "            xk = np.copy(log_sigma2)\n",
    "            xk[ii] += h\n",
    "            fk_plus_h = loss(xk, X, y, baysian_model, prior_dist_logpdf)\n",
    "            xk[ii] -= 2 * h\n",
    "            fk_minus_h = loss(xk, X, y, baysian_model, prior_dist_logpdf)\n",
    "            jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "        return np.array(jac_list)\n",
    "\n",
    "    return minimize(\n",
    "        loss,\n",
    "        np.array([0.0]),\n",
    "        args=(\n",
    "            X.to_numpy(),\n",
    "            y.to_numpy(),\n",
    "            baysian_model,\n",
    "            prior_dist_logpdf\n",
    "        ),\n",
    "        method=\"BFGS\",\n",
    "        jac=jac,\n",
    "        callback=callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4cc92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_to_id(cols, cats):\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        for ii, cat in enumerate(cats):\n",
    "            if isinstance(cat, str):\n",
    "                if col.startswith(cat):\n",
    "                    out[col] = ii\n",
    "            else:\n",
    "                for cc in cat:\n",
    "                    if cc in col:\n",
    "                        out[col] = ii\n",
    "    return out\n",
    "\n",
    "def solve_multiple_sigma2(X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "    def callback(intermediate_result):\n",
    "        print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "    def loss(log_sigma2, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "        n_feat = X.shape[1]\n",
    "        sigma2_list = []\n",
    "        for col in cols:\n",
    "            sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\n",
    "        diag_sigma2 = np.array(sigma2_list)\n",
    "\n",
    "        m = np.array([0] * n_feat)\n",
    "        p = 1 / diag_sigma2\n",
    "\n",
    "        res = minimize(\n",
    "            baysian_model._loss,\n",
    "            np.array([0] * n_feat),\n",
    "            args=(X, y, m, p),\n",
    "            method=\"BFGS\",\n",
    "            jac=baysian_model._jac,\n",
    "        )\n",
    "\n",
    "        theta_star = res.x\n",
    "\n",
    "        H = baysian_model._hess(theta_star, X, y, m, p)\n",
    "\n",
    "        out = (\n",
    "            baysian_model._loss(theta_star, X, y, m, p)\n",
    "            - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "            + 0.5 * np.linalg.slogdet(H)[1]\n",
    "        )\n",
    "\n",
    "        if prior_dist_logpdf:\n",
    "            out += - np.sum(prior_dist_logpdf(p))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def jac(log_sigma2, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "        h = np.log(1 + 1e-2)\n",
    "        jac_list = []\n",
    "        for ii in range(len(log_sigma2)):\n",
    "            xk = np.copy(log_sigma2)\n",
    "            xk[ii] += h\n",
    "            fk_plus_h = loss(xk, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf)\n",
    "            xk[ii] -= 2 * h\n",
    "            fk_minus_h = loss(xk, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf)\n",
    "            jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "        return np.array(jac_list)\n",
    "    \n",
    "    return adam(\n",
    "        loss,\n",
    "        np.array([0.0]*len(set(col_to_id.values()))),\n",
    "        jac,\n",
    "        args=(\n",
    "            X.to_numpy(),\n",
    "            y.to_numpy(),\n",
    "            cols,\n",
    "            col_to_id,\n",
    "            baysian_model,\n",
    "            prior_dist_logpdf\n",
    "        ),\n",
    "        learning_rate=0.2,\n",
    "        maxiter=2000\n",
    "    )\n",
    "\n",
    "    return minimize(\n",
    "        loss,\n",
    "        np.array([0.0]*len(set(col_to_id.values()))),\n",
    "        args=(\n",
    "            X.to_numpy(),\n",
    "            y.to_numpy(),\n",
    "            cols,\n",
    "            col_to_id,\n",
    "            baysian_model,\n",
    "            prior_dist_logpdf\n",
    "        ),\n",
    "        method=\"BFGS\",\n",
    "        jac=jac,\n",
    "        # callback=callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f72ff",
   "metadata": {},
   "source": [
    "___\n",
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf333b60",
   "metadata": {},
   "source": [
    "## Create artificial dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031edb61",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab81624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_dataset():\n",
    "    n_samples = 200\n",
    "    cats = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "    X = pd.DataFrame(\n",
    "        data={\n",
    "            \"col1\": np.random.choice(cats, size=n_samples),\n",
    "            \"col2\": np.random.choice(cats, size=n_samples),\n",
    "            \"col3\": np.random.choice(cats, size=n_samples),\n",
    "        }\n",
    "    )\n",
    "    X_preprocessed = pd.get_dummies(X)\n",
    "\n",
    "    theta = np.random.multivariate_normal(\n",
    "        np.zeros(len(cats) * X.shape[1]),\n",
    "        np.diag(np.array([1e-1] * len(cats) + [1] * len(cats) + [1e1] * len(cats))),\n",
    "    )\n",
    "\n",
    "    y = pd.Series(\n",
    "        data=np.random.binomial(1, expit(np.dot(X_preprocessed.to_numpy(), theta))),\n",
    "        index=X_preprocessed.index,\n",
    "    )\n",
    "    return X_preprocessed, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1971cfb",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4aed85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_dataset():\n",
    "    n_samples = 200\n",
    "    sigma2 = 3\n",
    "    X = pd.DataFrame(\n",
    "        data={\n",
    "            \"col1\": np.random.normal(size=n_samples),\n",
    "            \"col2\": np.random.normal(size=n_samples),\n",
    "            \"col3\": np.random.normal(size=n_samples),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    theta = np.random.multivariate_normal(\n",
    "        np.zeros(X.shape[1]),\n",
    "        cov=np.eye(X.shape[1]),\n",
    "    )\n",
    "\n",
    "    y = pd.Series(\n",
    "        data=np.random.multivariate_normal(np.dot(X.to_numpy(), theta), sigma2*np.eye(n_samples)),\n",
    "        index=X.index,\n",
    "    )\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfce23",
   "metadata": {},
   "source": [
    "___\n",
    "# Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166fdea",
   "metadata": {},
   "source": [
    "## Linear case vs scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79b7b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_regression_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da648472",
   "metadata": {},
   "source": [
    "### Without prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b64c0",
   "metadata": {},
   "source": [
    "#### Single sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61402275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([0.12174703])=330.8328804142951\n",
      "fun([0.77667171])=329.61532615263354\n",
      "fun([1.05115124])=329.45021365000724\n",
      "fun([1.21081429])=329.41935087316045\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 329.41935087316045\n",
      "        x: [ 1.211e+00]\n",
      "      nit: 4\n",
      "      jac: [-6.268e-06]\n",
      " hess_inv: [[ 4.883e+03]]\n",
      "     nfev: 9\n",
      "     njev: 9\n",
      "[0.29795456]\n"
     ]
    }
   ],
   "source": [
    "res = solve_single_sigma2(X_train, y_train, BayesianNormalRegression())\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d50362b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857252370376939\n",
      "0.3060726009289764\n"
     ]
    }
   ],
   "source": [
    "model = BayesianRidge(fit_intercept=False, lambda_1=1e-6, lambda_2=1e-6, alpha_1=1e-6, alpha_2=1e-6)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.lambda_)\n",
    "print(model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236945b1",
   "metadata": {},
   "source": [
    "#### Multiple sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8382d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " success: True\n",
      "     fun: 328.98166366399823\n",
      "       x: [ 8.616e-01  5.335e-02  1.957e+00]\n",
      "     nit: 2000\n",
      "     jac: [-3.272e-12  8.776e-09  8.117e-12]\n",
      "    nfev: 2000\n",
      "[0.42250251 0.94805066 0.14123691]\n"
     ]
    }
   ],
   "source": [
    "col_to_id = {col: idx for idx, col in enumerate(X_train.columns)}\n",
    "res = solve_multiple_sigma2(X_train, y_train, X_train.columns, col_to_id, BayesianNormalRegression())\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7ebf6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4224326 , 0.94777537, 0.14123189])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ARDRegression(fit_intercept=False, lambda_1=1e-6, lambda_2=1e-6, alpha_1=1e-6, alpha_2=1e-6)\n",
    "model.fit(X_train, y_train)\n",
    "model.lambda_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0074bf9",
   "metadata": {},
   "source": [
    "### With prior\n",
    "The Laplace approximation is now False..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "327bef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_p = 2\n",
    "var = 4\n",
    "b = mu_p / var\n",
    "a = mu_p * b\n",
    "\n",
    "def prior_dist_logpdf(x):\n",
    "    return gamma.logpdf(x, a, loc=0, scale=1/b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60708901",
   "metadata": {},
   "source": [
    "#### Single sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d901ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([0.20654455])=367.058287471299\n",
      "fun([1.02506896])=357.3798578622874\n",
      "fun([1.34120722])=356.22097675245107\n",
      "fun([1.52783682])=355.99843543084984\n",
      "fun([1.57418763])=355.988218528898\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 355.988218528898\n",
      "        x: [ 1.574e+00]\n",
      "      nit: 5\n",
      "      jac: [-3.997e-06]\n",
      " hess_inv: [[ 1.161e+03]]\n",
      "     nfev: 9\n",
      "     njev: 9\n",
      "[0.20717579]\n"
     ]
    }
   ],
   "source": [
    "res = solve_single_sigma2(X_train, y_train, BayesianNormalRegression(), prior_dist_logpdf=prior_dist_logpdf)\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcb2abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23276575806556768\n",
      "0.29255481740490347\n"
     ]
    }
   ],
   "source": [
    "model = BayesianRidge(fit_intercept=False, lambda_1=a, lambda_2=b, alpha_1=1e-6, alpha_2=1e-6)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.lambda_)\n",
    "print(model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f7c27",
   "metadata": {},
   "source": [
    "#### Multiple sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5d60a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " success: True\n",
      "     fun: 363.5315247081752\n",
      "       x: [ 4.706e-01  3.706e-01 ...  3.869e+00  1.868e+00]\n",
      "     nit: 2000\n",
      "     jac: [ 1.299e-10  2.477e-10 ... -5.014e-09  8.769e-09]\n",
      "    nfev: 2000\n",
      "[0.62461863 0.69034597 0.77016686 0.74600738 0.57203243 0.75982663\n",
      " 0.29185964 0.73572376 0.5408608  0.27222476 0.74439525 0.71127027\n",
      " 0.18566508 0.10946436 0.7156498  0.10613942 0.02088191 0.15450772]\n"
     ]
    }
   ],
   "source": [
    "col_to_id = {col: idx for idx, col in enumerate(X_train.columns)}\n",
    "res = solve_multiple_sigma2(X_train, y_train, X_train.columns, col_to_id, BayesianNormalRegression(), prior_dist_logpdf=prior_dist_logpdf)\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367f5ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.1735877 , 2.36389791, 2.63702173, 2.52148513, 1.95091468,\n",
       "       2.58215885, 1.04212236, 2.55951982, 1.94131518, 0.93648339,\n",
       "       2.59258687, 2.47034478, 0.61780362, 0.33854574, 2.5857624 ,\n",
       "       0.33506046, 0.06491804, 0.49007262])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ARDRegression(fit_intercept=False, lambda_1=a, lambda_2=b, alpha_1=1e-6, alpha_2=1e-6)\n",
    "model.fit(X_train, y_train)\n",
    "model.lambda_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89608f9e",
   "metadata": {},
   "source": [
    "## Classification case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e478a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fadca95e",
   "metadata": {},
   "source": [
    "### Single sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "916dd7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([0.08553351])=332.11695278671453\n",
      "fun([0.71763203])=328.873167764825\n",
      "fun([0.88103372])=328.6857385514037\n",
      "fun([0.93960688])=328.670400793988\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 328.670400793988\n",
      "        x: [ 9.396e-01]\n",
      "      nit: 4\n",
      "      jac: [-4.186e-06]\n",
      " hess_inv: [[ 1.219e+03]]\n",
      "     nfev: 8\n",
      "     njev: 8\n",
      "[0.39078143]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33e94f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44072012115786874\n",
      "0.39773091276625355\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e400bbf",
   "metadata": {},
   "source": [
    "## Single sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422bdfc",
   "metadata": {},
   "source": [
    "### Naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4580a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5623351446188083\n",
      "0.5\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 45;\n                var nbb_unformatted_code = \"y_pred = np.array([np.mean(y_train)] * len(y_test))\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_formatted_code = \"y_pred = np.array([np.mean(y_train)] * len(y_test))\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = np.array([np.mean(y_train)] * len(y_test))\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab81dd",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616d7ea",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0b57a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_default_parameters</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031313</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>{'m': 0, 'p': 0.001}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.001}}</td>\n",
       "      <td>-0.311680</td>\n",
       "      <td>-0.432727</td>\n",
       "      <td>-0.624367</td>\n",
       "      <td>-0.275618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377357</td>\n",
       "      <td>0.139255</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.189113</td>\n",
       "      <td>-0.190769</td>\n",
       "      <td>-0.169377</td>\n",
       "      <td>-0.200170</td>\n",
       "      <td>-0.203862</td>\n",
       "      <td>-0.190658</td>\n",
       "      <td>0.012001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027904</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>{'m': 0, 'p': 0.0017782794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0017782...</td>\n",
       "      <td>-0.312519</td>\n",
       "      <td>-0.416230</td>\n",
       "      <td>-0.594905</td>\n",
       "      <td>-0.275720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368316</td>\n",
       "      <td>0.127459</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.189337</td>\n",
       "      <td>-0.191047</td>\n",
       "      <td>-0.169708</td>\n",
       "      <td>-0.200395</td>\n",
       "      <td>-0.204093</td>\n",
       "      <td>-0.190916</td>\n",
       "      <td>0.011964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026473</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>{'m': 0, 'p': 0.0031622776601683794}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0031622...</td>\n",
       "      <td>-0.313839</td>\n",
       "      <td>-0.400069</td>\n",
       "      <td>-0.565815</td>\n",
       "      <td>-0.275980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359529</td>\n",
       "      <td>0.115838</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.189701</td>\n",
       "      <td>-0.191501</td>\n",
       "      <td>-0.170236</td>\n",
       "      <td>-0.200761</td>\n",
       "      <td>-0.204472</td>\n",
       "      <td>-0.191334</td>\n",
       "      <td>0.011910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024158</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>{'m': 0, 'p': 0.005623413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0056234...</td>\n",
       "      <td>-0.315778</td>\n",
       "      <td>-0.383827</td>\n",
       "      <td>-0.537204</td>\n",
       "      <td>-0.276387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350936</td>\n",
       "      <td>0.104452</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.190292</td>\n",
       "      <td>-0.192236</td>\n",
       "      <td>-0.171084</td>\n",
       "      <td>-0.201352</td>\n",
       "      <td>-0.205086</td>\n",
       "      <td>-0.192010</td>\n",
       "      <td>0.011825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>{'m': 0, 'p': 0.01}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.01}}</td>\n",
       "      <td>-0.318500</td>\n",
       "      <td>-0.367465</td>\n",
       "      <td>-0.508722</td>\n",
       "      <td>-0.277033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342501</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.191247</td>\n",
       "      <td>-0.193426</td>\n",
       "      <td>-0.172432</td>\n",
       "      <td>-0.202309</td>\n",
       "      <td>-0.206084</td>\n",
       "      <td>-0.193100</td>\n",
       "      <td>0.011697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>{'m': 0, 'p': 0.01778279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0177827...</td>\n",
       "      <td>-0.322193</td>\n",
       "      <td>-0.351113</td>\n",
       "      <td>-0.480661</td>\n",
       "      <td>-0.278165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334384</td>\n",
       "      <td>0.082382</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.192780</td>\n",
       "      <td>-0.195322</td>\n",
       "      <td>-0.174564</td>\n",
       "      <td>-0.203833</td>\n",
       "      <td>-0.207694</td>\n",
       "      <td>-0.194839</td>\n",
       "      <td>0.011504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>{'m': 0, 'p': 0.03162277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0316227...</td>\n",
       "      <td>-0.326720</td>\n",
       "      <td>-0.334859</td>\n",
       "      <td>-0.453479</td>\n",
       "      <td>-0.280108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326743</td>\n",
       "      <td>0.072223</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.195209</td>\n",
       "      <td>-0.198308</td>\n",
       "      <td>-0.177902</td>\n",
       "      <td>-0.206232</td>\n",
       "      <td>-0.210266</td>\n",
       "      <td>-0.197583</td>\n",
       "      <td>0.011217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.014102</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>{'m': 0, 'p': 0.056234132519034905}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0562341...</td>\n",
       "      <td>-0.331744</td>\n",
       "      <td>-0.319195</td>\n",
       "      <td>-0.428453</td>\n",
       "      <td>-0.283286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320023</td>\n",
       "      <td>0.063349</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.198977</td>\n",
       "      <td>-0.202914</td>\n",
       "      <td>-0.183007</td>\n",
       "      <td>-0.209928</td>\n",
       "      <td>-0.214304</td>\n",
       "      <td>-0.201826</td>\n",
       "      <td>0.010815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016554</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>{'m': 0, 'p': 0.09999999999999999}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.0999999...</td>\n",
       "      <td>-0.336485</td>\n",
       "      <td>-0.304739</td>\n",
       "      <td>-0.407270</td>\n",
       "      <td>-0.288190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314785</td>\n",
       "      <td>0.056295</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.204686</td>\n",
       "      <td>-0.209814</td>\n",
       "      <td>-0.190532</td>\n",
       "      <td>-0.215465</td>\n",
       "      <td>-0.220488</td>\n",
       "      <td>-0.208197</td>\n",
       "      <td>0.010304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>{'m': 0, 'p': 0.1778279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.1778279...</td>\n",
       "      <td>-0.340192</td>\n",
       "      <td>-0.292514</td>\n",
       "      <td>-0.392050</td>\n",
       "      <td>-0.295155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311855</td>\n",
       "      <td>0.051269</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.213149</td>\n",
       "      <td>-0.219822</td>\n",
       "      <td>-0.201120</td>\n",
       "      <td>-0.223533</td>\n",
       "      <td>-0.229618</td>\n",
       "      <td>-0.217448</td>\n",
       "      <td>0.009756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>{'m': 0, 'p': 0.31622776601683794}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.3162277...</td>\n",
       "      <td>-0.343172</td>\n",
       "      <td>-0.283906</td>\n",
       "      <td>-0.384348</td>\n",
       "      <td>-0.304565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312331</td>\n",
       "      <td>0.047855</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.225453</td>\n",
       "      <td>-0.233898</td>\n",
       "      <td>-0.215366</td>\n",
       "      <td>-0.235032</td>\n",
       "      <td>-0.242562</td>\n",
       "      <td>-0.230462</td>\n",
       "      <td>0.009296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>{'m': 0, 'p': 0.5623413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 0.5623413...</td>\n",
       "      <td>-0.347524</td>\n",
       "      <td>-0.280987</td>\n",
       "      <td>-0.384889</td>\n",
       "      <td>-0.317184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317767</td>\n",
       "      <td>0.045357</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.242944</td>\n",
       "      <td>-0.253189</td>\n",
       "      <td>-0.233969</td>\n",
       "      <td>-0.251172</td>\n",
       "      <td>-0.260270</td>\n",
       "      <td>-0.248309</td>\n",
       "      <td>0.009050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>{'m': 0, 'p': 1.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1.0}}</td>\n",
       "      <td>-0.356604</td>\n",
       "      <td>-0.286597</td>\n",
       "      <td>-0.393833</td>\n",
       "      <td>-0.334331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330119</td>\n",
       "      <td>0.043041</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.267094</td>\n",
       "      <td>-0.279034</td>\n",
       "      <td>-0.258058</td>\n",
       "      <td>-0.273492</td>\n",
       "      <td>-0.283907</td>\n",
       "      <td>-0.272317</td>\n",
       "      <td>0.009071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008785</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>{'m': 0, 'p': 1.7782794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1.7782794...</td>\n",
       "      <td>-0.373814</td>\n",
       "      <td>-0.303805</td>\n",
       "      <td>-0.411297</td>\n",
       "      <td>-0.357686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351396</td>\n",
       "      <td>0.040194</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.299296</td>\n",
       "      <td>-0.312775</td>\n",
       "      <td>-0.289279</td>\n",
       "      <td>-0.303701</td>\n",
       "      <td>-0.314827</td>\n",
       "      <td>-0.303976</td>\n",
       "      <td>0.009308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>{'m': 0, 'p': 3.162277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 3.1622776...</td>\n",
       "      <td>-0.401304</td>\n",
       "      <td>-0.334826</td>\n",
       "      <td>-0.437415</td>\n",
       "      <td>-0.388691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.382953</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.340451</td>\n",
       "      <td>-0.355258</td>\n",
       "      <td>-0.329256</td>\n",
       "      <td>-0.343193</td>\n",
       "      <td>-0.354085</td>\n",
       "      <td>-0.344449</td>\n",
       "      <td>0.009572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.006890</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>{'m': 0, 'p': 5.62341325190349}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 5.6234132...</td>\n",
       "      <td>-0.438960</td>\n",
       "      <td>-0.379632</td>\n",
       "      <td>-0.471534</td>\n",
       "      <td>-0.427760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.390191</td>\n",
       "      <td>-0.405968</td>\n",
       "      <td>-0.378408</td>\n",
       "      <td>-0.392096</td>\n",
       "      <td>-0.401450</td>\n",
       "      <td>-0.393623</td>\n",
       "      <td>0.009584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>{'m': 0, 'p': 10.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 10.0}}</td>\n",
       "      <td>-0.483969</td>\n",
       "      <td>-0.434967</td>\n",
       "      <td>-0.511277</td>\n",
       "      <td>-0.473538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473051</td>\n",
       "      <td>0.025157</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.446060</td>\n",
       "      <td>-0.462086</td>\n",
       "      <td>-0.434727</td>\n",
       "      <td>-0.448107</td>\n",
       "      <td>-0.454600</td>\n",
       "      <td>-0.449116</td>\n",
       "      <td>0.009118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>{'m': 0, 'p': 17.78279410038923}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 17.782794...</td>\n",
       "      <td>-0.531612</td>\n",
       "      <td>-0.494612</td>\n",
       "      <td>-0.552766</td>\n",
       "      <td>-0.522577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523968</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.503552</td>\n",
       "      <td>-0.518569</td>\n",
       "      <td>-0.493673</td>\n",
       "      <td>-0.506201</td>\n",
       "      <td>-0.509344</td>\n",
       "      <td>-0.506268</td>\n",
       "      <td>0.008085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.006147</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>{'m': 0, 'p': 31.622776601683796}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 31.622776...</td>\n",
       "      <td>-0.576682</td>\n",
       "      <td>-0.551140</td>\n",
       "      <td>-0.591772</td>\n",
       "      <td>-0.569786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571728</td>\n",
       "      <td>0.013115</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.557156</td>\n",
       "      <td>-0.569756</td>\n",
       "      <td>-0.549364</td>\n",
       "      <td>-0.560084</td>\n",
       "      <td>-0.560646</td>\n",
       "      <td>-0.559401</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>{'m': 0, 'p': 56.23413251903491}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 56.234132...</td>\n",
       "      <td>-0.614727</td>\n",
       "      <td>-0.598364</td>\n",
       "      <td>-0.624705</td>\n",
       "      <td>-0.610040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611712</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.601958</td>\n",
       "      <td>-0.611370</td>\n",
       "      <td>-0.596374</td>\n",
       "      <td>-0.604565</td>\n",
       "      <td>-0.603871</td>\n",
       "      <td>-0.603628</td>\n",
       "      <td>0.004822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.005628</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>{'m': 0, 'p': 100.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 100.0}}</td>\n",
       "      <td>-0.643393</td>\n",
       "      <td>-0.633448</td>\n",
       "      <td>-0.649609</td>\n",
       "      <td>-0.640455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641630</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.635466</td>\n",
       "      <td>-0.641835</td>\n",
       "      <td>-0.631778</td>\n",
       "      <td>-0.637421</td>\n",
       "      <td>-0.636462</td>\n",
       "      <td>-0.636592</td>\n",
       "      <td>0.003245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>{'m': 0, 'p': 177.82794100389228}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 177.82794...</td>\n",
       "      <td>-0.662965</td>\n",
       "      <td>-0.657117</td>\n",
       "      <td>-0.666683</td>\n",
       "      <td>-0.661210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.661955</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.658226</td>\n",
       "      <td>-0.662240</td>\n",
       "      <td>-0.655934</td>\n",
       "      <td>-0.659532</td>\n",
       "      <td>-0.658739</td>\n",
       "      <td>-0.658934</td>\n",
       "      <td>0.002041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>{'m': 0, 'p': 316.2277660168379}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 316.22776...</td>\n",
       "      <td>-0.675383</td>\n",
       "      <td>-0.672011</td>\n",
       "      <td>-0.677551</td>\n",
       "      <td>-0.674364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674810</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.672620</td>\n",
       "      <td>-0.675034</td>\n",
       "      <td>-0.671252</td>\n",
       "      <td>-0.673431</td>\n",
       "      <td>-0.672888</td>\n",
       "      <td>-0.673045</td>\n",
       "      <td>0.001227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>{'m': 0, 'p': 562.341325190349}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 562.34132...</td>\n",
       "      <td>-0.682888</td>\n",
       "      <td>-0.680965</td>\n",
       "      <td>-0.684132</td>\n",
       "      <td>-0.682304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.682565</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.681301</td>\n",
       "      <td>-0.682713</td>\n",
       "      <td>-0.680504</td>\n",
       "      <td>-0.681784</td>\n",
       "      <td>-0.681444</td>\n",
       "      <td>-0.681549</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>{'m': 0, 'p': 1000.0}</td>\n",
       "      <td>{'default_parameters': {'m': 0, 'p': 1000.0}}</td>\n",
       "      <td>-0.687289</td>\n",
       "      <td>-0.686199</td>\n",
       "      <td>-0.687997</td>\n",
       "      <td>-0.686957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.687107</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.686386</td>\n",
       "      <td>-0.687198</td>\n",
       "      <td>-0.685929</td>\n",
       "      <td>-0.686666</td>\n",
       "      <td>-0.686464</td>\n",
       "      <td>-0.686528</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.031313      0.002291         0.000839        0.000047   \n",
       "1        0.027904      0.002463         0.000908        0.000059   \n",
       "2        0.026473      0.002976         0.000908        0.000038   \n",
       "3        0.024158      0.003380         0.000918        0.000232   \n",
       "4        0.020368      0.001990         0.000767        0.000016   \n",
       "5        0.018054      0.001623         0.000806        0.000030   \n",
       "6        0.016542      0.000949         0.000795        0.000027   \n",
       "7        0.014102      0.001220         0.000799        0.000043   \n",
       "8        0.016554      0.003581         0.001007        0.000225   \n",
       "9        0.012117      0.000650         0.000840        0.000099   \n",
       "10       0.010755      0.000348         0.000772        0.000020   \n",
       "11       0.010590      0.001100         0.000906        0.000089   \n",
       "12       0.009805      0.000603         0.000890        0.000036   \n",
       "13       0.008785      0.000387         0.000873        0.000024   \n",
       "14       0.008291      0.000209         0.000863        0.000058   \n",
       "15       0.006890      0.000330         0.000817        0.000008   \n",
       "16       0.006114      0.000136         0.000789        0.000007   \n",
       "17       0.006213      0.000301         0.000826        0.000047   \n",
       "18       0.006147      0.000166         0.000807        0.000040   \n",
       "19       0.005532      0.000237         0.000838        0.000071   \n",
       "20       0.005628      0.000574         0.000837        0.000155   \n",
       "21       0.005244      0.000092         0.000781        0.000034   \n",
       "22       0.004629      0.000072         0.000775        0.000010   \n",
       "23       0.004661      0.000172         0.000791        0.000045   \n",
       "24       0.005227      0.000374         0.000854        0.000108   \n",
       "\n",
       "                param_default_parameters  \\\n",
       "0                   {'m': 0, 'p': 0.001}   \n",
       "1   {'m': 0, 'p': 0.0017782794100389228}   \n",
       "2   {'m': 0, 'p': 0.0031622776601683794}   \n",
       "3    {'m': 0, 'p': 0.005623413251903491}   \n",
       "4                    {'m': 0, 'p': 0.01}   \n",
       "5     {'m': 0, 'p': 0.01778279410038923}   \n",
       "6     {'m': 0, 'p': 0.03162277660168379}   \n",
       "7    {'m': 0, 'p': 0.056234132519034905}   \n",
       "8     {'m': 0, 'p': 0.09999999999999999}   \n",
       "9      {'m': 0, 'p': 0.1778279410038923}   \n",
       "10    {'m': 0, 'p': 0.31622776601683794}   \n",
       "11     {'m': 0, 'p': 0.5623413251903491}   \n",
       "12                    {'m': 0, 'p': 1.0}   \n",
       "13     {'m': 0, 'p': 1.7782794100389228}   \n",
       "14      {'m': 0, 'p': 3.162277660168379}   \n",
       "15       {'m': 0, 'p': 5.62341325190349}   \n",
       "16                   {'m': 0, 'p': 10.0}   \n",
       "17      {'m': 0, 'p': 17.78279410038923}   \n",
       "18     {'m': 0, 'p': 31.622776601683796}   \n",
       "19      {'m': 0, 'p': 56.23413251903491}   \n",
       "20                  {'m': 0, 'p': 100.0}   \n",
       "21     {'m': 0, 'p': 177.82794100389228}   \n",
       "22      {'m': 0, 'p': 316.2277660168379}   \n",
       "23       {'m': 0, 'p': 562.341325190349}   \n",
       "24                 {'m': 0, 'p': 1000.0}   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0        {'default_parameters': {'m': 0, 'p': 0.001}}          -0.311680   \n",
       "1   {'default_parameters': {'m': 0, 'p': 0.0017782...          -0.312519   \n",
       "2   {'default_parameters': {'m': 0, 'p': 0.0031622...          -0.313839   \n",
       "3   {'default_parameters': {'m': 0, 'p': 0.0056234...          -0.315778   \n",
       "4         {'default_parameters': {'m': 0, 'p': 0.01}}          -0.318500   \n",
       "5   {'default_parameters': {'m': 0, 'p': 0.0177827...          -0.322193   \n",
       "6   {'default_parameters': {'m': 0, 'p': 0.0316227...          -0.326720   \n",
       "7   {'default_parameters': {'m': 0, 'p': 0.0562341...          -0.331744   \n",
       "8   {'default_parameters': {'m': 0, 'p': 0.0999999...          -0.336485   \n",
       "9   {'default_parameters': {'m': 0, 'p': 0.1778279...          -0.340192   \n",
       "10  {'default_parameters': {'m': 0, 'p': 0.3162277...          -0.343172   \n",
       "11  {'default_parameters': {'m': 0, 'p': 0.5623413...          -0.347524   \n",
       "12         {'default_parameters': {'m': 0, 'p': 1.0}}          -0.356604   \n",
       "13  {'default_parameters': {'m': 0, 'p': 1.7782794...          -0.373814   \n",
       "14  {'default_parameters': {'m': 0, 'p': 3.1622776...          -0.401304   \n",
       "15  {'default_parameters': {'m': 0, 'p': 5.6234132...          -0.438960   \n",
       "16        {'default_parameters': {'m': 0, 'p': 10.0}}          -0.483969   \n",
       "17  {'default_parameters': {'m': 0, 'p': 17.782794...          -0.531612   \n",
       "18  {'default_parameters': {'m': 0, 'p': 31.622776...          -0.576682   \n",
       "19  {'default_parameters': {'m': 0, 'p': 56.234132...          -0.614727   \n",
       "20       {'default_parameters': {'m': 0, 'p': 100.0}}          -0.643393   \n",
       "21  {'default_parameters': {'m': 0, 'p': 177.82794...          -0.662965   \n",
       "22  {'default_parameters': {'m': 0, 'p': 316.22776...          -0.675383   \n",
       "23  {'default_parameters': {'m': 0, 'p': 562.34132...          -0.682888   \n",
       "24      {'default_parameters': {'m': 0, 'p': 1000.0}}          -0.687289   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           -0.432727          -0.624367          -0.275618  ...   \n",
       "1           -0.416230          -0.594905          -0.275720  ...   \n",
       "2           -0.400069          -0.565815          -0.275980  ...   \n",
       "3           -0.383827          -0.537204          -0.276387  ...   \n",
       "4           -0.367465          -0.508722          -0.277033  ...   \n",
       "5           -0.351113          -0.480661          -0.278165  ...   \n",
       "6           -0.334859          -0.453479          -0.280108  ...   \n",
       "7           -0.319195          -0.428453          -0.283286  ...   \n",
       "8           -0.304739          -0.407270          -0.288190  ...   \n",
       "9           -0.292514          -0.392050          -0.295155  ...   \n",
       "10          -0.283906          -0.384348          -0.304565  ...   \n",
       "11          -0.280987          -0.384889          -0.317184  ...   \n",
       "12          -0.286597          -0.393833          -0.334331  ...   \n",
       "13          -0.303805          -0.411297          -0.357686  ...   \n",
       "14          -0.334826          -0.437415          -0.388691  ...   \n",
       "15          -0.379632          -0.471534          -0.427760  ...   \n",
       "16          -0.434967          -0.511277          -0.473538  ...   \n",
       "17          -0.494612          -0.552766          -0.522577  ...   \n",
       "18          -0.551140          -0.591772          -0.569786  ...   \n",
       "19          -0.598364          -0.624705          -0.610040  ...   \n",
       "20          -0.633448          -0.649609          -0.640455  ...   \n",
       "21          -0.657117          -0.666683          -0.661210  ...   \n",
       "22          -0.672011          -0.677551          -0.674364  ...   \n",
       "23          -0.680965          -0.684132          -0.682304  ...   \n",
       "24          -0.686199          -0.687997          -0.686957  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         -0.377357        0.139255               14           -0.189113   \n",
       "1         -0.368316        0.127459               13           -0.189337   \n",
       "2         -0.359529        0.115838               12           -0.189701   \n",
       "3         -0.350936        0.104452               10           -0.190292   \n",
       "4         -0.342501        0.093232                9           -0.191247   \n",
       "5         -0.334384        0.082382                8           -0.192780   \n",
       "6         -0.326743        0.072223                6           -0.195209   \n",
       "7         -0.320023        0.063349                5           -0.198977   \n",
       "8         -0.314785        0.056295                3           -0.204686   \n",
       "9         -0.311855        0.051269                1           -0.213149   \n",
       "10        -0.312331        0.047855                2           -0.225453   \n",
       "11        -0.317767        0.045357                4           -0.242944   \n",
       "12        -0.330119        0.043041                7           -0.267094   \n",
       "13        -0.351396        0.040194               11           -0.299296   \n",
       "14        -0.382953        0.036270               15           -0.340451   \n",
       "15        -0.424439        0.031154               16           -0.390191   \n",
       "16        -0.473051        0.025157               17           -0.446060   \n",
       "17        -0.523968        0.018888               18           -0.503552   \n",
       "18        -0.571728        0.013115               19           -0.557156   \n",
       "19        -0.611712        0.008484               20           -0.601958   \n",
       "20        -0.641630        0.005201               21           -0.635466   \n",
       "21        -0.661955        0.003077               22           -0.658226   \n",
       "22        -0.674810        0.001782               23           -0.672620   \n",
       "23        -0.682565        0.001019               24           -0.681301   \n",
       "24        -0.687107        0.000578               25           -0.686386   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            -0.190769           -0.169377           -0.200170   \n",
       "1            -0.191047           -0.169708           -0.200395   \n",
       "2            -0.191501           -0.170236           -0.200761   \n",
       "3            -0.192236           -0.171084           -0.201352   \n",
       "4            -0.193426           -0.172432           -0.202309   \n",
       "5            -0.195322           -0.174564           -0.203833   \n",
       "6            -0.198308           -0.177902           -0.206232   \n",
       "7            -0.202914           -0.183007           -0.209928   \n",
       "8            -0.209814           -0.190532           -0.215465   \n",
       "9            -0.219822           -0.201120           -0.223533   \n",
       "10           -0.233898           -0.215366           -0.235032   \n",
       "11           -0.253189           -0.233969           -0.251172   \n",
       "12           -0.279034           -0.258058           -0.273492   \n",
       "13           -0.312775           -0.289279           -0.303701   \n",
       "14           -0.355258           -0.329256           -0.343193   \n",
       "15           -0.405968           -0.378408           -0.392096   \n",
       "16           -0.462086           -0.434727           -0.448107   \n",
       "17           -0.518569           -0.493673           -0.506201   \n",
       "18           -0.569756           -0.549364           -0.560084   \n",
       "19           -0.611370           -0.596374           -0.604565   \n",
       "20           -0.641835           -0.631778           -0.637421   \n",
       "21           -0.662240           -0.655934           -0.659532   \n",
       "22           -0.675034           -0.671252           -0.673431   \n",
       "23           -0.682713           -0.680504           -0.681784   \n",
       "24           -0.687198           -0.685929           -0.686666   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0            -0.203862         -0.190658         0.012001  \n",
       "1            -0.204093         -0.190916         0.011964  \n",
       "2            -0.204472         -0.191334         0.011910  \n",
       "3            -0.205086         -0.192010         0.011825  \n",
       "4            -0.206084         -0.193100         0.011697  \n",
       "5            -0.207694         -0.194839         0.011504  \n",
       "6            -0.210266         -0.197583         0.011217  \n",
       "7            -0.214304         -0.201826         0.010815  \n",
       "8            -0.220488         -0.208197         0.010304  \n",
       "9            -0.229618         -0.217448         0.009756  \n",
       "10           -0.242562         -0.230462         0.009296  \n",
       "11           -0.260270         -0.248309         0.009050  \n",
       "12           -0.283907         -0.272317         0.009071  \n",
       "13           -0.314827         -0.303976         0.009308  \n",
       "14           -0.354085         -0.344449         0.009572  \n",
       "15           -0.401450         -0.393623         0.009584  \n",
       "16           -0.454600         -0.449116         0.009118  \n",
       "17           -0.509344         -0.506268         0.008085  \n",
       "18           -0.560646         -0.559401         0.006558  \n",
       "19           -0.603871         -0.603628         0.004822  \n",
       "20           -0.636462         -0.636592         0.003245  \n",
       "21           -0.658739         -0.658934         0.002041  \n",
       "22           -0.672888         -0.673045         0.001227  \n",
       "23           -0.681444         -0.681549         0.000717  \n",
       "24           -0.686464         -0.686528         0.000413  \n",
       "\n",
       "[25 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1778279410038923\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 46;\n                var nbb_unformatted_code = \"model = BayesianLogisticRegression()\\nparam_grid = {\\\"default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 3, 25)]}\\ngrid_search = GridSearchCV(\\n    model,\\n    param_grid,\\n    scoring=make_scorer(log_loss, greater_is_better=False),\\n    return_train_score=True,\\n)\\ngrid_search.fit(X_train, y_train)\\ndisplay(pd.DataFrame(grid_search.cv_results_))\\nprint(grid_search.best_estimator_.default_parameters[\\\"p\\\"])\";\n                var nbb_formatted_code = \"model = BayesianLogisticRegression()\\nparam_grid = {\\\"default_parameters\\\": [{\\\"m\\\": 0, \\\"p\\\": p} for p in np.logspace(-3, 3, 25)]}\\ngrid_search = GridSearchCV(\\n    model,\\n    param_grid,\\n    scoring=make_scorer(log_loss, greater_is_better=False),\\n    return_train_score=True,\\n)\\ngrid_search.fit(X_train, y_train)\\ndisplay(pd.DataFrame(grid_search.cv_results_))\\nprint(grid_search.best_estimator_.default_parameters[\\\"p\\\"])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BayesianLogisticRegression()\n",
    "param_grid = {\"default_parameters\": [{\"m\": 0, \"p\": p} for p in np.logspace(-3, 3, 25)]}\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(log_loss, greater_is_better=False),\n",
    "    return_train_score=True,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "display(pd.DataFrame(grid_search.cv_results_))\n",
    "print(grid_search.best_estimator_.default_parameters[\"p\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d9c0c",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e12ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3897214331971086\n",
      "0.83\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 47;\n                var nbb_unformatted_code = \"y_pred = grid_search.best_estimator_.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_formatted_code = \"y_pred = grid_search.best_estimator_.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e5324",
   "metadata": {},
   "source": [
    "### Bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fc8e5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 48;\n                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y):\\n    n_feat = X.shape[1]\\n    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\\n    diag_sigma2 = np.exp(log_sigma2)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "def loss(log_sigma2, X, y):\n",
    "    n_feat = X.shape[1]\n",
    "    log_sigma2 = np.array([log_sigma2[0]] * n_feat)\n",
    "    diag_sigma2 = np.exp(log_sigma2)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=BayesianLogisticRegression()._jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = loss(xk, X, y)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = loss(xk, X, y)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb1653",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02a0bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([1.])=61.060407669288374\n",
      "fun([1.43303518])=60.64547121441359\n",
      "fun([1.50461444])=60.63622982158609\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 60.63622982158609\n",
      "        x: [ 1.505e+00]\n",
      "      nit: 3\n",
      "      jac: [-1.711e-06]\n",
      "     nfev: 4\n",
      "     njev: 4\n",
      " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
      "[0.22210291]\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 49;\n                var nbb_unformatted_code = \"res = minimize(\\n    loss,\\n    np.array([0.0]),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n                var nbb_formatted_code = \"res = minimize(\\n    loss,\\n    np.array([0.0]),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    loss,\n",
    "    np.array([0.0]),\n",
    "    args=(\n",
    "        X_train.to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac,\n",
    "    callback=callback,\n",
    ")\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8ba6f",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99c547a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                               &#x27;p&#x27;: array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianLogisticRegression</label><div class=\"sk-toggleable__content\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0,\n",
       "                                               &#x27;p&#x27;: array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianLogisticRegression(default_parameters={'m': 0,\n",
       "                                               'p': array([0.22210291])},\n",
       "                           optimize_kwargs={}, optimize_method='L-BFGS-B',\n",
       "                           prior_parameters={})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 50;\n                var nbb_unformatted_code = \"model = BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1 / np.exp(res.x)})\\nmodel.fit(X_train, y_train)\";\n                var nbb_formatted_code = \"model = BayesianLogisticRegression(default_parameters={\\\"m\\\": 0, \\\"p\\\": 1 / np.exp(res.x)})\\nmodel.fit(X_train, y_train)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BayesianLogisticRegression(default_parameters={\"m\": 0, \"p\": 1 / np.exp(res.x)})\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5823562",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "708cb5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.382668628864105\n",
      "0.8366666666666667\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 51;\n                var nbb_unformatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_formatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769d024",
   "metadata": {},
   "source": [
    "## Multiple sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b99d86de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 52;\n                var nbb_unformatted_code = \"cats = list(X.columns)\";\n                var nbb_formatted_code = \"cats = list(X.columns)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cats = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc63f684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 53;\n                var nbb_unformatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\";\n                var nbb_formatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_id = get_col_to_id(X_train.columns, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d024e1e",
   "metadata": {},
   "source": [
    "### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e9e2b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 54;\n                var nbb_unformatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y, cols, col_to_id):\\n    n_feat = X.shape[1]\\n    sigma2_list = []\\n    for col in cols:\\n        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\\n    diag_sigma2 = np.array(sigma2_list)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y, cols, col_to_id):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y, cols, col_to_id)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y, cols, col_to_id)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n                var nbb_formatted_code = \"def callback(intermediate_result):\\n    print(f\\\"fun({intermediate_result.x})={intermediate_result.fun}\\\")\\n\\n\\ndef loss(log_sigma2, X, y, cols, col_to_id):\\n    n_feat = X.shape[1]\\n    sigma2_list = []\\n    for col in cols:\\n        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\\n    diag_sigma2 = np.array(sigma2_list)\\n\\n    m = np.array([0] * n_feat)\\n    p = 1 / diag_sigma2\\n\\n    res = minimize(\\n        BayesianLogisticRegression()._loss,\\n        np.array([0] * n_feat),\\n        args=(X, y, m, p),\\n        method=\\\"L-BFGS-B\\\",\\n        jac=BayesianLogisticRegression()._jac,\\n    )\\n\\n    theta_star = res.x\\n\\n    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\\n\\n    out = (\\n        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\\n        - 0.5 * n_feat * np.log(2 * np.pi)\\n        + 0.5 * np.linalg.slogdet(H)[1]\\n    )\\n    return out\\n\\n\\ndef jac(log_sigma2, X, y, cols, col_to_id):\\n    h = np.log(1 + 1e-2)\\n    jac_list = []\\n    for ii in range(len(log_sigma2)):\\n        xk = np.copy(log_sigma2)\\n        xk[ii] += h\\n        fk_plus_h = loss(xk, X, y, cols, col_to_id)\\n        xk[ii] -= 2 * h\\n        fk_minus_h = loss(xk, X, y, cols, col_to_id)\\n        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\\n    return np.array(jac_list)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def callback(intermediate_result):\n",
    "    print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "def loss(log_sigma2, X, y, cols, col_to_id):\n",
    "    n_feat = X.shape[1]\n",
    "    sigma2_list = []\n",
    "    for col in cols:\n",
    "        sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\n",
    "    diag_sigma2 = np.array(sigma2_list)\n",
    "\n",
    "    m = np.array([0] * n_feat)\n",
    "    p = 1 / diag_sigma2\n",
    "\n",
    "    res = minimize(\n",
    "        BayesianLogisticRegression()._loss,\n",
    "        np.array([0] * n_feat),\n",
    "        args=(X, y, m, p),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=BayesianLogisticRegression()._jac,\n",
    "    )\n",
    "\n",
    "    theta_star = res.x\n",
    "\n",
    "    H = BayesianLogisticRegression()._hess(theta_star, X, y, m, p)\n",
    "\n",
    "    out = (\n",
    "        BayesianLogisticRegression()._loss(theta_star, X, y, m, p)\n",
    "        - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "        + 0.5 * np.linalg.slogdet(H)[1]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def jac(log_sigma2, X, y, cols, col_to_id):\n",
    "    h = np.log(1 + 1e-2)\n",
    "    jac_list = []\n",
    "    for ii in range(len(log_sigma2)):\n",
    "        xk = np.copy(log_sigma2)\n",
    "        xk[ii] += h\n",
    "        fk_plus_h = loss(xk, X, y, cols, col_to_id)\n",
    "        xk[ii] -= 2 * h\n",
    "        fk_minus_h = loss(xk, X, y, cols, col_to_id)\n",
    "        jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "    return np.array(jac_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b16238",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce721f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun([-0.12615174 -0.02922013  0.99158052])=59.20723001635101\n",
      "fun([-0.65596357 -0.09487296  2.58736172])=55.51144842183872\n",
      "fun([-0.92151598 -0.04330354  2.7817241 ])=55.354216890488715\n",
      "fun([-1.45481759  0.00776873  2.92284706])=55.22032802613284\n",
      "fun([-1.91015232 -0.0504136   2.87268155])=55.15229109268441\n",
      "fun([-2.32653384  0.08854265  2.77926044])=55.127431462018016\n",
      "fun([-2.83233246 -0.01662156  2.74486042])=55.10404821297138\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 55.10404821297138\n",
      "        x: [-2.832e+00 -1.662e-02  2.745e+00]\n",
      "      nit: 7\n",
      "      jac: [ 2.489e-06  5.180e-06 -3.672e-06]\n",
      "     nfev: 11\n",
      "     njev: 11\n",
      " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
      "[16.98503158  1.01676047  0.06425727]\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 55;\n                var nbb_unformatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\\nres = minimize(\\n    loss,\\n    np.array([0.0] * len(cats)),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n        X_train.columns,\\n        col_to_id,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n                var nbb_formatted_code = \"col_to_id = get_col_to_id(X_train.columns, cats)\\nres = minimize(\\n    loss,\\n    np.array([0.0] * len(cats)),\\n    args=(\\n        X_train.to_numpy(),\\n        y_train.to_numpy(),\\n        X_train.columns,\\n        col_to_id,\\n    ),\\n    method=\\\"L-BFGS-B\\\",\\n    jac=jac,\\n    callback=callback,\\n)\\nprint(res)\\nprint(1 / np.exp(res.x))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_id = get_col_to_id(X_train.columns, cats)\n",
    "res = minimize(\n",
    "    loss,\n",
    "    np.array([0.0] * len(cats)),\n",
    "    args=(\n",
    "        X_train.to_numpy(),\n",
    "        y_train.to_numpy(),\n",
    "        X_train.columns,\n",
    "        col_to_id,\n",
    "    ),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=jac,\n",
    "    callback=callback,\n",
    ")\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cbcd9",
   "metadata": {},
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2ad250f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0, &#x27;p&#x27;: 5},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={&#x27;col1_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985...\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col3_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562}})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesianLogisticRegression</label><div class=\"sk-toggleable__content\"><pre>BayesianLogisticRegression(default_parameters={&#x27;m&#x27;: 0, &#x27;p&#x27;: 5},\n",
       "                           optimize_kwargs={}, optimize_method=&#x27;L-BFGS-B&#x27;,\n",
       "                           prior_parameters={&#x27;col1_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985031576377285},\n",
       "                                             &#x27;col1_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 16.985...\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col2_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 1.0167604709151716},\n",
       "                                             &#x27;col3_a&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_b&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_c&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_d&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_e&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562},\n",
       "                                             &#x27;col3_f&#x27;: {&#x27;m&#x27;: 0,\n",
       "                                                        &#x27;p&#x27;: 0.06425726951140562}})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesianLogisticRegression(default_parameters={'m': 0, 'p': 5},\n",
       "                           optimize_kwargs={}, optimize_method='L-BFGS-B',\n",
       "                           prior_parameters={'col1_a': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_b': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_c': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_d': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_e': {'m': 0,\n",
       "                                                        'p': 16.985031576377285},\n",
       "                                             'col1_f': {'m': 0,\n",
       "                                                        'p': 16.985...\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col2_e': {'m': 0,\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col2_f': {'m': 0,\n",
       "                                                        'p': 1.0167604709151716},\n",
       "                                             'col3_a': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_b': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_c': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_d': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_e': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562},\n",
       "                                             'col3_f': {'m': 0,\n",
       "                                                        'p': 0.06425726951140562}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 56;\n                var nbb_unformatted_code = \"prior_parameters = {}\\n\\nfor col in X_train.columns:\\n    prior_parameters[col] = {\\\"m\\\": 0, \\\"p\\\": (1 / np.exp(res.x))[col_to_id[col]]}\\n\\nmodel = BayesianLogisticRegression(prior_parameters=prior_parameters)\\nmodel.fit(X_train, y_train)\";\n                var nbb_formatted_code = \"prior_parameters = {}\\n\\nfor col in X_train.columns:\\n    prior_parameters[col] = {\\\"m\\\": 0, \\\"p\\\": (1 / np.exp(res.x))[col_to_id[col]]}\\n\\nmodel = BayesianLogisticRegression(prior_parameters=prior_parameters)\\nmodel.fit(X_train, y_train)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prior_parameters = {}\n",
    "\n",
    "for col in X_train.columns:\n",
    "    prior_parameters[col] = {\"m\": 0, \"p\": (1 / np.exp(res.x))[col_to_id[col]]}\n",
    "\n",
    "model = BayesianLogisticRegression(prior_parameters=prior_parameters)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c5079",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a067a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2996493214889956\n",
      "0.8833333333333333\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 57;\n                var nbb_unformatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_formatted_code = \"y_pred = model.predict(X_test)\\n\\nprint(log_loss(y_test, y_pred))\\nprint(roc_auc_score(y_test, y_pred))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(log_loss(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bay_opt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
