{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3911da83",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Packages</a></span></li><li><span><a href=\"#Doc\" data-toc-modified-id=\"Doc-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Doc</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayesian-model\" data-toc-modified-id=\"Bayesian-model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Bayesian model</a></span></li><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Optimization</a></span></li></ul></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Link\" data-toc-modified-id=\"Link-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Link</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#Logit\" data-toc-modified-id=\"Logit-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Logit</a></span></li></ul></li><li><span><a href=\"#Bayesian-GLM\" data-toc-modified-id=\"Bayesian-GLM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Bayesian GLM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interface\" data-toc-modified-id=\"Interface-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Interface</a></span></li><li><span><a href=\"#BayesianLogisticRegression\" data-toc-modified-id=\"BayesianLogisticRegression-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>BayesianLogisticRegression</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"#Miscellenaous\" data-toc-modified-id=\"Miscellenaous-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Miscellenaous</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read\" data-toc-modified-id=\"Read-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Read</a></span></li><li><span><a href=\"#Split-train-test\" data-toc-modified-id=\"Split-train-test-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Split train test</a></span></li></ul></li><li><span><a href=\"#Studies\" data-toc-modified-id=\"Studies-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Studies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-sigma2\" data-toc-modified-id=\"Single-sigma2-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Single sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-model\" data-toc-modified-id=\"Naive-model-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Naive model</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.1.2.1\"><span class=\"toc-item-num\">5.1.2.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.1.2.2\"><span class=\"toc-item-num\">5.1.2.2&nbsp;&nbsp;</span>Test</a></span></li></ul></li><li><span><a href=\"#Bayesian-optimisation\" data-toc-modified-id=\"Bayesian-optimisation-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Bayesian optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.1.3.1\"><span class=\"toc-item-num\">5.1.3.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-5.1.3.2\"><span class=\"toc-item-num\">5.1.3.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.1.3.3\"><span class=\"toc-item-num\">5.1.3.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li><li><span><a href=\"#Multiple-sigma2\" data-toc-modified-id=\"Multiple-sigma2-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Multiple sigma2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayesian-optimization\" data-toc-modified-id=\"Bayesian-optimization-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Bayesian optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-5.2.1.1\"><span class=\"toc-item-num\">5.2.1.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Retrain\" data-toc-modified-id=\"Retrain-5.2.1.2\"><span class=\"toc-item-num\">5.2.1.2&nbsp;&nbsp;</span>Retrain</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-5.2.1.3\"><span class=\"toc-item-num\">5.2.1.3&nbsp;&nbsp;</span>Test</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95e967",
   "metadata": {},
   "source": [
    "___\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8298c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_dir = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(module_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression\n",
    "from scipy.optimize import minimize, OptimizeResult\n",
    "\n",
    "from module.bayesian_model import  BayesianNormalRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8b504",
   "metadata": {},
   "source": [
    "___\n",
    "# Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00b1b9",
   "metadata": {},
   "source": [
    "## Bayesian model\n",
    "The purpose of bayesian optimization is to model the parameters as random variables. For instance, in the use case that I am going to investigate, a binary classification, we can model the law of the dependent variable $Y_{i}$ conditioned on the parameters $\\boldsymbol{\\Theta}$ as a Bernoulli random variable depending on observed independent variables $\\boldsymbol{X_{i}}$ and the parameters as a multivariate normal random variable with a diagonal covariance matrix (parameters of $\\boldsymbol{\\sigma^{2}}$, which is a vector in my notation, may differ). Vectors are written in bold format in the following equations and random variable in uppercase.\n",
    "$$\n",
    "\\begin{align} \n",
    "Y_{i} \\mid \\boldsymbol{\\Theta} & \\sim \\text{Bern}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta}) \\\\\n",
    "\\boldsymbol{\\Theta} & \\sim \\mathcal{N}(0,\\boldsymbol{\\sigma^{2}}I)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It is possible to get one step further by modelling the diagonal elements of the covariance matrix, $\\boldsymbol{\\sigma^{2}}$, as a random variable as well, $\\boldsymbol{\\Sigma^{2}}$, with $Y_{i}$ and $\\boldsymbol{\\sigma^{2}}$ conditionally independent given $\\boldsymbol{\\Theta}$:\n",
    "$$\n",
    "\\begin{align} \n",
    "Y_{i} \\mid \\boldsymbol{\\Theta} & \\sim \\text{Bern}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta}) \\\\\n",
    "\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma} & \\sim \\mathcal{N}(0,\\boldsymbol{\\Sigma^{2}}I) \\\\\n",
    "\\Sigma_{i} & \\sim \\Gamma(\\alpha_{i}, \\beta_{i})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Optimization\n",
    "I am going to focus only on the 2nd model as it is more general. The purpose is to maximize $f_{\\Sigma \\mid \\boldsymbol{Y}}(\\sigma \\mid \\boldsymbol{y})$, $\\boldsymbol{Y}$ being the vectors of all the $Y_{i}$ random variables. Unfortunately, there is no closed form for the law of $Y_{i} \\mid \\boldsymbol{\\Sigma}$ contrary to the normal linear case (when $Y_{i} \\mid \\boldsymbol{\\Theta} \\sim \\mathcal{N}(\\boldsymbol{X_{i}}^{T}.\\boldsymbol{\\Theta},\\sigma^{2}I)$). That's being said, it is possible to approximate the density function of $\\Sigma \\mid \\boldsymbol{Y}$ thanks to the conditional independence and Bayes theorem:\n",
    "$$\n",
    "\\begin{align} \n",
    "f_{\\boldsymbol{\\Sigma} \\mid Y_{i}}(\\boldsymbol{\\sigma} \\mid y_{i})\n",
    "& = \\frac{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} \\mid \\boldsymbol{\\Sigma} = \\sigma ) f_{\\Sigma}(\\sigma)\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\\\\n",
    "& = \\frac{ \n",
    "    f_{Y_{i}, \\boldsymbol{\\Theta}, \\boldsymbol{\\Sigma}}(y_{i}, \\boldsymbol{\\theta}, \\sigma)\n",
    "}{\n",
    "    f_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta} \\mid y_{i}, \\boldsymbol{\\sigma})\n",
    "}\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\\\\n",
    "& = \\frac{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} \\mid \\boldsymbol{\\Theta} = \\theta )\n",
    "    f_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\sigma})\n",
    "    f_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\sigma})\n",
    "}{\n",
    "    f_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\sigma}}(\\boldsymbol{\\theta} \\mid y_{i}, \\boldsymbol{\\sigma})\n",
    "}\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\mathbb{P}( Y_{i} = y_{i} )\n",
    "}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We would like to find $\\boldsymbol{\\sigma}$ that maximize the previous density function given the observations $\\boldsymbol{y}$. The numerator can be evaluated provided that we know $\\boldsymbol{\\theta}$. The second element of the denominator does not depend on $\\boldsymbol{\\sigma}$ so we can discard it when trying to maximize our objective function. Finally, the first element of the denominator is unknown. This is why we need to approximate it by means of Laplace approximation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\boldsymbol{\\sigma} ; \\boldsymbol{y})\n",
    "&= \\sum \n",
    "    log\\left(\\mathbb{P}\\left( Y_{i} = y_{i} \\mid \\boldsymbol{\\Theta} = \\theta^{*} \\right)\\right)\n",
    "    + log(f_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{\\Sigma}}(\\boldsymbol{\\theta^{*}} \\mid \\boldsymbol{\\sigma}))\n",
    "    + log(f_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\sigma}))\n",
    "    - log(\\tilde{f}_{\\boldsymbol{\\Theta} \\mid Y_{i}, \\boldsymbol{\\sigma}}(\\boldsymbol{\\theta^{*}} \\mid y_{i}, \\boldsymbol{\\sigma}))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a63f4e",
   "metadata": {},
   "source": [
    "___\n",
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beca00",
   "metadata": {},
   "source": [
    "## adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fa5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(\n",
    "    fun,\n",
    "    x0,\n",
    "    jac,\n",
    "    args=(),\n",
    "    learning_rate=0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    eps=1e-8,\n",
    "    startiter=0,\n",
    "    maxiter=1000,\n",
    "    callback=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"``scipy.optimize.minimize`` compatible implementation of ADAM -\n",
    "    [http://arxiv.org/pdf/1412.6980.pdf].\n",
    "    Adapted from ``autograd/misc/optimizers.py``.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "\n",
    "    for i in range(startiter, startiter + maxiter):\n",
    "        g = jac(x, *args)\n",
    "\n",
    "        if callback and callback(x):\n",
    "            break\n",
    "\n",
    "        m = (1 - beta1) * g + beta1 * m  # first  moment estimate.\n",
    "        v = (1 - beta2) * (g**2) + beta2 * v  # second moment estimate.\n",
    "        mhat = m / (1 - beta1**(i + 1))  # bias correction.\n",
    "        vhat = v / (1 - beta2**(i + 1))\n",
    "        x = x - learning_rate * mhat / (np.sqrt(vhat) + eps)\n",
    "\n",
    "    i += 1\n",
    "    return OptimizeResult(x=x, fun=fun(x, *args), jac=g, nit=i, nfev=i, success=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32c426",
   "metadata": {},
   "source": [
    "## loss and jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399ed157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_single_sigma2(X, y, baysian_model, prior_dist_logpdf=None):\n",
    "    def callback(intermediate_result):\n",
    "        print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "    def loss(log_sigma2, X, y, baysian_model, prior_dist_logpdf = None):\n",
    "        n_feat = X.shape[1]\n",
    "        single_p = np.exp(-log_sigma2[0])\n",
    "        p = np.array([single_p] * n_feat)\n",
    "        m = np.array([0] * n_feat)\n",
    "\n",
    "        res = minimize(\n",
    "            baysian_model._loss,\n",
    "            np.array([0] * n_feat),\n",
    "            args=(X, y, m, p),\n",
    "            method=\"BFGS\",\n",
    "            jac=baysian_model._jac,\n",
    "        )\n",
    "\n",
    "        theta_star = res.x\n",
    "\n",
    "        H = baysian_model._hess(theta_star, X, y, m, p)\n",
    "\n",
    "        out = (\n",
    "            baysian_model._loss(theta_star, X, y, m, p)\n",
    "            - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "            + 0.5 * np.linalg.slogdet(H)[1]\n",
    "        )\n",
    "\n",
    "        if prior_dist_logpdf:\n",
    "            out += - prior_dist_logpdf(single_p)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def jac(log_sigma2, X, y, baysian_model, prior_dist_logpdf = None):\n",
    "        h = np.log(1 + 1e-2)\n",
    "        jac_list = []\n",
    "        for ii in range(len(log_sigma2)):\n",
    "            xk = np.copy(log_sigma2)\n",
    "            xk[ii] += h\n",
    "            fk_plus_h = loss(xk, X, y, baysian_model, prior_dist_logpdf)\n",
    "            xk[ii] -= 2 * h\n",
    "            fk_minus_h = loss(xk, X, y, baysian_model, prior_dist_logpdf)\n",
    "            jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "        return np.array(jac_list)\n",
    "\n",
    "    return adam(\n",
    "        loss,\n",
    "        np.array([0.0]),\n",
    "        jac,\n",
    "        args=(\n",
    "            X.to_numpy(),\n",
    "            y.to_numpy(),\n",
    "            baysian_model,\n",
    "            prior_dist_logpdf\n",
    "        ),\n",
    "        learning_rate=0.2,\n",
    "        maxiter=2000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4cc92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_to_id(cols, cats):\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "        for ii, cat in enumerate(cats):\n",
    "            if isinstance(cat, str):\n",
    "                if col.startswith(cat):\n",
    "                    out[col] = ii\n",
    "            else:\n",
    "                for cc in cat:\n",
    "                    if cc in col:\n",
    "                        out[col] = ii\n",
    "    return out\n",
    "\n",
    "def solve_multiple_sigma2(X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "    def callback(intermediate_result):\n",
    "        print(f\"fun({intermediate_result.x})={intermediate_result.fun}\")\n",
    "\n",
    "\n",
    "    def loss(log_sigma2, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "        n_feat = X.shape[1]\n",
    "        sigma2_list = []\n",
    "        for col in cols:\n",
    "            sigma2_list.append(np.exp(log_sigma2[col_to_id[col]]))\n",
    "        diag_sigma2 = np.array(sigma2_list)\n",
    "\n",
    "        m = np.array([0] * n_feat)\n",
    "        p = 1 / diag_sigma2\n",
    "\n",
    "        res = minimize(\n",
    "            baysian_model._loss,\n",
    "            np.array([0] * n_feat),\n",
    "            args=(X, y, m, p),\n",
    "            method=\"BFGS\",\n",
    "            jac=baysian_model._jac,\n",
    "        )\n",
    "\n",
    "        theta_star = res.x\n",
    "\n",
    "        H = baysian_model._hess(theta_star, X, y, m, p)\n",
    "\n",
    "        out = (\n",
    "            baysian_model._loss(theta_star, X, y, m, p)\n",
    "            - 0.5 * n_feat * np.log(2 * np.pi)\n",
    "            + 0.5 * np.linalg.slogdet(H)[1]\n",
    "        )\n",
    "\n",
    "        if prior_dist_logpdf:\n",
    "            out += - np.sum(prior_dist_logpdf(p))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def jac(log_sigma2, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf=None):\n",
    "        h = np.log(1 + 1e-2)\n",
    "        jac_list = []\n",
    "        for ii in range(len(log_sigma2)):\n",
    "            xk = np.copy(log_sigma2)\n",
    "            xk[ii] += h\n",
    "            fk_plus_h = loss(xk, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf)\n",
    "            xk[ii] -= 2 * h\n",
    "            fk_minus_h = loss(xk, X, y, cols, col_to_id, baysian_model, prior_dist_logpdf)\n",
    "            jac_list.append((fk_plus_h - fk_minus_h) / 2 * h)\n",
    "        return np.array(jac_list)\n",
    "    \n",
    "    return adam(\n",
    "        loss,\n",
    "        np.array([0.0]*len(set(col_to_id.values()))),\n",
    "        jac,\n",
    "        args=(\n",
    "            X.to_numpy(),\n",
    "            y.to_numpy(),\n",
    "            cols,\n",
    "            col_to_id,\n",
    "            baysian_model,\n",
    "            prior_dist_logpdf\n",
    "        ),\n",
    "        learning_rate=0.2,\n",
    "        maxiter=2000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f72ff",
   "metadata": {},
   "source": [
    "___\n",
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf333b60",
   "metadata": {},
   "source": [
    "## Create artificial dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1971cfb",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4aed85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_dataset():\n",
    "    n_samples = 200\n",
    "    sigma2 = 3\n",
    "    X = pd.DataFrame(\n",
    "        data={\n",
    "            \"col1\": np.random.normal(size=n_samples),\n",
    "            \"col2\": np.random.normal(size=n_samples),\n",
    "            \"col3\": np.random.normal(size=n_samples),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    theta = np.random.multivariate_normal(\n",
    "        np.zeros(X.shape[1]),\n",
    "        cov=np.eye(X.shape[1]),\n",
    "    )\n",
    "\n",
    "    y = pd.Series(\n",
    "        data=np.random.multivariate_normal(np.dot(X.to_numpy(), theta), sigma2*np.eye(n_samples)),\n",
    "        index=X.index,\n",
    "    )\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfce23",
   "metadata": {},
   "source": [
    "___\n",
    "# Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166fdea",
   "metadata": {},
   "source": [
    "## Linear case vs scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b7b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_regression_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da648472",
   "metadata": {},
   "source": [
    "### Without prior\n",
    "In order the model assumption to be correct (Laplace approximation), there must not be any regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b64c0",
   "metadata": {},
   "source": [
    "#### Single sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61402275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " success: True\n",
      "     fun: 307.2522129069934\n",
      "       x: [-1.172e+00]\n",
      "     nit: 2000\n",
      "     jac: [-5.656e-16]\n",
      "    nfev: 2000\n",
      "[3.22729835]\n"
     ]
    }
   ],
   "source": [
    "res = solve_single_sigma2(X_train, y_train, BayesianNormalRegression())\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50362b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.22467681696065\n"
     ]
    }
   ],
   "source": [
    "model = BayesianRidge(fit_intercept=False, lambda_1=1e-6, lambda_2=1e-6, alpha_1=1e-6, alpha_2=1e-6)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236945b1",
   "metadata": {},
   "source": [
    "#### Multiple sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8382d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " success: True\n",
      "     fun: 307.2382248615769\n",
      "       x: [-1.319e+00 -9.886e-01 -1.239e+00]\n",
      "     nit: 2000\n",
      "     jac: [-5.656e-16  0.000e+00 -5.656e-16]\n",
      "    nfev: 2000\n",
      "[3.7405191  2.68742002 3.45228315]\n"
     ]
    }
   ],
   "source": [
    "col_to_id = {col: idx for idx, col in enumerate(X_train.columns)}\n",
    "res = solve_multiple_sigma2(X_train, y_train, X_train.columns, col_to_id, BayesianNormalRegression())\n",
    "print(res)\n",
    "print(1 / np.exp(res.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7ebf6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.73699569, 2.68580224, 3.44890969])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ARDRegression(fit_intercept=False, lambda_1=1e-6, lambda_2=1e-6, alpha_1=1e-6, alpha_2=1e-6)\n",
    "model.fit(X_train, y_train)\n",
    "model.lambda_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bay_opt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
